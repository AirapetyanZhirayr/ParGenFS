paper name,abstract,keywords,link
Using an Iterative Reallocation Partitioning Algorithm to Verify Test Multidimensionality,"This article addresses the issue of assigning items to different test dimensions (e.g., determining which dimension an item belongs to) with cluster analysis. Previously, hierarchical methods have been used (Roussos et al. 1997); however, the findings here suggest that an iterative reallocation partitioning (IRP) algorithm provides interpretively similar solutions and statistically better solutions to the problem. More importantly, it is shown that the inherent nature of locally optimal solutions in the IRP algorithm leads to a method that aids in determining the appropriateness of performing a cluster analysis—a feature that is lacking in the standard hierarchical methods currently in the literature.","Test theory, Cluster analysis, k-means clustering",https://link.springer.com//article/10.1007/s00357-019-09347-z
MCC: a Multiple Consensus Clustering Framework,"Consensus clustering has emerged as an important extension of the classical clustering problem. Given a set of input clusterings of a given dataset, consensus clustering aims to find a single final clustering which is a better fit in some sense than the existing clusterings. There is a significant drawback in generating a single consensus clustering since different input clusterings could differ significantly. In this paper, we develop a new framework, called Multiple Consensus Clustering (MCC), to explore multiple clustering views of a given dataset from a set of input clusterings. Instead of generating a single consensus, we propose two sets of approaches to obtain multiple consensus. One employs the meta clustering method, and the other uses a hierarchical tree structure and further applies a dynamic programming algorithm to generate a flat partition from the hierarchical tree using the modularity measure. Multiple consensuses are finally obtained by applying consensus clustering algorithms to each cluster of the partition. Extensive experimental results on 11 real-world datasets and a case study on a Protein-Protein Interaction (PPI) dataset demonstrate the effectiveness of the MCC framework.","Consensus clusterings, Multiple clusterings",https://link.springer.com//article/10.1007/s00357-019-09318-4
Note: t for Two (Clusters),"The computation for cluster analysis is done by iterative algorithms. But here, a straightforward, non-iterative procedure is presented for clustering in the special case of one variable and two groups. The method is univariate but may reasonably be applied to multivariate datasets when the first principal component or a single factor explains much of the variation in the data. The t method is motivated by the fact that minimizing the within-groups sum of squares is equivalent to maximizing the between-groups sum of squares, and that Student’s t statistic measures the between-groups difference in means relative to within-groups variation. That is, the t statistic is the ratio of the difference in sample means, divided by the standard error of this difference. So, maximizing the t statistic is developed as a method for clustering univariate data into two clusters. In this situation, the t method gives the same results as the K-means algorithm. K-means tacitly assumes equality of variances; here, however, with t, equality of variances need not be assumed because separate variances may be used in computing t. The t method is applied to some datasets; the results are compared with those obtained by fitting mixtures of distributions.","Cluster analysis, Student’s t, Unequal variances",https://link.springer.com//article/10.1007/s00357-019-09335-3
The δ-Machine: Classification Based on Distances Towards Prototypes,"We introduce the δ-machine, a statistical learning tool for classification based on (dis)similarities between profiles of the observations to profiles of a representation set consisting of prototypes. In this article, we discuss the properties of the δ-machine, propose an automatic decision rule for deciding on the number of clusters for the K-means method on the predictive perspective, and derive variable importance measures and partial dependence plots for the machine. We performed five simulation studies to investigate the properties of the δ-machine. The first three simulation studies were conducted to investigate selection of prototypes, different (dis)similarity functions, and the definition of representation set. Results indicate that we best use the Lasso to select prototypes, that the Euclidean distance is a good dissimilarity function, and that finding a small representation set of prototypes gives sparse but competitive results. The remaining two simulation studies investigated the performance of the δ-machine with imbalanced classes and with unequal covariance matrices for the two classes. The results obtained show that the δ-machine is robust to class imbalances, and that the four (dis)similarity functions had the same performance regardless of the covariance matrices. We also showed the classification performance of the δ-machine compared with three other classification methods on ten real datasets from UCI database, and discuss two empirical examples in detail.","Dissimilarity space, Nonlinear classification, The Lasso",https://link.springer.com//article/10.1007/s00357-019-09338-0
A Framework for Quantifying Qualitative Responses in Pairwise Experiments,"Suppose an experiment is conducted on pairs of objects with outcome response a continuous variable measuring the interactions among the pairs. Furthermore, assume the response variable is hard to measure numerically but we may code its values into low and high levels of interaction (and possibly a third category in between if neither label applies). In this paper, we estimate the interaction values from the information contained in the coded data and the design structure of the experiment. A novel estimation method is introduced and shown to enjoy several optimal properties including maximum explained variance in the responses with minimum number of parameters and for any probability distribution underlying the responses. Furthermore, the interactions have the simple interpretation of correlation (in absolute value), size of error is estimable from the experiment, and only a single run of each pair is needed for the experiment. We also explore possible applications of the technique. Three applications are presented, one on protein interaction, a second on drug combination, and the third on machine learning. The first two applications are illustrated using real life data while for the third application, the data are generated via binary coding of an image.","Quantification, Positive semidefinite programming, Networks, Protein interaction, Drug synergy, Machine learning",https://link.springer.com//article/10.1007/s00357-019-09337-1
A New Relationship Between Intuitionistic Fuzzy Sets and Genetics,"Intuitionistic fuzzy sets represent a generalization of the concept of fuzzy sets and has many applications to different fields of science. This paper applies the concept of intuitionistic fuzzy sets to genetics. More precisely, we consider some examples of hypergroupoids associated to biological inheritance, construct sequences of join spaces and find their intuitionistic fuzzy grades.","Intuitionistic fuzzy set, Join space, Genotype, Phenotype",https://link.springer.com//article/10.1007/s00357-018-9276-8
Three-Way Symbolic Tree-Maps and Ultrametrics,"Three-way dissimilarities are a generalization of (two-way) dissimilarities which can be used to indicate the lack of homogeneity or resemblance between any three objects. Such maps have applications in cluster analysis and have been used in areas such as psychology and phylogenetics, where three-way data tables can arise. Special examples of such dissimilarities are three-way tree-metrics and ultrametrics, which arise from leaf-labelled trees with edges labelled by positive real numbers. Here we consider three-way maps which arise from leaf-labelled trees where instead the interior vertices are labelled by an arbitrary set of values. For unrooted trees, we call such maps three-way symbolic tree-maps; for rooted trees, we call them three-way symbolic ultrametrics since they can be considered as a generalization of the (two-way) symbolic ultrametrics of Bocker and Dress. We show that, as with two- and three-way tree-metrics and ultrametrics, three-way symbolic tree-maps and ultrametrics can be characterized via certain k-point conditions. In the unrooted case, our characterization is mathematically equivalent to one presented by Gurvich for a certain class of edge-labelled hypergraphs. We also show that it can be decided whether or not an arbitrary three-way symbolic map is a tree-map or a symbolic ultrametric using a triplet-based approach that relies on the so-called BUILD algorithm for deciding when a set of 3-leaved trees or triplets can be displayed by a single tree. We envisage that our results will be useful in developing new approaches and algorithms for understanding 3-way data, especially within the area of phylogenetics.","Three-way dissimilarity, Three-way symbolic map, Symbolic ultrametric, Ultrametric, Tree-metric, Phylogenetic tree",https://link.springer.com//article/10.1007/s00357-018-9274-x
Attribute Hierarchy Models in Cognitive Diagnosis: Identifiability of the Latent Attribute Space and Conditions for Completeness of the Q-Matrix,"Educational researchers have argued that a realistic view of the role of attributes in cognitively diagnostic modeling should account for the possibility that attributes are not isolated entities, but interdependent in their effect on test performance. Different approaches have been discussed in the literature; among them the proposition to impose a hierarchical structure so that mastery of one or more attributes is a prerequisite of mastering one or more other attributes. A hierarchical organization of attributes constrains the latent attribute space such that several proficiency classes, as they exist if attributes are not hierarchically organized, are no longer defined because the corresponding attribute combinations cannot occur with the given attribute hierarchy. Hence, the identification of the latent attribute space is often difficult—especially, if the number of attributes is large. As an additional complication, constructing a complete Q-matrix may not at all be straightforward if the attributes underlying the test items are supposed to have a hierarchical structure. In this article, the conditions of identifiability of the latent space if attributes are hierarchically organized and the conditions of completeness of the Q-matrix are studied.","Cognitive diagnosis, Attribute hierarchy, Latent attribute apace, Q-Matrix, Completeness, DINA model, General DCMs",https://link.springer.com//article/10.1007/s00357-018-9278-6
A Note on Applying the BCH Method Under Linear Equality and Inequality Constraints,"Researchers often wish to relate estimated scores on latent variables to exogenous covariates not previously used in analyses. The BCH method corrects for asymptotic bias in estimates due to these scores’ uncertainty and has been shown to be relatively robust. When applying the BCH approach however, two problems arise. First, negative cell proportions can be obtained. Second, the approach cannot deal with situations where marginals need to be fixed to specific values, such as edit restrictions. The BCH approach can handle these problems when placed in a framework of quadratic loss functions and linear equality and inequality constraints. This research note gives the explicit form for equality constraints and demonstrates how solutions for inequality constraints may be obtained using numerical methods.","Classification, Latent class analysis, Three-step procedure, BCH method",https://link.springer.com//article/10.1007/s00357-018-9298-2
Quantum-Behaved Particle Swarm Optimization for Parameter Optimization of Support Vector Machine,"Support vector machine (SVM) parameters such as penalty parameter and kernel parameters have a great influence on the complexity and accuracy of SVM model. In this paper, quantum-behaved particle swarm optimization (QPSO) has been employed to optimize the parameters of SVM, so that the classification error can be reduced. To evaluate the proposed model (QPSO-SVM), the experiment adopted seven standard classification datasets which are obtained from UCI machine learning data repository. For verification, the results of the QPSO-SVM algorithm are compared with the standard PSO, and genetic algorithm (GA) which is one of the well-known optimization algorithms. Moreover, the results of QPSO are compared with the grid search, which is a conventional method of searching parameter values. The experimental results demonstrated that the proposed model is capable to find the optimal values of the SVM parameters. The results also showed lower classification error rates compared with standard PSO and GA algorithms.","Quantum particle swarm optimization (QPSO), Optimization algorithms, Support vector machine (SVM), Classification, Parameter optimization",https://link.springer.com//article/10.1007/s00357-018-9299-1
MDCGen: Multidimensional Dataset Generator for Clustering,"We present a tool for generating multidimensional synthetic datasets for testing, evaluating, and benchmarking unsupervised classification algorithms. Our proposal fills a gap observed in previous approaches with regard to underlying distributions for the creation of multidimensional clusters. As a novelty, normal and non-normal distributions can be combined for either independently defining values feature by feature (i.e., multivariate distributions) or establishing overall intra-cluster distances. Being highly flexible, parameterizable, and randomizable, MDCGen also implements classic pursued features: (a) customization of cluster-separation, (b) overlap control, (c) addition of outliers and noise, (d) definition of correlated variables and rotations, (e) flexibility for allowing or avoiding isolation constraints per dimension, (f) creation of subspace clusters and subspace outliers, (g) importing arbitrary distributions for the value generation, and (h) dataset quality evaluations, among others. As a result, the proposed tool offers an improved range of potential datasets to perform a more comprehensive testing of clustering algorithms.","Clustering, Dataset generator, Synthetic data",https://link.springer.com//article/10.1007/s00357-019-9312-3
Comparing the Utility of Different Classification Schemes for Emotive Language Analysis,"In this paper we investigated the utility of different classification schemes for emotive language analysis with the aim of providing experimental justification for the choice of scheme for classifying emotions in free text. We compared six schemes: (1) Ekman's six basic emotions, (2) Plutchik's wheel of emotion, (3) Watson and Tellegen's Circumplex theory of affect, (4) the Emotion Annotation Representation Language (EARL), (5) WordNet–Affect, and (6) free text. To measure their utility, we investigated their ease of use by human annotators as well as the performance of supervised machine learning. We assembled a corpus of 500 emotionally charged text documents. The corpus was annotated manually using an online crowdsourcing platform with five independent annotators per document. Assuming that classification schemes with a better balance between completeness and complexity are easier to interpret and use, we expect such schemes to be associated with higher inter–annotator agreement. We used Krippendorff's alpha coefficient to measure inter–annotator agreement according to which the six classification schemes were ranked as follows: (1) six basic emotions (α = 0.483), (2) wheel of emotion (α = 0.410), (3) Circumplex (α = 0.312), EARL (α = 0.286), (5) free text (α = 0.205), and (6) WordNet–Affect (α = 0.202). However, correspondence analysis of annotations across the schemes highlighted that basic emotions are oversimplified representations of complex phenomena and as such likely to lead to invalid interpretations, which are not necessarily reflected by high inter-annotator agreement. To complement the result of the quantitative analysis, we used semi–structured interviews to gain a qualitative insight into how annotators interacted with and interpreted the chosen schemes. The size of the classification scheme was highlighted as a significant factor affecting annotation. In particular, the scheme of six basic emotions was perceived as having insufficient coverage of the emotion space forcing annotators to often resort to inferior alternatives, e.g. using happiness as a surrogate for love. On the opposite end of the spectrum, large schemes such as WordNet–Affect were linked to choice fatigue, which incurred significant cognitive effort in choosing the best annotation. In the second part of the study, we used the annotated corpus to create six training datasets, one for each scheme. The training data were used in cross–validation experiments to evaluate classification performance in relation to different schemes. According to the F-measure, the classification schemes were ranked as follows: (1) six basic emotions (F = 0.410), (2) Circumplex (F = 0.341), (3) wheel of emotion (F = 0.293), (4) EARL (F = 0.254), (5) free text (F = 0.159) and (6) WordNet–Affect (F = 0.158). Not surprisingly, the smallest scheme was ranked the highest in both criteria. Therefore, out of the six schemes studied here, six basic emotions are best suited for emotive language analysis. However, both quantitative and qualitative analysis highlighted its major shortcoming – oversimplification of positive emotions, which are all conflated into happiness. Further investigation is needed into ways of better balancing positive and negative emotions.","Annotation, Crowdsourcing, Text classification, Sentiment analysis, Supervised machine learning",https://link.springer.com//article/10.1007/s00357-019-9307-0
Card Sorting Data Collection Methodology: How Many Participants Is Most Efficient?,"Pairwise similarity judgments and card sorting methodologies are different ways of generating data for similarity matrices used in various analyses such as multidimensional scaling and cluster analysis. Pairwise similarity judgments are considered the gold standard methodology, but can be cumbersome for large numbers of stimuli given the geometric increase in number of judgments necessary to fill the matrix. Card sorting methods provide a more expedient means of gathering this information, although they typically generate only binary data. Nonetheless, aggregated matrices generated from card sorts approximate pairwise similarity matrices. The current study used pairwise similarity and card sorting results from two existing studies that used the same stimuli to determine the optimal number of participants needed in a card sorting task to approximate the similarity matrix of pairwise data collection. In these studies, approximately 10–15 participants provided optimal estimation of the similarity matrix, with minimal increases for higher numbers of participants.","Pairwise similarity matrix, Card sorting, Power analysis, Multidimensional scaling, Cluster analysis",https://link.springer.com//article/10.1007/s00357-018-9292-8
Effects of Distance and Shape on the Estimation of the Piecewise Growth Mixture Model,"The piecewise growth mixture model is used in longitudinal studies to tackle non-continuous trajectories and unobserved heterogeneity in a compound way. This study investigated how factors such as latent distance and shape influence the model. Two simulation studies were used exploring the 2- and 3-class situation with sample size, latent distance (Mahalanobis distance), and shape being considered as the influencing factor. The results of two simulations showed that a non-parallel shape led to a slightly better overall model fit. Parameter estimation is affected by the shape, mainly through the parameter differences between latent classes.","Piecewise growth mixture model (PGMM), Latent distance, Shape, Parameter difference",https://link.springer.com//article/10.1007/s00357-018-9291-9
Exploratory Visual Inspection of Category Associations and Correlation Estimation in Multidimensional Subspaces,"In this paper, we aimed to estimate associations among categories in a multi-way contingency table. To simplify estimation and interpretation of results, we stacked multiple variables to form a two-way stacked table and analyzed it using the biplot in correspondence analysis (CA) paradigm. The correspondence analysis biplot allowed visual inspection of category associations in a twodimensional plane, and the CA solution numerically estimated the category relationships. We utilized parallel analysis and identified two statistically meaningful dimensions with which a plane was constructed. In the plane, we examined metric space mapping, which was converted into correlations, between school districts and categories of school-relevant variables. The results showed differential correlation patterns among school districts and this correlational information may be useful for stake holders or policy makers to pinpoint possible causes of low school performance and school-relevant behaviors.","Correspondence analysis, Biplot, Parallel analysis for corresponddence analysis, Associations among categories, New York City school districts",https://link.springer.com//article/10.1007/s00357-018-9277-7
Hierarchies from Lowest Stable Ancestors in Nonbinary Phylogenetic Networks,"The reconstruction of the evolutionary history of a set of species is an important problem in classification and phylogenetics. Phylogenetic networks are a generalization of evolutionary trees that are used to represent histories for species that have undergone reticulate evolution, an important evolutionary force for many organisms (e.g. plants or viruses). In this paper, we present a novel approach to understanding the structure of networks that are not necessarily binary. More specifically, we define the concept of a closed set and show that the collection of closed sets of a network forms a hierarchy, and that this hierarchy can be deduced from either the subtrees or subnetworks on all 3-subsets. This allows us to also show that closed sets generalize the concept of the SN-sets of a binary network, sets which have proven very useful in elucidating the structure of binary networks. We also characterize the minimal closed sets (under set inclusion) for a special class of networks (2-terminal networks). Taken together, we anticipate that our results should be useful for the development of new phylogenetic network reconstruction algorithms.","Phylogenetic network, Hierarchy, Lower Stable Ancestor, Nonbinary network",https://link.springer.com//article/10.1007/s00357-018-9279-5
On Fractionally-Supervised Classification: Weight Selection and Extension to the Multivariate t-Distribution,"Recent work on fractionally-supervised classification (FSC), an approach that allows classification to be carried out with a fractional amount of weight given to the unlabelled points, is further developed in two respects. The primary development addresses a question of fundamental importance over how to choose the amount of weight given to the unlabelled points. The resolution of this matter is essential because it makes FSC more readily applicable to real problems. Interestingly, the resolution of the weight selection problem opens up the possibility of a different approach to model selection in model-based clustering and classification. A secondary development demonstrates that the FSC approach can be effective beyond Gaussian mixture models. To this end, an FSC approach is illustrated using mixtures of multivariate t-distributions.","Fractionally-supervised classification, Weight selection, Multivariate t-distribution",https://link.springer.com//article/10.1007/s00357-018-9280-z
On the Logistic Behaviour of the Topological Ultrametricity of Data,"Recently, it has been observed that topological ultrametricity of data can be expressed as an integral over a function which describes local ultrametricity. It was then observed empirically that this function begins as a sharply decreasing function, in order to increase again back to one. After providing a method for estimating the falling part of the local ultrametricity of data, empirical evidence is given for its logistic behaviour in relation to the number of connected components of the Vietoris-Rips graphs involved. The result is a functional dependence between that number and the number of maximal cliques. Further, it turns out that the logistic parameters depend linearly on the datasize. These observations are interpreted in terms of the Erdős-Rényi model for random graphs. Thus the findings allow to define a percolationbased index for almost ultrametricity which can be estimated in O(N2 logN) time which is more efficient than most ultrametricity indices.","Ultrametricty, Random graphs, Percolation, Topological data analysis, Logistic function",https://link.springer.com//article/10.1007/s00357-018-9281-y
Basic Co-Occurrence Latent Semantic Vector Space Model,"The vector representation is one of the important parts in document clustering or classification, which can quantify the text. In this paper, a novel Cooccurrence Latent Semantic Vector Space Model (CLSVSM) is presented and the co-occurrence distribution is further studied. This model is developed based on the Vector Space Model (VSM), embedding the co-occurrence latent semantic of the documents’ keywords to represent their vectors. First, experiments were conducted to test the model performance, using documents from Chinese National Knowledge Infrastructure (CNKI). The results showed the Entropy (E), Purity (P) and F1 value of CLMSVM is 20% better than in VSM in the documents clustering testing, which reveals that CLSVSM can improve the accuracy of clustering of documents, meanwhile reducing sparse degree of vectors. Second, it is the best to estimate the latent semantic: maximum (MAX), minimum (MIN), average (AVE), and median (MED)? More experiments are performed to compare the four estimators. The results indicate that Max and AVE are preferred method, while MIN method is the worst, which coincided with the discussion. Some essential questions were discussed at the end. These questions related to the trends of co-occurrence frequency, the function of co-occurrence intensity and its distribution, which reinforced the model.","CLSVSM, VSM, Clustering, High-dimensional vector, Co-occurrence, Co-word",https://link.springer.com//article/10.1007/s00357-018-9283-9
Conditional Independence and Dimensionality of Cognitive Diagnostic Models: a Test for Model Fit,"Nonparametric cognitive diagnosis methods are useful in cognitive diagnosis modeling for calibration efficiency, especially when sample size is small or large, or the latent attributes are more complex. This article proposes the Mantel-Haenszel chi-squared statistic as an index for detecting the misspecification of latent attributes as well as testlet effects in nonparametric cognitive diagnosis methods. The proposed theoretical considerations are augmented by simulation studies conducted to assess the performance of the Mantel-Haenszel statistic under various conditions within the nonparametric diagnosis framework, with a special focus on situations were the set of latent abilities assumed to underlie the data was underspecified.","Cognitive diagnosis model, Nonparametric approach, Local independence, Qmatrix validation",https://link.springer.com//article/10.1007/s00357-018-9287-5
A Reliable Small Sample Classification Algorithm by Elman Neural Network Based on PLS and GA,"Aiming at the small sample with high-feature dimension and few numbers will cause a serious problem if simply using the traditional Elman neural network to deal with the small sample; these problems include poor learning ability, the redundancy structure, and incomplete training; these defects will result in lower operating efficiency and poor recognition precision. In this paper, combining the theory of partial least squares (PLS) and genetic algorithm (GA), as well as the nature of Elman neural network, and an optimized Elman neural network classification algorithm based on PLS and GA (PLS-GA-Elman) is established. The new algorithm reduces the feature dimension of small samples by PLS, the relatively ideal low-dimensional data is obtained, and the purpose reduces the neural network’s inputs and simplifies its structure. Using GA to optimize the connection weights, threshold values, and the number of hidden neurons and adopting the optimized way of encoding respectively and evolving simultaneously can improve the neural network incomplete training condition, leading to fewer number of samples and improvement in training speed and generalization ability; this ensures the optimal Elman neural network algorithm. A new algorithm based on twice consecutive optimization was the basis for a precise classification model. The results of experimental analysis illustrate that operating efficiency and classification precision of the new algorithm have been improved.","Small sample, Elman neural network, Partial least squares, Genetic algorithm, PLS-GA-Elman algorithm",https://link.springer.com//article/10.1007/s00357-018-9288-4
Modeling Community Structure and Topics in Dynamic Text Networks,"The last decade has seen great progress in both dynamic network modeling and topic modeling. This paper draws upon both areas to create a bespoke Bayesian model applied to a dataset consisting of the top 467 US political blogs in 2012, their posts over the year, and their links to one another. Our model allows dynamic topic discovery to inform the latent network model and the network structure to facilitate topic identification. Our results find complex community structure within this set of blogs, where community membership depends strongly upon the set of topics in which the blogger is interested. We examine the time varying nature of the Sensational Crime topic, as well as the network properties of the Election News topic, as notable and easily interpretable empirical examples.","Networks, Natural language processing, Topic modeling, Political blogs, Community detection",https://link.springer.com//article/10.1007/s00357-018-9289-3
Distance and Consensus for Preference Relations Corresponding to Ordered Partitions,"Ranking is an important part of several areas of contemporary research, including social sciences, decision theory, data analysis, and information retrieval. The goal of this paper is to align developments in quantitative social sciences and decision theory with the current thought in Computer Science, including a few novel results. Specifically, we consider binary preference relations, the so-called weak orders that are in one-to-one correspondence with rankings. We show that the conventional symmetric difference distance between weak orders, considered as sets of ordered pairs, coincides with the celebrated Kemeny distance between the corresponding rankings, despite the seemingly much simpler structure of the former. Based on this, we review several properties of the geometric space of weak orders involving the ternary relation “between,” and contingency tables for cross-partitions. Next, we reformulate the consensus ranking problem as a variant of finding an optimal linear ordering, given a correspondingly defined consensus matrix. The difference is in a subtracted term, the partition concentration that depends only on the distribution of the objects in the individual parts. We apply our results to the conventional Likert scale to show that the Kemeny consensus rule is rather insensitive to the data under consideration and, therefore, should be supplemented with more sensitive consensus schemes.","Ranking, Tied ranking, Ordered partition, Weak order, Distance, Consensus, Muchnik test",https://link.springer.com//article/10.1007/s00357-018-9290-x
Multiscale Clustering for Functional Data,"In an era of massive and complex data, clustering is one of the most important procedures for understanding and analyzing unstructured multivariate data. Classical methods such as K-means and hierarchical clustering, however, are not efficient in grouping data that are high dimensional and have inherent multiscale structures. This paper presents new clustering procedures that can adapt to multiscale characteristics and high dimensionality of data. The proposed methods are based on a novel combination of multiresolution analysis and functional data analysis. As the core of the methodology, a clustering approach using the concept of multiresolution analysis may reflect both the global trend and local activities of data, and functional data analysis handles the high-dimensional data efficiently. Practical algorithms to implement the proposed methods are further discussed. The empirical performance of the proposed methods is evaluated through numerical studies including a simulation study and real data analysis, which demonstrates promising results of the proposed clustering.","Empirical mode decomposition, Functional data, High-dimensional data, Multiresolution analysis, Wavelet transform",https://link.springer.com//article/10.1007/s00357-019-09313-9
Growth Mixture Modeling with Measurement Selection,"Growth mixture models are an important tool for detecting group structure in repeated measures data. Unlike traditional clustering methods, they explicitly model the repeated measurements on observations, and the statistical framework they are based on allows for model selection methods to be used to select the number of clusters. However, the basic growth mixture model makes the assumption that all of the measurements in the data have grouping information that separate the clusters. In other clustering contexts, it has been shown that including non-clustering variables in clustering procedures can lead to poor estimation of the group structure both in terms of the number of clusters and cluster membership/parameters. In this paper, we present an extension of the growth mixture model that allows for incorporation of stepwise variable selection based on the work done by Maugis, Celeux, and Martin-Magniette (2009) and Raftery and Dean (2006). Results presented on a simulation study suggest that the method performs well in correctly selecting the clustering variables and improves on recovery of the cluster structure compared with the basic growth mixture model. The paper also presents an application of the model to a clinical study dataset and concludes with a discussion and suggestions for directions of future work in this area.","Cluster analysis, Growth mixture model, Repeated measurements, Longitudinal data, Measurement selection",https://link.springer.com//article/10.1007/s00357-018-9275-9
A Mixture of Coalesced Generalized Hyperbolic Distributions,"A mixture of multiple scaled generalized hyperbolic distributions (MMSGHDs) is introduced. Then, a coalesced generalized hyperbolic distribution (CGHD) is developed by joining a generalized hyperbolic distribution with a multiple scaled generalized hyperbolic distribution. After detailing the development of the MMSGHDs, which arises via implementation of a multi-dimensional weight function, the density of the mixture of CGHDs is developed. A parameter estimation scheme is developed using the ever-expanding class of MM algorithms and the Bayesian information criterion is used for model selection. The issue of cluster convexity is examined and a special case of the MMSGHDs is developed that is guaranteed to have convex clusters. These approaches are illustrated and compared using simulated and real data. The identifiability of the MMSGHDs and the mixture of CGHDs are discussed in an appendix.","Clustering, Coalesced distributions, Convexity, Finite mixture models, Generalized hyperbolic distribution, Mixture of mixtures, MM algorithm, Multiple scaled distributions",https://link.springer.com//article/10.1007/s00357-019-09319-3
Comparison of Similarity Measures for Categorical Data in Hierarchical Clustering,"This paper deals with similarity measures for categorical data in hierarchical clustering, which can deal with variables with more than two categories, and which aspire to replace the simple matching approach standardly used in this area. These similarity measures consider additional characteristics of a dataset, such as a frequency distribution of categories or the number of categories of a given variable. The paper recognizes two main aims. First, to compare and evaluate the selected similarity measures regarding the quality of produced clusters in hierarchical clustering. Second, to propose new similarity measures for nominal variables. All the examined similarity measures are compared regarding the quality of the produced clusters using the mean ranked scores of two internal evaluation coefficients. The analysis is performed on the generated datasets, and thus, it allows determining in which particular situations a certain similarity measure is recommended for use.","Similarity measures, Nominal variables, Hierarchical cluster analysis, Comparison, Evaluation",https://link.springer.com//article/10.1007/s00357-019-09317-5
Simultaneous Method of Orthogonal Non-metric Non-negative Matrix Factorization and Constrained Non-hierarchical Clustering,"For multivariate categorical data, it is important to detect both clustering structures and low dimensions such that clusters are discriminated. This is because it is easy to interpret the features of clusters through the estimated low dimensions. It is sure that these existing methods for dimensional reduction clustering are useful to achieve such purpose; however, the interpretation sometimes becomes complicated due to the sign of the estimated parameters. Thus, we propose new dimensional reduction clustering with non-negativity constraints for all parameters. The proposed method has several advantages. First, when the features of clusters are interpreted, it is easier to interpret the clusters since effects of sign should not be considered. In addition, from the non-negativity and orthogonality constraints, the estimated components become perfect simple structure, which is interpretable descriptions. Second, we showed that the clustering results are not inferior to these existing methods through the simulations, although the constraints for the proposed method are strong.","Dimensional reduction, Perfect simple structure, Categorical, K-mode clustering",https://link.springer.com//article/10.1007/s00357-018-9284-8
Optimal Landmark Point Selection Using Clustering for Manifold Modeling and Data Classification,"As data volume and dimensions continue to grow, effective and efficient methods are needed to obtain the low dimensional features of the data that describe its true structure. Most nonlinear dimensionality reduction methods (NLDR) utilize the Euclidean distance between the data points to form a general idea of the data manifold structure. Isomap uses the geodesic distance between data points and then uses classical multidimensional scaling(cMDS) to obtain low dimensional features. As the data size increases Isomap becomes complex. To overcome this disadvantage, Landmark Isomap (L-Isomap) uses selected data points called landmark points and finds the geodesic distance from these points to all other non-landmark points. Traditionally, landmark points are randomly selected without considering any statistical property of the data manifold. We contend that the quality of the features extracted is dependent on the selection of the landmark points. In applications such as data classification, the net accuracy is dependent on the quality of the features selected, and hence landmark points selection might play a crucial role. In this paper, we propose a clustering approach to obtain the landmark points. These new points are now used to represent the data, and Fisher’s linear discriminants are used for classification. The proposed method is tested with different datasets to verify the efficacy of the approach.","Clustering, Landmark isomap, Fisher’s linear discrimant analysis, Geodesic, Feature extraction, Manifolds, MDS",https://link.springer.com//article/10.1007/s00357-018-9285-7
Robustification of Gaussian Bayes Classifier by the Minimum β-Divergence Method,"The goal of classification is to classify new objects into one of the several known populations. A common problem in most of the existing classifiers is that they are very much sensitive to outliers. To overcome this problem, several author’s attempt to robustify some classifiers including Gaussian Bayes classifiers based on robust estimation of mean vectors and covariance matrices. However, these type of robust classifiers work well when only training datasets are contaminated by outliers. They produce misleading results like the traditional classifiers when the test data vectors are contaminated by outliers as well. Most of them also show weak performance if we gradually increase the number of variables in the dataset by fixing the sample size. As the remedies of these problems, an attempt is made to propose a highly robust Gaussian Bayes classifiers by the minimum β-divergence method. The performance of the proposed method depends on the value of tuning parameter β, initialization of Gaussian parameters, detection of outlying test vectors, and detection of their variable-wise outlying components. We have discussed some techniques in this paper to improve the performance of the proposed method by tackling these issues. The proposed classifier reduces to the MLE-based Gaussian Bayes classifier when β → 0. The performance of the proposed method is investigated using both synthetic and real datasets. It is observed that the proposed method improves the performance over the traditional and other robust linear classifiers in presence of outliers. Otherwise, it keeps equal performance.","Classification, Gaussian Bayes classifier, Minimum β-divergence estimators, β-weight function, Outlier detection, Robustness",https://link.springer.com//article/10.1007/s00357-019-9306-1
Multiclass Classification Based on Multi-criteria Decision-making,"Lots of real-world problems require multiclass classification. Since most general classification methods are originally introduced for binary problems (including two classes), they should be extended to multiclass problems. A solution proposed for multiclass problems is to decompose such problems to several binary ones and then combine the results obtained from smaller problems as a tree-based structure to obtain the final solution. In this study, a novel method which uses VlseKriterijumska optimizacija I Kompromisno Resenje multi-criteria decision-making was proposed to build the best directed binary tree with minimum error. The proposed method is independent of classifier; nevertheless, in the current experiments, the support vector machine was employed as the base classifier. The proposed method was tested on datasets and the results were compared with other methods. It can be seen that it improves precision of predictions significantly.","Multiclass classification, Multi-criteria decision-making, Hierarchical decomposition, Decision-making criterion",https://link.springer.com//article/10.1007/s00357-018-9286-6
A New Method for Classifying Random Variables Based on Support Vector Machine,"In this paper, a new version of Support Vector Machine (SVM) is proposed which any of training samples are considered the random variables. Hence, in order to achieve robustness, the constraint in SVM must be replaced with probability of constraint. In this new model, by applying the nonparametric statistical methods, we obtain the optimal separating hyperplane by solving a quadratic optimization problem. Afterwards, we present the least squares model of our proposed method. The efficiency of our proposed method is shown by several examples for both cases (linear and nonlinear) with probabilistic constraints.","Probabilistic constraints, Support Vector Machine, Least squares Support Vector Machine, Mathematical expectation, Plug-in estimator;Monte Carlo simulation",https://link.springer.com//article/10.1007/s00357-018-9282-x
Nonlinear Time Series Clustering Based on Kolmogorov-Smirnov 2D Statistic,"Time series clustering is to assign a set of time series into groups that share certain similarity. It has become an attractive analytic tool as many applications require such classifications. Clustering may also result in more accurate parameter estimates when a group of time series are assumed to share common models and parameters, especially for short panel time series. Many existing time series clustering methods are based on the assumption that the time series are linear. However, linearity assumptions often fail to hold. In this paper we consider the problem of clustering nonlinear time series. We propose the use of a two dimensional Kolmogorov-Smirnov statistic as a distance measure of two time series by measuring the affinity of nonlinear serial dependence structures. It is nonparametric in nature hence no model assumption are needed. The approach is illustrated with simulation studies as well as real data examples.","Cross validation, Dissimilarity measure, Hierarchical clustering, Generalized Ward’s linkage",https://link.springer.com//article/10.1007/s00357-018-9271-0
Risks of Classification of the Gaussian Markov Random Field Observations,"Given the spatial lattice endowed with particular neighborhood structure, the problem of classifying a scalar Gaussian Markov random field (GMRF) observation into one of two populations specified by different regression coefficients and special parametric covariance (precision) matrix is considered. Classification rule based on the plug-in Bayes discriminant function with inserted ML estimators of regression coefficients, spatial dependence and scale parameters is studied. The novel closed-form expression for the actual risk and the approximation of the expected risk (AER) associated with the aforementioned classifier are derived. This is the extension of the previous study of GMRF classification to the case of complete parametric uncertainty. Derived AER is used as the main performance measure for the considered classifier. GMRF sampled on a regular 2-dimensional unit spacing lattice endowed with neighborhood structure based on the Euclidean distance between sites is used for a simulation experiment. The sampling properties of ML estimators and the accuracy of the derived AER for various values of spatial dependence parameters and Mahalanobis distance are studied. The influence of the neighborhood size on the accuracy of the proposed AER is examined as well.","Bayes discriminant function, Training sample, Actual risk, Spatial weight",https://link.springer.com//article/10.1007/s00357-018-9269-7
Effects of Item Calibration Errors on Computerized Adaptive Testing under Cognitive Diagnosis Models,"In a cognitive diagnostic computerized adaptive testing (CD-CAT) exam, an item pool that consists of items with calibrated item parameters is used for item selection and attribute estimation. The parameter estimates for the items in the item pool are often treated as if they were the true population parameters, and therefore, the calibration errors are ignored. The purpose of this study was to investigate the effects of calibration errors on the attribute classification accuracy, the measurement precision of attribute mastery classification, and the test information under the log-linear cognitive diagnosis model (LCDM) framework. The deterministic input, noisy “and” gate (DINA) model and the compensatory re-parameterized unified model (C-RUM) were used in fixed-length CD-CAT simulations. The results showed that high levels of calibration errors were associated with low classification accuracy, low test information, and misleading estimation of measurement precision. The effects of calibration errors decreased as the test length increased, and the DINA model appeared to be more vulnerable in the presence of calibration errors. The C-RUM was less influenced by calibration errors because of its additive characteristics in the LCDM framework. The same conclusions applied when item exposure control was incorporated and when different item selection methods were used. Finally, the use of a larger calibration sample size to calibrate the item pool was found to reduce the magnitudes of error variances and increase the attribute classification accuracy.","Computerized adaptive testing, Cognitive diagnosis models, Calibration errors, Log-linear cognitive diagnosis model",https://link.springer.com//article/10.1007/s00357-018-9265-y
Adaptive Exponential Power Depth with Application to Classification,"Depth functions have many applications in multivariate data analysis, including discriminant analysis and classification. In this paper, we introduce a novel class of data depth: exponential power depth (EPD) functions. Under some conditions, we show that the EPD functions are a statistical depth function, and the sample EPD functions are consistent and asymptotically normal. Based on the proposed EPD functions, we construct a DD-plot (depth-versus-depth plot), which can be applied to the classification problem. Since the EPD functions contain the two tuning parameters, we provide a data-driven approach to select these tuning parameters. The simulation studies and two real data analysis are conducted to assess the finite sample performance of the proposed method.","Classification, Exponential power depth, Statistical depth function",https://link.springer.com//article/10.1007/s00357-018-9264-z
A Density-Sensitive Hierarchical Clustering Method,"We define a hierarchical clustering method: α-unchaining single linkage or SL(α). The input of this algorithm is a finite space with a distance function and a certain parameter α. This method is sensitive to the density of the distribution and offers some solution to the so-called chaining effect. We also define a modified version, SL*(α), to treat the chaining through points or small blocks. We study the theoretical properties of these methods and offer some theoretical background for the treatment of chaining effects.","Hierarchical clustering, Single linkage, Chaining effect, Weakly unchaining, α-bridge-unchaining",https://link.springer.com//article/10.1007/s00357-018-9266-x
A Novel Honey-Bees Mating Optimization Approach with Higher order Neural Network for Classification,"In the recent past, several biological and natural phenomena have extensively attracted researchers towards the rapid development of science and engineering. Basically solving the optimization problems in various Engineering discipline is a popular topic among the other problem solving strategies. Most of the biological processes include the swarm intelligence research areas where the activity and the behavior of real insects have been studied. One of the recently developed Swarm algorithms is the Honey Bee Mating Optimization (HBMO) algorithm which is based on the mating behavior of bees. In this work, a hybrid metaheuristic honey bee mating based Pi-Sigma Neural Network (PSNN) have been proposed to successfully solve the classification problem of data mining. The proposed approach combines HBMO with the PSNN and is compared with other techniques like GA (Genetic Algorithm), DE (Differential Evolution), and PSO (Particle Swarm Optimization). Experimental results reveal that the proposed approach is steady as well as reliable and provides better classification accuracy than others.","Honey bee mating optimization, Pi-sigma neural network, Higher order neural network, Nature inspired optimization algorithm",https://link.springer.com//article/10.1007/s00357-018-9270-1
Modeling Binary Time Series Using Gaussian Processes with Application to Predicting Sleep States,"Motivated by the problem of predicting sleep states, we develop a mixed effects model for binary time series with a stochastic component represented by a Gaussian process. The fixed component captures the effects of covariates on the binary-valued response. The Gaussian process captures the residual variations in the binary response that are not explained by covariates and past realizations. We develop a frequentist modeling framework that provides efficient inference and more accurate predictions. Results demonstrate the advantages of improved prediction rates over existing approaches such as logistic regression, generalized additive mixed model, models for ordinal data, gradient boosting, decision tree and random forest. Using our proposed model, we show that previous sleep state and heart rates are significant predictors for future sleep states. Simulation studies also show that our proposed method is promising and robust. To handle computational complexity, we utilize Laplace approximation, golden section search and successive parabolic interpolation. With this paper, we also submit an R-package (HIBITS) that implements the proposed procedure.","Binary time series, Classification, Gaussian process, Latent process, Sleep state",https://link.springer.com//article/10.1007/s00357-018-9268-8
On the Negative Bias of the Gini Coefficient due to Grouping,"The Gini coefficient is a measure of statistical dispersion that is commonly used as a measure of inequality of income, wealth or opportunity. Empirical research has shown that the coefficient may have a nonnegligible downward bias when data are grouped. It is unknown under which grouping conditions the downward bias occurs. In this note it is shown that the Gini coefficient strictly decreases if the data are partitioned into equal sized groups.","Statistical dispersion, Measure of inequality, Inequality of income, Inequality of wealth, Grouping data",https://link.springer.com//article/10.1007/s00357-018-9267-9
On the Interpretation of Ensemble Classifiers in Terms of Bayes Classifiers,"Many of the best classifiers are ensemble methods such as bagging, random forests, boosting, and Bayes model averaging. We give conditions under which each of these four classifiers can be regarded as a Bayes classifier. We also give conditions under which stacking achieves the minimal Bayes risk.We compare the four classifiers with a logistic regression classifier to assess the cost of interpretability. First we characterize the increase in risk from using an ensemble method in a logistic classifier versus using it directly. Second, we characterize the change in risk from applying logistic regression to an ensemble method versus using the logistic classifier itself. Third, we give necessary and sufficient conditions for the logistic classifier to be worse than combining the logistic classifier and the Bayes classifier. Hence these results extend to ensemble classifiers that are asymptotically Bayes.","Boosting, Random forest, Bagging, BMA, Bayes classifier, Stacking",https://link.springer.com//article/10.1007/s00357-018-9257-y
A Copula Based ICA Algorithm and Its Application to Time Series Clustering,"Independent component analysis (ICA) is a method to recover the original independent variables from the linear transformations of the observations. Most of ICA algorithms are formulated as an optimization of a contrast function which minimizes the cross-dependency among the components. In this paper, we propose an innovative algorithm for performing ICA problem which uses a contrast function based on the Hoeffding’s measure of pairwise dependence. This measure takes its minimum if and only if the random variables are independent, and takes its maximum if and only if one of the variable is a function of the other. Since the Hoeffding’s index is computed based on the rank values rather than the actual values of the data, it is significantly robust to the outliers and performs well even in the presence of noise. The proposed algorithm is evaluated using simulated data. The algorithm is utilized as a pre-processing method for clustering of trends in time series data. This pre-processing technique establish new components from original observations which have adequate information trend of time series. For illustrative purposes, the proposed methodology is applied to clustering of two real data sets involving financial time series.","Copula, Hoeffding Measure of dependence, Independent component analysis, Time series clustering",https://link.springer.com//article/10.1007/s00357-018-9258-x
Functional Sufficient Dimension Reduction for Functional Data Classification,"We consider two novel functional classification methods for binary response and functional predictor. We extend the most popular functional sufficient dimension reduction methods such as functional sliced inverse regression (FSIR) and functional sliced average variance estimation (FSAVE) by introducing a regularized estimation procedure and incorporating the localized information of the functional predictor in the analysis. Compared to the existing FSIR and FSAVE, the proposed methods are appealing because they are capable of estimating more than one effective dimension reduction direction, whereas FSIR detects only one such direction and FSAVE produces inefficient estimation in the case of binary response. Moreover, our methods make use of the localized information of the functional predictor, thereby more efficiently capturing the nonlinear relation between the binary response and the functional predictor. Furthermore, the proposed methods can be extended to incorporate the ancillary unlabeled data in semi-supervised learning. The empirical performance and the applications of the proposed methods are demonstrated by simulation studies and real applications.","Classification, Functional data, Localization, Sufficient dimension reduction",https://link.springer.com//article/10.1007/s00357-018-9256-z
How to Build a Complete Q-Matrix for a Cognitively Diagnostic Test,"The Q-matrix of a cognitively diagnostic test is said to be complete if it guarantees the identifiability of all possible proficiency classes among examinees. An incomplete Q-matrix causes examinees to be assigned to proficiency classes to which they do not belong. Completeness of the Q-matrix is therefore a key requirement of any cognitively diagnostic test. The importance of the completeness property of the Q-matrix of a test as a fundamental condition to guarantee a reliable estimate of an examinee’s attribute profile has only recently been realized by researchers. In fact, inspection of extant assessments based on the cognitive diagnosis framework often revealed that, in hindsight, the Q-matrices used with these tests were not complete. Thus, the availability of rules for building a complete Q-matrix at the early stages of test development is perhaps at least as desirable as rules for identifying the completeness of a given Q-matrix. This article presents procedures for constructing Q-matrices that are complete. The famous Fraction-Subtraction test problems by K. K. Tatsuoka (1984) are used throughout for illustration.","Q-matrix, Completeness, Cognitive Diagnosis, Diagnostic Classification Models (DCMs), General DCMs",https://link.springer.com//article/10.1007/s00357-018-9255-0
A New Chaotic Whale Optimization Algorithm for Features Selection,"The whale optimization algorithm (WOA) is a novel evolutionary algorithm inspired by the behavior of whales. Similar to other evolutionary algorithms, entrapment in local optima and slow convergence speed are two probable problems it encounters in solving challenging real applications. This paper presents a novel chaotic whale optimization algorithm (CWOA) to overcome these problems where chaotic search is embedded in the searching iterations of WOA. Ten chaotic maps are considered to improve the performance of WOA. Experiments on ten benchmark datasets show the novel CWOA is effective for selecting relevant features with a high classification performance and a small number of features. Additionally the performance of CWOA is compared with WOA and ten other optimization algorithms. The experimental results show that circle chaotic map is the best chaotic map to significantly boost the performance of WOA. Moreover, chaotic with modifications of exploration operators outperform the highest performance.","Whale optimization algorithm, Chaotic maps, Features selection",https://link.springer.com//article/10.1007/s00357-018-9261-2
A Comparative Study of Divisive and Agglomerative Hierarchical Clustering Algorithms,"A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps: first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the node levels of the resulting dendrogram. A set of 12 such algorithms is presented and compared to their agglomerative counterpart (when available). These algorithms are evaluated using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated with the given dissimilarities. Applied to a hundred random data tables and to three real life examples, these comparisons are in favor of methods which are based on unusual ratio-type formulas to evaluate the intermediate bipartitions, namely the Silhouette formula, the Dunn's formula and the Mollineda et al. formula. These formulas take into account both the within cluster and the between cluster mean dissimilarities. Their use in divisive algorithms performs very well and slightly better than in their agglomerative counterpart.","Hierarchical clustering, Dissimilarity data, Splitting procedures, Evaluation of hierarchy, Dendrogram, Ultrametrics",https://link.springer.com//article/10.1007/s00357-018-9259-9
Treelike Families of Multiweights,"Let T = (T,w) be a weighted finite tree with leaves 1, ..., n. For any I := {i1, ..., ik} ⊂ {1, ..., n}, let DI (T ) be the weight of the minimal subtree of T connecting i1, ..., ik; the DI (T ) are called k-weights of T . Given a family of real numbers parametrized by the k-subsets of \( \left\{1,\dots, n\right\},{\left\{{D}_I\right\}}_{I\in \left(\underset{k}{\left\{1,\dots, n\right\}}\right)}, \) we say that a weighted tree T = (T,w) with leaves 1, ..., n realizes the family if DI (T ) = DI for any I. Weighted graphs have applications in several disciplines, such as biology, archaeology, engineering, computer science, in fact, they can represent hydraulic webs, railway webs, computer networks...; moreover, in biology, weighted trees are used to represent the evolution of the species. In this paper we give a characterization of the families of real numbers parametrized by the k-subsets of some set that are realized by some weighted tree.","Weighted trees, Dissimilarity families",https://link.springer.com//article/10.1007/s00357-018-9260-3
Qualitative Judgement of Research Impact: Domain Taxonomy as a Fundamental Framework for Judgement of the Quality of Research,"The appeal of metric evaluation of research impact has attracted considerable interest in recent times. Although the public at large and administrative bodies are much interested in the idea, scientists and other researchers are much more cautious, insisting that metrics are but an auxiliary instrument to the qualitative peer-based judgement. The goal of this article is to propose availing of such a well positioned construct as domain taxonomy as a tool for directly assessing the scope and quality of research. We first show how taxonomies can be used to analyze the scope and perspectives of a set of research projects or papers. Then we proceed to define a research team or researcher’s rank by those nodes in the hierarchy that have been created or significantly transformed by the results of the researcher. An experimental test of the approach in the data analysis domain is described. Although the concept of taxonomy seems rather simplistic to describe all the richness of a research domain, its changes and use can be made transparent and subject to open discussions.","Research impact, Scientometrics, Stratification, Rank aggregation, Multicriteria decision making, Semantic analysis, Taxonomy",https://link.springer.com//article/10.1007/s00357-018-9247-0
Two-Stage Metropolis-Hastings for Tall Data,"This paper discusses the challenges presented by tall data problems associated with Bayesian classification (specifically binary classification) and the existing methods to handle them. Current methods include parallelizing the likelihood, subsampling, and consensus Monte Carlo. A new method based on the two-stage Metropolis-Hastings algorithm is also proposed. The purpose of this algorithm is to reduce the exact likelihood computational cost in the tall data situation. In the first stage, a new proposal is tested by the approximate likelihood based model. The full likelihood based posterior computation will be conducted only if the proposal passes the first stage screening. Furthermore, this method can be adopted into the consensus Monte Carlo framework. The two-stage method is applied to logistic regression, hierarchical logistic regression, and Bayesian multivariate adaptive regression splines.","Bayesian inference, Logistic model, Bayesian multivariate adaptive regression splines, Markov chain monte carlo, Metropolis-hastings algorithm, Tall data",https://link.springer.com//article/10.1007/s00357-018-9248-z
A New Support Vector Machine Plus with Pinball Loss,"The hinge loss support vector machine (SVM) is sensitive to outliers. This paper proposes a new support vector machine with a pinball loss function (PSVM+). The new model is less sensitive to noise, especially the feature noise around the decision boundary. Furthermore, the PSVM+ is more stable than the hinge loss support vector machine plus (SVM+) for re-sampling. It also embeds the additional information into the corresponding optimization problem, which is helpful to further improve the learning performance. Meanwhile, the computational complexity of the PSVM+ is similar to that of the SVM+.","Support vector machine, SVM+, Additional information, Re-sampling, Pinball loss",https://link.springer.com//article/10.1007/s00357-018-9249-y
The Hierarchical Spectral Merger Algorithm: A New Time Series Clustering Procedure,"We present a new method for time series clustering which we call the Hierarchical Spectral Merger (HSM) method. This procedure is based on the spectral theory of time series and identifies series that share similar oscillations or waveforms. The extent of similarity between a pair of time series is measured using the total variation distance between their estimated spectral densities. At each step of the algorithm, every time two clusters merge, a new spectral density is estimated using the whole information present in both clusters, which is representative of all the series in the new cluster. The method is implemented in an R package HSMClust. We present two applications of the HSM method, one to data coming from wave-height measurements in oceanography and the other to electroencefalogram (EEG) data.","Hierarchical spectral merger clustering: Time series clustering, Hierarchical clustering, Total variation distance, Time series, Spectral analysis",https://link.springer.com//article/10.1007/s00357-018-9250-5
Extended Box Clustering for Classification Problems,"In this work we address a technique, based on elementary convex sets called box hulls, for effectively grouping finite point sets into non-convex objects, called box clusters. The proposed clustering approach is based on homogeneity conditions, not according to some distance measure, and it is situated inside the theoretical framework of Supervised clustering. This approach extends the so-called (convex) box clustering, originally developed in the context of the logical analysis of data, to non-convex geometry. We briefly discuss the topological properties of these clusters and introduce a family of hypergraphs, called incompatibility hypergraphs; the main aim for these hypergraphs is their role in clustering algorithms, even if they have strong theoretical properties as shown in other works in literature. We also discuss of supervised classification problems and generalized Voronoi diagrams are considered to define a classifier based on box clusters. Finally, computational experiments on real world data are used to show the efficacy of our methods both in terms of clustering and accuracy.","Box clusters, Incompatibility hypergraphs, Box-based classifier, Voronoi diagrams",https://link.springer.com//article/10.1007/s00357-018-9253-2
A Multivariate Logistic Distance Model for the Analysis of Multiple Binary Responses,"We propose a Multivariate Logistic Distance (MLD) model for the analysis of multiple binary responses in the presence of predictors. The MLD model can be used to simultaneously assess the dimensional/factorial structure of the data and to study the effect of the predictor variables on each of the response variables. To enhance interpretation, the results of the proposed model can be graphically represented in a biplot, showing predictor variable axes, the categories of the response variables and the subjects’ positions. The interpretation of the biplot uses a distance rule. The MLD model belongs to the family of marginal models for multivariate responses, as opposed to latent variable models and conditionally specified models. By setting the distance between the two categories of every response variable to be equal, the MLD model becomes equivalent to a marginal model for multivariate binary data estimated using a GEE method. In that case the MLD model can be fitted using existing statistical packages with a GEE procedure, e.g., the genmod procedure from SAS or the geepack package from R. Without the equality constraint, the MLD model is a general model which can be fitted by its own right. We applied the proposed model to empirical data to illustrate its advantages.","Multivariate binary data, Biplots, Multidimensional scaling, Multidimensional unfolding, Marginal model, Clustered bootstrap, Generalized estimating equations",https://link.springer.com//article/10.1007/s00357-018-9251-4
The Lack of Cross-Validation Can Lead to Inflated Results and Spurious Conclusions: A Re-Analysis of the MacArthur Violence Risk Assessment Study,"Cross-validation is an important evaluation strategy in behavioral predictive modeling; without it, a predictive model is likely to be overly optimistic. Statistical methods have been developed that allow researchers to straightforwardly cross-validate predictive models by using the same data employed to construct the model. In the present study, cross-validation techniques were used to construct several decision-tree models with data from the MacArthur Violence Risk Assessment Study (Monahan et al., 2001). The models were then compared with the original (non-cross-validated) Classification of Violence Risk assessment tool. The results show that the measures of predictive model accuracy (AUC, misclassification error, sensitivity, specificity, positive and negative predictive values) degrade considerably when applied to a testing sample, compared with the training sample used to fit the model initially. In addition, unless false negatives (that is, incorrectly predicting individuals to be nonviolent) are considered more costly than false positives (that is, incorrectly predicting individuals to be violent), the models generally make few predictions of violence. The results suggest that employing cross-validation when constructing models can make an important contribution to increasing the reliability and replicability of psychological research.","Classification trees, Cross-validation, Replicability, Misclassification costs, Random forests, Violence prediction",https://link.springer.com//article/10.1007/s00357-018-9252-3
Latent Ignorability and Item Selection for Nursing Home Case-Mix Evaluation,"In the social, behavioral, and health sciences it is often of interest to identify latent or unobserved groups in the population with the group membership of the individuals depending on a set of observed variables. In particular, we focus on the field of nursing home assessment in which the response variables typically come from the administration of questionnaires made of categorical items. These types of data may suffer from missing values and the use of lengthy questionnaires may be problematic as a large number of items could have a negative impact on the responses. In such a context, we introduce an extended version of the Latent Class (LC) model aimed at dealing with missing values, by assuming a form of latent ignorability. Moreover, we propose an item selection algorithm, based on the LC model, for finding the smallest subset of items providing an amount of information close to that of the initial set. The proposed approach is illustrated through an application to a dataset collected within an Italian project on the quality-of-life of nursing home patients.","Expectation-Maximization algorithm, Missing responses, Polytomous items, Quality-of-life, ULISSE project",https://link.springer.com//article/10.1007/s00357-017-9227-9
On the Ultrametric Generated by Random Distribution of Points in Euclidean Spaces of Large Dimensions with Correlated Coordinates,Recently a general theorem stating that the matrix of normalized Euclidean distances on the set of specially distributed random points in the n-dimensional Euclidean space ℝn with independent coordinates converges in probability as n→∞ to the ultrametric matrix had been proved. The main theorem of the present paper extends this result to the case of weakly correlated coordinates of random points. Prior to formulating and stating this result we give two illustrative examples describing particular algorithms of generation of such nearly ultrametric spaces.,"Ultrametric space, High dimension data, Degree of ultrametricity, Law of large numbers",https://link.springer.com//article/10.1007/s00357-017-9236-8
Investigating the Performance of a Variation of Multiple Correspondence Analysis for Multiple Imputation in Categorical Data Sets,"Non-response in survey data, especially in multivariate categorical variables, is a common problem which often leads to invalid inferences and inefficient estimates. A regularized iterative multiple correspondence analysis (RIMCA) algorithm in single imputation (SI) has been suggested for the handling of missing categorical data in survey analysis. This paper proposes an adapted version of the SI algorithm for multiple imputation (MI). The SI and MI techniques are compared for both simulated and real questionnaire data. A comparison between RIMCA MI and Sequential Regression Multiple Imputation (SRMI) is shown to establish the success of the proposed MI procedure.","Incomplete categorical data, Multiple correspondence analysis, Multiple imputation, Principal component analysis, Regularized iterative multiple correspondence analysis",https://link.springer.com//article/10.1007/s00357-017-9238-6
Modeling Threshold Interaction Effects Through the Logistic Classification Trunk,"We introduce a model dealing with the identification of interaction effects in binary response data, which integrates recursive partitioning and generalized linear models. It derives from an ad-hoc specification and consequent implementation of the Simultaneous Threshold Interaction Modeling Algorithm (STIMA). The model, called Logistic Classification Trunk, allows us to obtain regression parameters by maximum likelihood through the simultaneous estimation of both main effects and threshold interaction effects. The main feature of this model is that it allows the user to evaluate a unique model and simultaneously the importance of both effects obtained by first growing a classification trunk and then by pruning it back to avoid overfitting. We investigate the choice of a suitable pruning parameter through a simulation study and compare the classification accuracy of the Logistic Classification Trunk with that of 13 alternative models/classifiers on 25 binary response datasets.","STIMA, Generalized linear modeling, Logistic Regression, Recursive partitioning, Interaction effects, Regression trunk",https://link.springer.com//article/10.1007/s00357-017-9241-y
Improving SVM Classification on Imbalanced Datasets by Introducing a New Bias,"Support Vector Machine (SVM) learning from imbalanced datasets, as well as most learning machines, can show poor performance on the minority class because SVMs were designed to induce a model based on the overall error. To improve their performance in these kind of problems, a low-cost post-processing strategy is proposed based on calculating a new bias to adjust the function learned by the SVM.The proposed bias will consider the proportional size between classes in order to improve performance on the minority class. This solution avoids not only introducing and tuning new parameters, but also modifying the standard optimization problem for SVM training.Experimental results on 34 datasets, with different degrees of imbalance, show that the proposed method actually improves the classification on imbalanced datasets, by using standardized error measures based on sensitivity and g-means. Furthermore, its performance is comparable to well-known cost-sensitive and Synthetic Minority Over-sampling Technique (SMOTE) schemes, without adding complexity or computational costs.","Support Vector Machine, Post-processing, Bias, Cost-sensitive strategy, SMOTE",https://link.springer.com//article/10.1007/s00357-017-9242-x
A Semiparametric and Location-Shift Copula-Based Mixture Model,"Modeling mixtures of distributions has rested on Gaussian distributions and/or a conditional independence hypothesis for a long time. Only recently have researchers begun to construct and study broader generic models without appealing to such hypotheses. Some of these extensions use copulas as a tool to build flexible models, as they permit modeling the dependence and the marginal distributions separately. But this approach also has drawbacks. First, the practitioner has to make more arbitrary choices, and second, marginal misspecification may loom on the horizon. This paper aims at overcoming these limitations by presenting a copulabased mixture model which is semiparametric. Thanks to a location-shift hypothesis, semiparametric estimation, also, is feasible, allowing for data adaptation without any modeling effort.","Location, Shift, Copula, Mixture, Clustering, Semiparametric, Nonparametric",https://link.springer.com//article/10.1007/s00357-017-9243-9
Exact One-Sided Bayes Factors for 2 by 2 Contingency Tables,"One-sided Bayes factors are obtained for the association in 2 by 2 tables, under three different sampling designs. These Bayes factors are relevant when one has a prior believe about the direction of an association. It is further shown how prior knowledge can be incorporated in determining the support of a positive association against the hypothesis of row-column independence.","2 by 2 tables, One-sided Bayes factors, Row-column independence",https://link.springer.com//article/10.1007/s00357-017-9244-8
"Analysis of Web Visit Histories, Part II: Predicting Navigation by Nested STUMP Regression Trees","This paper constitutes part II of the contribution to the analysis of web visit histories through a new methodological framework for web usage-structure mining considering association rules theory. The aim is to explore through a tree structure the sequence of direct rules (i.e. paths) that characterize a web navigator who keeps standing longer on a web page with respect to the path characterizing navigators who leave the web earlier. A novel tree-based structure is introduced to take into account that the learning sample changes click by click leaving out navigators who drop off from the web after any click. The response variable at each time point is the remaining number of clicks before leaving the web. The split is induced by the predictors that describe the preferred web sections. The methodology introduced results in a Nested Stump Regression Tree that is an hierarchy of stump trees, where a stump is a tree with only one split or, equivalently, with only two terminal nodes. Suitable properties are outlined. As in first part of the contribution to the analysis of the web visit histories, a methodological description is provided by considering a web portal with a fixed set of web sections, i.e. a data set coming from the UCI Machine Learning Repository.","Web path, Sequence rules, Recursive partitioning, Web Usage-Structure Mining",https://link.springer.com//article/10.1007/s00357-017-9239-5
Real-Time Detection of In-flight Aircraft Damage,"When there is damage to an aircraft, it is critical to be able to quickly detect and diagnose the problem so that the pilot can attempt to maintain control of the aircraft and land it safely. We develop methodology for real-time classification of flight trajectories to be able to distinguish between an undamaged aircraft and five different damage scenarios. Principal components analysis allows a lower-dimensional representation of multi-dimensional trajectory information in time. Random Forests provide a computationally efficient approach with sufficient accuracy to be able to detect and classify the different scenarios in real-time. We demonstrate our approach by classifying realizations of a 45 degree bank angle generated from the Generic Transport Model flight simulator in collaboration with NASA.","Ensemble learning, Sliding window, Aviation safety",https://link.springer.com//article/10.1007/s00357-017-9237-7
rCOSA: A Software Package for Clustering Objects on Subsets of Attributes,"rCOSA is a software package interfaced to the R language. It implements statistical techniques for clustering objects on subsets of attributes in multivariate data. The main output of COSA is a dissimilarity matrix that one can subsequently analyze with a variety of proximity analysis methods. Our package extends the original COSA software (Friedman and Meulman, 2004) by adding functions for hierarchical clustering methods, least squares multidimensional scaling, partitional clustering, and data visualization. In the many publications that cite the COSA paper by Friedman and Meulman (2004), the COSA program is actually used only a small number of times. This can be attributed to the fact that this original implementation is not very easy to install and use. Moreover, the available software is out-of-date. Here, we introduce an up-to-date software package and a clear guidance for this advanced technique. The software package and related links are available for free at: https://github.com/mkampert/rCOSA.","Distance-based clustering, Subsets of variables, Feature selection, Targeted clustering, Mixtures of numeric and categorical variables, Clustering in R, Multidimensional scaling, Proximities, Dissimilarities, Omics data",https://link.springer.com//article/10.1007/s00357-017-9240-z
On Strategies to Fix Degenerate k-means Solutions,"k-means is a benchmark algorithm used in cluster analysis. It belongs to the large category of heuristics based on location-allocation steps that alternately locate cluster centers and allocate data points to them until no further improvement is possible. Such heuristics are known to suffer from a phenomenon called degeneracy in which some of the clusters are empty. In this paper, we compare and propose a series of strategies to circumvent degenerate solutions during a k-means execution. Our computational experiments show that these strategies are effective, leading to better clustering solutions in the vast majority of the cases in which degeneracy appears in k-means. Moreover, we compare the use of our fixing strategies within k-means against the use of two initialization methods found in the literature. These results demonstrate how useful the proposed strategies can be, specially inside memorybased clustering algorithms.","k-means, Minimum sum-of-squares, Degeneracy, Clustering, Heuristics",https://link.springer.com//article/10.1007/s00357-017-9231-0
Robinsonian Matrices: Recognition Challenges,"Ultrametric inequality is involved in different operations on (dis)similarity matrices. Its coupling with a compatible ordering leads to nice interpretations in seriation problems. We accurately review the interval graph recognition problem for its tight connection with recognizing a dense Robinsonian dissimilarity (precisely, in the anti-ultrametric case). Since real life matrices are prone to errors or missing entries, we address the sparse case and make progress towards recognizing sparse Robinsonian dissimilarities with lexicographic breadth first search. The ultrametric inequality is considered from the same graph point of view and the intimate connection between cocomparability graph and dense Robinsonian similarity is established. The current trend in recognizing special graph structures is examined in regard to multiple lexicographic search sweeps. Teaching examples illustrate the issues addressed for both dense and sparse symmetric matrices.","Robinsonian matrices, Interval graphs, Cocomparability graphs, Lexicographic searches, Partition refinement",https://link.springer.com//article/10.1007/s00357-017-9230-1
The Effect of Model Misspecification on Growth Mixture Model Class Enumeration,"Multiple criteria have been proposed to aid in deciding how many latent classes to extract in growth mixture models; however, studies are just beginning to investigate the performance of these criteria under non-ideal conditions. We review these previous studies and conduct a simulation study to address the performance of fit criteria under two previously uninvestigated assumption violations: (1) linearity of covariates and (2) proper specification of the growth factor covariance matrix. Results show that, provided that estimation is carried out with a large number of random starts and final stage optimizations, BIC and the bootstrap likelihood ratio test perform exceedingly well at identifying whether the data are homogenous or whether latent classes may be present, even with misspecifications present. Results were far less favorable when software default estimation choices were selected. We discuss implications to empirical studies and speculate on the relation between estimation choices and fit criteria perform.","Growth mixture model, Class enumeration, Bootstrapped likelihood ratio test, Latent class, Misspecification",https://link.springer.com//article/10.1007/s00357-017-9233-y
Robust Clustering in Regression Analysis via the Contaminated Gaussian Cluster-Weighted Model,"The Gaussian cluster-weighted model (CWM) is a mixture of regression models with random covariates that allows for flexible clustering of a random vector composed of response variables and covariates. In each mixture component, a Gaussian distribution is adopted for both the covariates and the responses given the covariates. To make the approach robust with respect to the presence of mildly atypical observations, the contaminated Gaussian CWM is introduced. In addition to the parameters of the Gaussian CWM, each mixture component has a parameter controlling the proportion of outliers, one controlling the proportion of leverage points, one specifying the degree of contamination with respect to the response variables, and another specifying the degree of contamination with respect to the covariates. Crucially, these parameters do not have to be specified a priori, adding flexibility to the approach. Furthermore, once the model is estimated and the observations are assigned to the components, a finer intra-group classification in typical points, (mild) outliers, good leverage points, and bad leverage points—concepts of primary importance in robust regression analysis—can be directly obtained. Relations with other mixture-based contaminated models are analyzed, identifiability conditions are provided, an expectation-conditional maximization algorithm is outlined for parameter estimation, and various implementation and operational issues are discussed. Properties of the estimators of the regression coefficients are evaluated through Monte Carlo experiments and compared with other procedures. A sensitivity study is also conducted based on a real data set.","Mixture models, Cluster-weighted models, Model-based clustering, Contaminated Gaussian distribution, Robust regression",https://link.springer.com//article/10.1007/s00357-017-9234-x
Dimension-Reduced Clustering of Functional Data via Subspace Separation,"We propose a new method for finding an optimal cluster structure of functions as well as an optimal subspace for clustering simultaneously. The proposed method aims to minimize a distance between functional objects and their projections with the imposition of clustering penalties. It includes existing approaches to functional cluster analysis and dimension reduction, such as functional principal component k-means (Yamamoto, 2012) and functional factorial k-means (Yamamoto and Terada, 2014), as special cases. We show that these existing methods can perform poorly when a disturbing structure exists and that the proposed method can overcome this drawback by using subspace separation. A novel model selection procedure has been proposed, which can also be applied to other joint analyses of dimension reduction and clustering. We apply the proposed method to artificial and real data to demonstrate its performance as compared to the extant approaches.","Clustering, Dimension reduction, Multivariate functional data, Disturbing structure, Subspace separation",https://link.springer.com//article/10.1007/s00357-017-9232-z
Handling Missing Data in Item Response Theory. Assessing the Accuracy of a Multiple Imputation Procedure Based on Latent Class Analysis,"A critical issue in analyzing multi-item scales is missing data treatment. Previous studies on this topic in the framework of item response theory have shown that imputation procedures are in general associated with more accurate estimates of item location and discrimination parameters under several missing data generating mechanisms. This paper proposes a model-based multiple imputation procedure for multiple categorical items (dichotomous, multinomial or Likert-type) which relies on the results of latent class analysis to impute missing item responses. The effectiveness of the proposed technique is assessed in the estimation of item response theory parameters using a range of ad hoc measures. The accuracy of the method is assessed with respect to other single and multiple imputation procedures, under different missing data generating mechanisms and different rate of missingness (5% to 30%). The simulation results indicate that the proposed technique performs satisfactorily under all conditions and has the greatest potential with severe rates of missingness and under non ignorable missing data mechanisms. The method was implemented in R code with a function that calls scripts from a latent class analysis routine.","Item response theory, Multiple imputation analysis, Latent class analysis, Missingness, Accuracy measures",https://link.springer.com//article/10.1007/s00357-017-9220-3
Multivariate Response and Parsimony for Gaussian Cluster-Weighted Models,"A family of parsimonious Gaussian cluster-weighted models is presented. This family concerns a multivariate extension to cluster-weighted modelling that can account for correlations between multivariate responses. Parsimony is attained by constraining parts of an eigen-decomposition imposed on the component covariance matrices. A sufficient condition for identifiability is provided and an expectation-maximization algorithm is presented for parameter estimation. Model performance is investigated on both synthetic and classical real data sets and compared with some popular approaches. Finally, accounting for linear dependencies in the presence of a linear regression structure is shown to offer better performance, vis-à-vis clustering, over existing methodologies.","Cluster-weighted model, EM algorithm, Multivariate response, Modelbased clustering, Mixture models, Parsimonious models, Eigen-decomposition, Regression",https://link.springer.com//article/10.1007/s00357-017-9221-2
On the Correspondence Between Procrustes Analysis and Bidimensional Regression,"Procrustes analysis is defined as the problem of fitting a matrix of data to a target matrix as closely as possible (Gower and Dijksterhuis, 2004). The problem can take many forms, but the most common form, orthogonal Procrustes analysis, has as allowable transformations, a translation, a scaling, an orthogonal rotation, and a reflection. Procrustes analysis and other rotation methods have a long history in quantitative psychology, as well as in other fields, such as biology (Siegel and Benson, 1982) and shape analysis (Kendall, 1984). In the field of quantitative geography, the use of bidimensional regression (Tobler, 1965) has recently become popular. Tobler (1994) defines bidimensional regression as “an extension of ordinary regression to the case in which both the independent and dependent variables are two-dimensional.” In this paper, it is established that orthogonal Procrustes analysis (without reflection) and Euclidean bidimensional regression are the same. As such, both areas of development can borrow from the other, allowing for a richer landscape of possibilities.","Procrustes analysis, Bidimensional regression",https://link.springer.com//article/10.1007/s00357-017-9224-z
Maximal Interaction Two-Mode Clustering,"Most classical approaches for two-mode clustering of a data matrix are designed to attain homogeneous row by column clusters (blocks, biclusters), that is, biclusters with a small variation of data values within the blocks. In contrast, this article deals with methods that look for a biclustering with a large interaction between row and column clusters. Thereby an aggregated, condensed representation of the existing interaction structure is obtained, together with corresponding row and column clusters, which both allow a parsimonious visualization and interpretation. In this paper we provide a statistical justification, in terms of a probabilistic model, for a two-mode interaction clustering criterion that has been proposed by Bock (1980). Furthermore, we show that maximization of this criterion is equivalent to minimizing the classical least-squares two-mode partitioning criterion for the double-centered version of the data matrix. The latter implies that the interaction clustering criterion can be optimized by applying classical two-mode partitioning algorithms. We illustrate the usefulness of our approach for the case of an empirical data set from personality psychology and we compare this method with other biclustering approaches where interactions play a role.","Two-mode data, Biclustering, Capturing row by column interaction, Clustering criteria, Probabilistic clustering model, Classification likelihood",https://link.springer.com//article/10.1007/s00357-017-9226-x
Finding Ultrametricity in Data using Topology,The topological ultrametricity index can be approximated by the expected survival time of a dataset in the state of being ultrametric while only distances up to a given value are considered. It is observed that the quotient of the number of connected components by the number of maximal cliques in the Vietoris-Rips graph initially is the survival function of a Weibull distribution. This is shown for some codings of Fisher’s Iris data as well as for random samples in the Euclidean hypercube.,"Ultrametric, Topology, Weibull distribution, Vietoris-Rips graph",https://link.springer.com//article/10.1007/s00357-017-9228-8
Fusing Vantage Point Trees and Linear Discriminants for Fast Feature Classification,"This paper describes a classification strategy that can be regarded as a more general form of nearest-neighbor classification. It fuses the concepts of nearest neighbor, linear discriminant and Vantage-Point trees, yielding an efficient indexing data structure and classification algorithm. In the learning phase, we define a set of disjoint subspaces of reduced complexity that can be separated by linear discriminants, ending up with an ensemble of simple (weak) classifiers that work locally. In classification, the closest centroids to the query determine the set of classifiers considered, which responses are weighted. The algorithm was experimentally validated in datasets widely used in the field, attaining error rates that are favorably comparable to the state-of-the-art classification techniques. Lastly, the proposed solution has a set of interesting properties for a broad range of applications: 1) it is deterministic; 2) it classifies in time approximately logarithmic with respect to the size of the learning set, being far more efficient than nearest neighbor classification in terms of computational cost; and 3) it keeps the generalization ability of simple models.","Vantage-point tree, Linear discriminants, Nearest neighbor classification",https://link.springer.com//article/10.1007/s00357-017-9223-0
Versatile Hyper-Elliptic Clustering Approach for Streaming Data Based on One-Pass-Thrown-Away Learning,"Finding patterns or clusters in streaming data is very important in the present information mining. The most critical issue is the huge amount of data versus the limited size of storage space. In the previous works, the essential information of huge data was represented by subsets of data, grid summarization, or spherical function. Those forms of data representation are not compact enough to capture the topology of the arriving data points and may lead to the lack of information for generating the accurate cluster result. In this work, we proposed a new versatile hyper-elliptic clustering algorithm, called VHEC, to cluster the streaming data in one-pass-thrown-away fashion in order to preserve the original topology of data space. To cope with the problem of one-pass-thrown-away clustering, a new set of elliptic micro-cluster parameters, i.e. boundary, density, direction, intra-distance and inter-distance, was introduced. Furthermore, a feasible technique for merging two micro-clusters was developed. The proposed parameters and one-pass-throw-away clustering algorithm were tested against several benchmark data sets and structural clustering data sets. Our performance was compared with existing algorithms. Regardless of different sizes, shapes, and densities, VHEC outperformed the other previous data stream clustering algorithms on both synthetic and real data sets. Moreover, VHEC is more significantly robust to streaming speed and incoming data sequence than the other compared algorithms in terms of purity, Rand index, and adjusted Rand index measures.","Data stream clustering, Micro-cluster, One-pass-thrown-away learning",https://link.springer.com//article/10.1007/s00357-017-9222-1
Single-Valued Neutrosophic Clustering Algorithms Based on Similarity Measures,"Clustering plays an important role in data mining, pattern recognition, and machine learning. Then, single-valued neutrosophic sets (SVNSs) can describe and handle indeterminate and inconsistent information, while fuzzy sets and intuitionistic fuzzy sets cannot describe and deal with it. To cluster the information represented by single-valued neutrosophic data, this paper proposes single-valued neutrosophic clustering algorithms based on similarity measures of SVNSs. Firstly, we introduce a similarity measure between SVNSs based on the min and max operators and propose another new similarity measure between SVNSs. Then, we present clustering algorithms based on the similarity measures of SVNSs for the clustering analysis of single-valued neutrosophic data. Finally, an illustrative example is given to demonstrate the application and effectiveness of the single-valued neutrosophic clustering algorithms.","Single-valued neutrososophic set, Clustering algorithm, Similarity measure, Similarity matrix",https://link.springer.com//article/10.1007/s00357-017-9225-y
Model-Based Clustering,"The notion of defining a cluster as a component in a mixture model was put forth by Tiedeman in 1955; since then, the use of mixture models for clustering has grown into an important subfield of classification. Considering the volume of work within this field over the past decade, which seems equal to all of that which went before, a review of work to date is timely. First, the definition of a cluster is discussed and some historical context for model-based clustering is provided. Then, starting with Gaussian mixtures, the evolution of model-based clustering is traced, from the famous paper by Wolfe in 1965 to work that is currently available only in preprint form. This review ends with a look ahead to the next decade or so.","Cluster, Cluster analysis, Mixture models",https://link.springer.com//article/10.1007/s00357-016-9211-9
Piecewise Regression Mixture for Simultaneous Functional Data Clustering and Optimal Segmentation,"This paper introduces a novel mixture model-based approach to the simultaneous clustering and optimal segmentation of functional data, which are curves presenting regime changes. The proposed model consists of a finite mixture of piecewise polynomial regression models. Each piecewise polynomial regression model is associated with a cluster, and within each cluster, each piecewise polynomial component is associated with a regime (i.e., a segment). We derive two approaches to learning the model parameters: the first is an estimation approach which maximizes the observed-data likelihood via a dedicated expectation-maximization (EM) algorithm, then yielding a fuzzy partition of the curves into K clusters obtained at convergence by maximizing the posterior cluster probabilities. The second is a classification approach and optimizes a specific classification likelihood criterion through a dedicated classification expectation-maximization (CEM) algorithm. The optimal curve segmentation is performed by using dynamic programming. In the classification approach, both the curve clustering and the optimal segmentation are performed simultaneously as the CEM learning proceeds. We show that the classification approach is a probabilistic version generalizing the deterministic K-means-like algorithm proposed in Hébrail, Hugueney, Lechevallier, and Rossi (2010). The proposed approach is evaluated using simulated curves and real-world curves. Comparisons with alternatives including regression mixture models and the K-means-like algorithm for piecewise regression demonstrate the effectiveness of the proposed approach.","Model-based clustering, Functional data analysis, Optimal curve segmentation, Mixture models, Piecewise regression, EM algortihm, CEM algorithm.",https://link.springer.com//article/10.1007/s00357-016-9212-8
Finite Mixture Modeling of Gaussian Regression Time Series with Application to Dendrochronology,Finite mixture modeling is a popular statistical technique capable of accounting for various shapes in data. One popular application of mixture models is model-based clustering. This paper considers the problem of clustering regression autoregressive moving average time series. Two novel estimation procedures for the considered framework are developed. The first one yields the conditional maximum likelihood estimates which can be used in cases when the length of times series is substantial. Simple analytical expressions make fast parameter estimation possible. The second method incorporates the Kalman filter and yields the exact maximum likelihood estimates. The procedure for assessing variability in obtained estimates is discussed. We also show that the Bayesian information criterion can be successfully used to choose the optimal number of mixture components and correctly assess time series orders. The performance of the developed methodology is evaluated on simulation studies. An application to the analysis of tree ring data is thoroughly considered. The results are very promising as the proposed approach overcomes the limitations of other methods developed so far.,"Finite mixture models, Model-based clustering, EM algorithm, Kalman filter, Regression time series, Annual tree rings.",https://link.springer.com//article/10.1007/s00357-016-9216-4
Weighted Euclidean Biplots,"We construct a weighted Euclidean distance that approximates any distance or dissimilarity measure between individuals that is based on a rectangular cases-by-variables data matrix. In contrast to regular multidimensional scaling methods for dissimilarity data, our approach leads to biplots of individuals and variables while preserving all the good properties of dimension-reduction methods that are based on the singular-value decomposition. The main benefits are the decomposition of variance into components along principal axes, which provide the numerical diagnostics known as contributions, and the estimation of nonnegative weights for each variable. The idea is inspired by the distance functions used in correspondence analysis and in principal component analysis of standardized data, where the normalizations inherent in the distances can be considered as differential weighting of the variables. In weighted Euclidean biplots, we allow these weights to be unknown parameters, which are estimated from the data to maximize the fit to the chosen distances or dissimilarities. These weights are estimated using a majorization algorithm. Once this extra weight-estimation step is accomplished, the procedure follows the classical path in decomposing the matrix and displaying its rows and columns in biplots.","Biplot, Correspondence analysis, Distance, Majorization, Multidimensional scaling, Singular value decomposition, Weighted least squares",https://link.springer.com//article/10.1007/s00357-016-9213-7
Estimation of Generalized DINA Model with Order Restrictions,"Cognitive diagnostic models provide valuable information on whether a student has mastered each of the attributes a test intends to evaluate. Despite its generality, the generalized DINA model allows for the possibility of lower correct rates for students who master more attributes than those who know less. This paper considers the use of order-constrained parameter space of the G-DINA model to avoid such a counter-intuitive phenomenon and proposes two algorithms, the upward and downward methods, for parameter estimation. Through simulation studies, we compare the accuracy in parameter estimation and in classification of attribute patterns obtained from the proposed two algorithms and the current approach when the restricted parameter space is true. Our results show that the upward method performs the best among the three, and therefore it is recommended for estimation, regardless of the distribution of respondents’ attribute patterns, types of test items, and the sample size of the data.","Cognitive diagnostic model, G-DINA model, Order restrictions, Classification of attribute patterns.",https://link.springer.com//article/10.1007/s00357-016-9215-5
Fidelity-Commensurability Tradeoff in Joint Embedding of Disparate Dissimilarities,"In various data settings, it is necessary to compare observations from disparate data sources. We assume the data is in the dissimilarity representation (Pękalska and Duin, 2005) and investigate a joint embedding method (Priebe et al., 2013) that results in a commensurate representation of disparate dissimilarities. We further assume that there are “matched” observations from different conditions which can be considered to be highly similar, for the sake of inference. The joint embedding results in the joint optimization of fidelity (preservation of within-condition dissimilarities) and commensurability (preservation of between-condition dissimilarities between matched observations). We show that the tradeoff between these two criteria can be made explicit using weighted raw stress as the objective function for multidimensional scaling. In our investigations, we use a weight parameter, w, to control the tradeoff, and choose match detection as the inference task. Our results show weights that are optimal (with respect to the inference task) are different than equal weights for commensurability and fidelity and the proposed weighted embedding scheme provides significant improvements in statistical power.","Multidimensional scaling, Embedding of dissimilarities, Multimodal data fusion, Joint embedding, Inference.",https://link.springer.com//article/10.1007/s00357-016-9214-6
Kappa Coefficients for Circular Classifications,"Circular classifications are classification scales with categories that exhibit a certain periodicity. Since linear scales have endpoints, the standard weighted kappas used for linear scales are not appropriate for analyzing agreement between two circular classifications. A family of kappa coefficients for circular classifications is defined. The kappas differ only in one parameter. It is studied how the circular kappas are related and if the values of the circular kappas depend on the number of categories. It turns out that the values of the circular kappas can be strictly ordered in precisely two ways. The orderings suggest that the circular kappas are measuring the same thing, but to a different extent. If one accepts the use of magnitude guidelines, it is recommended to use stricter criteria for circular kappas that tend to produce higher values.","Cohen’s kappa, Weighted kappa, Inter-rater agreement, Linear scale, Circular scale.",https://link.springer.com//article/10.1007/s00357-016-9217-3
A Proof of the Duality of the DINA Model and the DINO Model,"The Deterministic Input Noisy Output “AND” gate (DINA) model and the Deterministic Input Noisy Output “OR” gate (DINO) model are two popular cognitive diagnosis models (CDMs) for educational assessment. They represent different views on how the mastery of cognitive skills and the probability of a correct item response are related. Recently, however, Liu, Xu, and Ying demonstrated that the DINO model and the DINA model share a “dual” relation. This means that one model can be expressed in terms of the other, and which of the two models is fitted to a given data set is essentially irrelevant because the results are identical. In this article, a proof of the duality of the DINA model and the DINO model is presented that is tailored to the form and parameterization of general CDMs that have become the new theoretical standard in cognitively diagnostic modeling.","Cognitive diagnosis, DINA model, DINO model, General cognitive diagnosis models",https://link.springer.com//article/10.1007/s00357-016-9202-x
On the Incommensurability Phenomenon,"Suppose that two large, multi-dimensional data sets are each noisy measurements of the same underlying random process, and principal components analysis is performed separately on the data sets to reduce their dimensionality. In some circumstances it may happen that the two lower-dimensional data sets have an inordinately large Procrustean fitting-error between them. The purpose of this manuscript is to quantify this “incommensurability phenomenon”. In particular, under specified conditions, the square Procrustean fitting-error of the two normalized lower-dimensional data sets is (asymptotically) a convex combination (via a correlation parameter) of the Hausdorff distance between the projection subspaces and the maximum possible value of the square Procrustean fitting-error for normalized data. We show how this gives rise to the incommensurability phenomenon, and we employ illustrative simulations and also use real data to explore how the incommensurability phenomenon may have an appreciable impact.","Incommensurability phenomenon, Procrustes fitting, Principal components analysis, Grassmannian, Hausdorff distance",https://link.springer.com//article/10.1007/s00357-016-9203-9
A Survey on Feature Weighting Based K-Means Algorithms,"In a real-world data set, there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process.With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analyzing some of the most popular, or innovative, feature weighting mechanisms based in K-Means.","Feature weighting, K-Means, Partitional clustering, Feature selection",https://link.springer.com//article/10.1007/s00357-016-9208-4
Improved Classification for Compositional Data Using the α-transformation,"In compositional data analysis, an observation is a vector containing nonnegative values, only the relative sizes of which are considered to be of interest. Without loss of generality, a compositional vector can be taken to be a vector of proportions that sum to one. Data of this type arise in many areas including geology, archaeology, biology, economics and political science. In this paper we investigate methods for classification of compositional data. Our approach centers on the idea of using the α-transformation to transform the data and then to classify the transformed data via regularized discriminant analysis and the k-nearest neighbors algorithm. Using the α-transformation generalizes two rival approaches in compositional data analysis, one (when α=1) that treats the data as though they were Euclidean, ignoring the compositional constraint, and another (when α = 0) that employs Aitchison’s centered log-ratio transformation. A numerical study with several real datasets shows that whether using α = 1 or α = 0 gives better classification performance depends on the dataset, and moreover that using an intermediate value of α can sometimes give better performance than using either 1 or 0.","Compositional data, Classification, α-transformation, α-metric, Jensen-Shannon divergence",https://link.springer.com//article/10.1007/s00357-016-9207-5
Design of Blurring Mean-Shift Algorithms for Data Classification,"The mean-shift algorithm is an iterative method of mode seeking and data clustering based on the kernel density estimator. The blurring mean-shift is an accelerated version which uses the original data only in the first step, then re-smoothes previous estimates. It converges to local centroids, but may suffer from problems of asymptotic bias, which fundamentally depend on the design of its smoothing components. This paper develops nearest-neighbor implementations and data-driven techniques of bandwidth selection, which enhance the clustering performance of the blurring method. These solutions can be applied to the whole class of mean-shift algorithms, including the iterative local mean method. Extended simulation experiments and applications to well known data-sets show the goodness of the blurring estimator with respect to other algorithms.","Bandwidth selection, Cluster stability, Kernel density, Image segmentation, Local means, Monotone convergence, Nearest neighbors",https://link.springer.com//article/10.1007/s00357-016-9205-7
Weighted Graphs with Distances in Given Ranges,"Let \( \mathcal{G} \) = (G,w) be a weighted simple finite connected graph, that is, let G be a simple finite connected graph endowed with a function w from the set of the edges of G to the set of real numbers. For any subgraph G′ of G, we define w(G′) to be the sum of the weights of the edges of G′. For any i, j vertices of G, we define D{i,j}(\( \mathcal{G} \)) to be the minimum of the weights of the simple paths of G joining i and j. The D{i,j}(\( \mathcal{G} \)) are called 2-weights of \( \mathcal{G} \). Weighted graphs and their reconstruction from 2-weights have applications in several disciplines, such as biology and psychology.Let \( {\left\{{m}_I\right\}}_{I\in \left(\frac{\left\{1,\dots, n\right\}}{2}\right)} \) and \( {\left\{{M}_I\right\}}_{I\in \left(\frac{\left\{1,\dots, n\right\}}{2}\right)} \) be two families of positive real numbers parametrized by the 2-subsets of {1, …, n} with mI ≤ MI for any I; we study when there exist a positive-weighted graph G and an n-subset {1, …, n} of the set of its vertices such that DI (\( \mathcal{G} \)) ∈ [mI,MI] for any \( I\in \left(\frac{\left\{1,\dots, n\right\}}{2}\right) \). Then we study the analogous problem for trees, both in the case of positive weights and in the case of general weights.","Weighted graphs, Distances, Range",https://link.springer.com//article/10.1007/s00357-016-9206-6
"Analysis of Web Visit Histories, Part I: Distance-Based Visualization of Sequence Rules","This paper constitutes Part I of the contribution to the analysis of web visit histories through a new methodological framework. Firstly, web usage and web structure mining are considered as an unique mining process to detect the latent structure of the web navigation across the web sections of a single portal. We extend association rules theory to web data defining new concepts of web (patterns) association and preference matrices, as well as of (indirect and direct) sequence rules. We identify the most significant rules, according to a multiple testing procedure. In the literature, web usage patterns can be visualized in no-distance-based graphs describing the navigation behavior across web pages with sequential arrows. In the following, we introduce a geometrical visualization of sequence rules at any click of the web navigation. In particular, we provide two distance-based visualization methods for the static analysis of all data tout court and the dynamic analysis to discover the most significant web paths click by click. A real world case study is considered throughout the methodological description.","Association rules, Sequence rules, Bonferroni inequality, Multidimensional scaling, Non-symmetric correspondence analysis",https://link.springer.com//article/10.1007/s00357-016-9204-8
The Classification Society’s Bibliography Over Four Decades: History and Content Analysis,"The Classification Literature Automated Search Service, an annual bibliography based on citation of one or more of a set of around 80 book or journal publications, ran from 1972 to 2012. We analyze here the years 1994 to 2012. The Classification Society’s Service, as it was termed, was produced by the Classification Society. In earlier decades it was distributed as a diskette or CD with the Journal of Classification. Among our findings are the following: an enormous increase in scholarly production in this area post approximately 2000; and another big increase in quantity of publications from approximately 2004. The over 93,000 bibliographic records used is the basis for determining the research disciplines that we analyze. We make all this data available for download, formatted in text and in XML, with an accompanying Apache Lucene/Solr search interface.","Cluster analysis, Data analysis, Data analytics, Correspondence analysis, Search, Retrieval, Bibliography.",https://link.springer.com//article/10.1007/s00357-016-9196-4
Power and Sample Size Computation for Wald Tests in Latent Class Models,"Latent class (LC) analysis is used by social, behavioral, and medical science researchers among others as a tool for clustering (or unsupervised classification) with categorical response variables, for analyzing the agreement between multiple raters, for evaluating the sensitivity and specificity of diagnostic tests in the absence of a gold standard, and for modeling heterogeneity in developmental trajectories. Despite the increased popularity of LC analysis, little is known about statistical power and required sample size in LC modeling. This paper shows how to perform power and sample size computations in LC models using Wald tests for the parameters describing association between the categorical latent variable and the response variables. Moreover, the design factors affecting the statistical power of these Wald tests are studied. More specifically, we show how design factors which are specific for LC analysis, such as the number of classes, the class proportions, and the number of response variables, affect the information matrix. The proposed power computation approach is illustrated using realistic scenarios for the design factors. A simulation study conducted to assess the performance of the proposed power analysis procedure shows that it performs well in all situations one may encounter in practice.","Latent class models, Sample size, Statistical power, Information matrix, Wald test, Design factor",https://link.springer.com//article/10.1007/s00357-016-9199-1
Divisive Latent Class Modeling as a Density Estimation Method for Categorical Data,"Traditionally latent class (LC) analysis is used by applied researchers as a tool for identifying substantively meaningful clusters. More recently, LC models have also been used as a density estimation tool for categorical variables. We introduce a divisive LC (DLC) model as a density estimation tool that may offer several advantages in comparison to a standard LC model. When using an LC model for density estimation, a considerable number of increasingly large LC models may have to be estimated before sufficient model-fit is achieved. A DLC model consists of a sequence of small LC models. Therefore, a DLC model can be estimated much faster and can easily utilize multiple processor cores, meaning that this model is more widely applicable and practical. In this study we describe the algorithm of fitting a DLC model, and discuss the various settings that indirectly influence the precision of a DLC model as a density estimation tool. These settings are illustrated using a synthetic data example, and the best performing algorithm is applied to a real-data example. The generated data example showed that, using specific decision rules, a DLC model is able to correctly model complex associations amongst categorical variables.","Latent class analysis, Categorical data, Mixture model, Density estimation, Divisive latent class model, Missing data, Multiple imputation.",https://link.springer.com//article/10.1007/s00357-016-9195-5
Simultaneous Predictive Gaussian Classifiers,"Gaussian distribution has for several decades been ubiquitous in the theory and practice of statistical classification. Despite the early proposals motivating the use of predictive inference to design a classifier, this approach has gained relatively little attention apart from certain specific applications, such as speech recognition where its optimality has been widely acknowledged. Here we examine statistical properties of different inductive classification rules under a generic Gaussian model and demonstrate the optimality of considering simultaneous classification of multiple samples under an attractive loss function. It is shown that the simpler independent classification of samples leads asymptotically to the same optimal rule as the simultaneous classifier when the amount of training data increases, if the dimensionality of the feature space is bounded in an appropriate manner. Numerical investigations suggest that the simultaneous predictive classifier can lead to higher classification accuracy than the independent rule in the low-dimensional case, whereas the simultaneous approach suffers more from noise when the dimensionality increases.","Bayesian modeling, Discriminant analysis, Inductive learning, Predictive inference, Probabilistic classification",https://link.springer.com//article/10.1007/s00357-016-9197-3
Contours and Tight Clusters,"Nested clusters arise independently in graph partitioning and in the study of contours. We take a step toward unifying these two instances of nested clusters. We show that the graph theoretical tight clusters introduced by Dress, Steel, Moulton and Wu in 2010 are a special case of nested clusters associated to contours.","Contours, Tight clusters.",https://link.springer.com//article/10.1007/s00357-016-9194-6
On the Properties of α-Unchaining Single Linkage Hierarchical Clustering,"In the election of a hierarchical clustering method, theoretic properties may give some insight to determine which method is the most suitable to treat a clustering problem. Herein, we study some basic properties of two hierarchical clustering methods: α-unchaining single linkage or SL(α) and a modified version of this one, SL∗(α). We compare the results with the properties satisfied by the classical linkage-based hierarchical clustering methods.","Hierarchical clustering, Single linkage, Chaining effect, Weakly unchaining, α-bridge-unchaining",https://link.springer.com//article/10.1007/s00357-016-9198-2
Inequalities Between Similarities for Numerical Data,Similarity measures are entities that can be used to quantify the similarity between two vectors with real numbers. We present inequalities between seven well known similarities. The inequalities are valid if the vectors contain non-negative real numbers.,"Bray-Curtis similarity, Ruzicka similarity, Ellenberg similarity, Gleason similarity",https://link.springer.com//article/10.1007/s00357-016-9200-z
A New Representation of Interval Symbolic Data and Its Application in Dynamic Clustering,"In this study, we consider the type of interval data summarizing the original samples (individuals) with classical point data. This type of interval data are termed interval symbolic data in a new research domain called, symbolic data analysis. Most of the existing research, such as the (centre, radius) and [lower boundary, upper boundary] representations, represent an interval using only the boundaries of the interval. However, these representations hold true only under the assumption that the individuals contained in the interval follow a uniform distribution. In practice, such representations may result in not only inconsistency with the facts, since the individuals are usually not uniformly distributed in many application aspects, but also information loss for not considering the point data within the intervals during the calculation. In this study, we propose a new representation of the interval symbolic data considering the point data contained in the intervals. Then we apply the city-block distance metric to the new representation and propose a dynamic clustering approach for interval symbolic data. A simulation experiment is conducted to evaluate the performance of our method. The results show that, when the individuals contained in the interval do not follow a uniform distribution, the proposed method significantly outperforms the Hausdorff and city-block distance based on traditional representation in the context of dynamic clustering. Finally, we give an application example on the automobile data set.","Interval data, Distribution, Symbolic data analysis, Dynamic clustering.",https://link.springer.com//article/10.1007/s00357-016-9193-7
Fractionally-Supervised Classification,"Traditionally, there are three species of classification: unsupervised, supervised, and semi-supervised. Supervised and semi-supervised classification differ by whether or not weight is given to unlabelled observations in the classification procedure. In unsupervised classification, or clustering, all observations are unlabeled and hence full weight is given to unlabelled observations. When some observations are unlabelled, it can be very difficult to a priori choose the optimal level of supervision, and the consequences of a sub-optimal choice can be non-trivial. A flexible fractionally-supervised approach to classification is introduced, where any level of supervision—ranging from unsupervised to supervised—can be attained. Our approach uses a weighted likelihood, wherein weights control the relative role that labelled and unlabelled data have in building a classifier. A comparison between our approach and the traditional species is presented using simulated and real data. Gaussian mixture models are used as a vehicle to illustrate our fractionally-supervised classification approach; however, it is broadly applicable and variations on the postulated model can be easily made.","Discriminant analysis, Finite mixture models, Fractionally-supervised classification, Model-based classification, Model-based clustering, Weighted likelihood",https://link.springer.com//article/10.1007/s00357-015-9188-9
The Analysis of Multivariate Data Using Semi-Definite Programming,"A model is presented for analyzing general multivariate data. The model puts as its prime objective the dimensionality reduction of the multivariate problem. The only requirement of the model is that the input data to the statistical analysis be a covariance matrix, a correlation matrix, or more generally a positive semi-definite matrix. The model is parameterized by a scale parameter and a shape parameter both of which take on non-negative values smaller than unity. We first prove a wellknown heuristic for minimizing rank and establish the conditions under which rank can be replaced with trace. This result allows us to solve our rank minimization problem as a Semi-Definite Programming (SDP) problem by a number of available solvers. We then apply the model to four case studies dealing with four well-known problems in multivariate analysis. The first problem is to determine the number of underlying factors in factor analysis (FA) or the number of retained components in principal component analysis (PCA). It is shown that our model determines the number of factors or components more efficiently than the commonly used methods. The second example deals with a problem that has received much attention in recent years due to its wide applications, and it concerns sparse principal components and variable selection in PCA. When applied to a data set known in the literature as the pitprop data, we see that our approach yields PCs with larger variances than PCs derived from other approaches. The third problem concerns sensitivity analysis of the multivariate models, a topic not widely researched in the sequel due to its difficulty. Finally, we apply the model to a difficult problem in PCA known as lack of scale invariance in the solutions of PCA. This is the problem that the solutions derived from analyzing the covariance matrix in PCA are generally different (and not linearly related to) the solutions derived from analyzing the correlation matrix. Using our model, we obtain the same solution whether we analyze the correlation matrix or the covariance matrix since the analysis utilizes only the signs of the correlations/covariances but not their values. This is where we introduce a new type of PCA, called Sign PCA, which we speculate on its applications in social sciences and other fields of science.","FA, PCA, SDP, Multivariate statistical analysis, Positive semi-definite, Kernel methods, Kernel trick, Sign PCA",https://link.springer.com//article/10.1007/s00357-015-9184-0
Bisecting K-Means and 1D Projection Divisive Clustering: A Unified Framework and Experimental Comparison,"The paper presents a least squares framework for divisive clustering. Two popular divisive clustering methods, Bisecting K-Means and Principal Direction Division, appear to be versions of the same least squares approach. The PDD recently has been enhanced with a stopping criterion taking into account the minima of the corresponding one-dimensional density function (dePDDP method). We extend this approach to Bisecting K-Means by projecting the data onto random directions and compare thus modified methods. It appears the dePDDP method is superior at datasets with relatively small numbers of clusters, whatever cluster intermix, whereas our version of Bisecting K-Means is superior at greater cluster numbers with noise entities added to the cluster structure.","Divisive clustering, Bisecting k-means, Split base decomposition, Uphierarchy, Principal directions, Random directions, Computational experiment, Cluster structure generator",https://link.springer.com//article/10.1007/s00357-015-9186-y
Affinity Propagation and Uncapacitated Facility Location Problems,"One of the most important distinctions that must be made in clustering research is the difference between models (or problems) and the methods for solving those problems. Nowhere is this more evident than with the evaluation of the popular affinity propagation algorithm (apcluster.m), which is a MATLAB implementation of a neural clustering method that has received significant attention in the biological sciences and other disciplines. Several authors have undertaken comparisons of apcluster.m with methods designed for models that fall within the class of uncapacitated facility location problems (UFLPs). These comparative models include the p-center (or K-center) model and, more importantly, the p-median (or K-median) model. The results across studies are conflicting and clouded by the fact that, although similar, the optimization model underlying apcluster.m is slightly different from the p-median model and appreciably different from the pcenter model. In this paper, we clarify that apcluster.m is actually a heuristic for a ‘maximization version’ of another model in the class of UFLPs, which is known as the simple plant location problem (SPLP). An exact method for the SPLP is described, and the apcluster.m program is compared to a fast heuristic procedure (sasplp.m) in both a simulation experiment and across numerous datasets from the literature. Although the exact method is the preferred approach when computationally feasible, both apcluster.m and sasplp.m are efficient and effective heuristic approaches, with the latter slightly outperforming the former in most instances.","Clustering, Exact algorithms, Heuristics, Simple plant location problem, Affinity propagation",https://link.springer.com//article/10.1007/s00357-015-9187-x
An Exact Algorithm for the Two-Mode KL-Means Partitioning Problem,"Two-mode partitioning applications are increasingly common in the physical and social sciences with a variety of models and methods spanning these applications. Two-mode KL-means partitioning (TMKLMP) is one type of two-mode partitioning model with a conceptual appeal that stems largely from the fact that it is a generalization of the ubiquitous (one-mode) K-means clustering problem. A number of heuristic methods have been proposed for TMKLMP, ranging from a two-mode version of the K-means heuristic to metaheuristic approaches based on simulated annealing, genetic algorithms, variable neighborhood search, fuzzy steps, and tabu search. We present an exact algorithm for TMKLMP based on branch-and-bound programming and demonstrate its utility for the clustering of brand switching, manufacturing cell formation, and journal citation data. Although the proposed branchand-bound algorithm does not obviate the need for approximation methods for large two-mode data sets, it does provide a first step in the development of methods that afford a guarantee of globally-optimal solutions for TMKLMP.","Two-mode partitioning, Exact algorithms, Branch-and-bound",https://link.springer.com//article/10.1007/s00357-015-9185-z
Discriminant Analysis of Interval Data: An Assessment of Parametric and Distance-Based Approaches,"Building on probabilistic models for interval-valued variables, parametric classification rules, based on Normal or Skew-Normal distributions, are derived for interval data. The performance of such rules is then compared with distancebased methods previously investigated. The results show that Gaussian parametric approaches outperform Skew-Normal parametric and distance-based ones in most conditions analyzed. In particular, with heterocedastic data a quadratic Gaussian rule always performs best. Moreover, restricted cases of the variance-covariance matrix lead to parsimonious rules which for small training samples in heterocedastic problems can outperform unrestricted quadratic rules, even in some cases where the model assumed by these rules is not true. These restrictions take into account the particular nature of interval data, where observations are defined by both MidPoints and Ranges, which may or may not be correlated. Under homocedastic conditions linear Gaussian rules are often the best rules, but distance-based methods may perform better in very specific conditions.","Discriminant analysis, Interval data, Parametric modelling of interval data, Symbolic Data Analysis",https://link.springer.com//article/10.1007/s00357-015-9189-8
Model-Based Clustering for Conditionally Correlated Categorical Data,"An extension of the latent class model is presented for clustering categorical data by relaxing the classical “class conditional independence assumption” of variables. This model consists in grouping the variables into inter-independent and intra-dependent blocks, in order to consider the main intra-class correlations. The dependency between variables grouped inside the same block of a class is taken into account by mixing two extreme distributions, which are respectively the independence and the maximum dependency. When the variables are dependent given the class, this approach is expected to reduce the biases of the latent class model. Indeed, it produces a meaningful dependency model with only a few additional parameters. The parameters are estimated, by maximum likelihood, by means of an EM algorithm. Moreover, a Gibbs sampler is used for model selection in order to overcome the computational intractability of the combinatorial problems involved by the block structure search. Two applications on medical and biological data sets show the relevance of this new model. The results strengthen the view that this model is meaningful and that it reduces the biases induced by the conditional independence assumption of the latent class model.","Categorical data, Clustering, Correlation, Expectation-Maximization algorithm, Gibbs sampler, Mixture model, Model selection.",https://link.springer.com//article/10.1007/s00357-015-9180-4
Kernel-Based Methods to Identify Overlapping Clusters with Linear and Nonlinear Boundaries,"Detecting overlapping structures and identifying non-linearly-separable clusters with complex shapes are two major issues in clustering. This paper presents two kernel based methods that produce overlapping clusters with both linear and nonlinear boundaries. To improve separability of input patterns, we used for both methods Mercer kernel technique. First, we propose Kernel Overlapping K-means I (KOKMI), a centroid based method, generalizing kernel K-means to produce nondisjoint clusters with nonlinear separations. Second, we propose Kernel Overlapping K-means II (KOKMII), a medoid based method improving the previous method in terms of efficiency and complexity. Experiments performed on non-linearly-separable and real multi-labeled data sets show that proposed learning methods outperform the existing ones.","Overlapping clustering, Non-disjoint clusters, Learning multi-labels, Kernel methods, Kernel K-means, Nonlinear separations, Non-linearly-separable clusters.",https://link.springer.com//article/10.1007/s00357-015-9181-3
Point Clustering via Voting Maximization,"In this paper, we propose an unsupervised point clustering framework. The goal is to cluster N given points into K clusters, so that similarities between objects in the same group are high while the similarities between objects in different groups are low. The point similarity is defined by a voting measure that takes into account the point distances. Using the voting formulation, the problem of clustering is reduced to the maximization of the sum of votes between the points of the same cluster. We have shown that the resulting clustering based on voting maximization has advantages concerning the cluster’s compactness, working well for clusters of different densities and/or sizes. In addition, the proposed scheme is able to detect outliers. Experimental results and comparisons to existing methods on real and synthetic datasets demonstrate the high performance and robustness of the proposed scheme.","Clustering, Partition, Voting, K-means, Partitional clustering.",https://link.springer.com//article/10.1007/s00357-015-9182-2
TOBAE: A Density-based Agglomerative Clustering Algorithm,"This paper presents a novel density based agglomerative clustering algorithm named TOBAE which is a parameter-less algorithm and automatically filters noise. It finds the appropriate number of clusters while giving a competitive running time. TOBAE works by tracking the cumulative density distribution of the data points on a grid and only requires the original data set as input. The clustering problem is solved by automatically finding the optimal density threshold for the clusters. It is applicable to any N-dimensional data set which makes it highly relevant for real world scenarios. The algorithm outperforms state of the art clustering algorithms by the additional feature of automatic noise filtration around clusters. The concept behind the algorithm is explained using the analogy of puddles (’tobae’), which the algorithm is inspired from. This paper provides a detailed algorithm for TOBAE along with the complexity analysis for both time and space. We show experimental results against known data sets and show how TOBAE competes with the best algorithms in the field while providing its own set of advantages.","Clustering, Agglomerative, Density distribution, Automatic, Noise removal, Non-parametric, Filtering, Terrain, Water puddles, Density threshold.",https://link.springer.com//article/10.1007/s00357-015-9166-2
On the Added Value of Bootstrap Analysis for K-Means Clustering,"Because of its deterministic nature, K-means does not yield confidence information about centroids and estimated cluster memberships, although this could be useful for inferential purposes. In this paper we propose to arrive at such information by means of a non-parametric bootstrap procedure, the performance of which is tested in an extensive simulation study. Results show that the coverage of hyper-ellipsoid bootstrap confidence regions for the centroids is in general close to the nominal coverage probability. For the cluster memberships, we found that probabilistic membership information derived from the bootstrap analysis can be used to improve the cluster assignment of individual objects, albeit only in the case of a very large number of clusters. However, in the case of smaller numbers of clusters, the probabilistic membership information still appeared to be useful as it indicates for which objects the cluster assignment resulting from the analysis of the original data is likely to be correct; hence, this information can be used to construct a partial clustering in which the latter objects only are assigned to clusters.","K-means, Bootstrapping, Clustering",https://link.springer.com//article/10.1007/s00357-015-9178-y
DESPOTA: DEndrogram Slicing through a PemutatiOn Test Approach,"Hierarchical clustering represents one of the most widespread analytical approaches to tackle classification problems mainly due to the visual powerfulness of the associated graphical representation, the dendrogram. That said, the requirement of appropriately choosing the number of clusters still represents the main difficulty for the final user. We introduce DESPOTA (DEndrogram Slicing through a PermutatiOn Test Approach), a novel approach exploiting permutation tests in order to automatically detect a partition among those embedded in a dendrogram. Unlike the traditional approach, DESPOTA includes in the search space also partitions not corresponding to horizontal cuts of the dendrogram. Applications on both real and syntethic datasets will show the effectiveness of our proposal.","Hierarchical clustering, Cluster detection, Permutation tests",https://link.springer.com//article/10.1007/s00357-015-9179-x
Classification Using the Zipfian Kernel,"We propose to use the Zipfian distribution as a kernel for the design of a nonparametric classifier in contrast to the Gaussian distribution used in most kernel methods. We show that the Zipfian distribution takes into account multifractal nature of data and gives a true picture of scaling properties inherent in data. We also show that this new look at data structure can lead to a simple classifier that can, for some tasks, outperform more complex systems.","Kernel machine, Zipfian kernel, Multivariate data, Correlation dimension, Harmonic series, Classification",https://link.springer.com//article/10.1007/s00357-015-9174-2
Erratum to: The Generalized Linear Mixed Cluster-Weighted Model,"Cluster-weighted models (CWMs) are a flexible family of mixture models for fitting the joint distribution of a random vector composed of a response variable and a set of covariates. CWMs act as a convex combination of the products of the marginal distribution of the covariates and the conditional distribution of the response given the covariates. In this paper, we introduce a broad family of CWMs in which the component conditional distributions are assumed to belong to the exponential family and the covariates are allowed to be of mixed-type. Under the assumption of Gaussian covariates, sufficient conditions for model identifiability are provided. Moreover, maximum likelihood parameter estimates are derived using the EM algorithm. Parameter recovery, classification assessment, and performance of some information criteria are investigated through a broad simulation design. An application to real data is finally presented, with the proposed model outperforming other well-established mixture-based approaches.","Cluster-weighted models, Model-based clustering, Generalized linear models, Mixed-type data",https://link.springer.com//article/10.1007/s00357-015-9177-z
Shuffled Graph Classification: Theory and Connectome Applications,"We develop a formalism to address statistical pattern recognition of graph valued data. Of particular interest is the case of all graphs having the same number of uniquely labeled vertices. When the vertex labels are latent, such graphs are called shuffled graphs. Our formalism provides insight to trivially answer a number of open statistical questions including: (i) under what conditions does shuffling the vertices degrade classification performance and (ii) do universally consistent graph classifiers exist? The answers to these questions lead to practical heuristic algorithms with state-of-the-art finite sample performance, in agreement with our theoretical asymptotics. Applying these methods to classify sex and autism in two different human connectome classification tasks yields successful classification results in both applications.","Statistical pattern recognition, Random graphs, Graph matching, Connectomics",https://link.springer.com//article/10.1007/s00357-015-9170-6
Influence Measures for CART Classification Trees,"This paper deals with measuring the influence of observations on the results obtained with CART classification trees. To define the influence of individuals on the analysis, we use influence measures to propose criterions to quantify the sensitivity of the CART classification tree analysis. The proposals are based on predictions and use jackknife trees. The analysis is extended to the pruned sequences of CART trees to produce CART specific notions of influence. Using the framework of influence functions, distributional results are derived.A numerical example, the well known spam dataset, is presented to illustrate the notions developed throughout the paper. A real dataset relating the administrative classification of cities surrounding Paris, France, to the characteristics of their tax revenues distribution, is finally analyzed using the new influence-based tools.","Influential individuals, Influence functions, Decision trees, CART.",https://link.springer.com//article/10.1007/s00357-015-9172-4
Feature Relevance in Ward’s Hierarchical Clustering Using the Lp Norm,"In this paper we introduce a new hierarchical clustering algorithm called Wardp. Unlike the original Ward, Wardp generates feature weights, which can be seen as feature rescaling factors thanks to the use of the Lp norm. The feature weights are cluster dependent, allowing a feature to have different degrees of relevance at different clusters.We validate our method by performing experiments on a total of 75 real-world and synthetic datasets, with and without added features made of uniformly random noise. Our experiments show that: (i) the use of our feature weighting method produces results that are superior to those produced by the original Ward method on datasets containing noise features; (ii) it is indeed possible to estimate a good exponent p under a totally unsupervised framework. The clusterings produced by Wardp are dependent on p. This makes the estimation of a good value for this exponent a requirement for this algorithm, and indeed for any other also based on the Lp norm.","Ward method, Hierarchical clustering, Feature weights, Feature relevance, Lp norm, Minkowski metric.",https://link.springer.com//article/10.1007/s00357-015-9167-1
Outlier Identification in Model-Based Cluster Analysis,"In model-based clustering based on normal-mixture models, a few outlying observations can influence the cluster structure and number. This paper develops a method to identify these, however it does not attempt to identify clusters amidst a large field of noisy observations. We identify outliers as those observations in a cluster with minimal membership proportion or for which the cluster-specific variance with and without the observation is very different. Results from a simulation study demonstrate the ability of our method to detect true outliers without falsely identifying many non-outliers and improved performance over other approaches, under most scenarios. We use the contributed R package MCLUST for model-based clustering, but propose a modified prior for the cluster-specific variance which avoids degeneracies in estimation procedures. We also compare results from our outlier method to published results on National Hockey League data.","Normal-mixture models, Influential points, MCLUST, Prior, National Hockey League.",https://link.springer.com//article/10.1007/s00357-015-9171-5
The Generalized Linear Mixed Cluster-Weighted Model,"Cluster-weighted models (CWMs) are a flexible family of mixture models for fitting the joint distribution of a random vector composed of a response variable and a set of covariates. CWMs act as a convex combination of the products of the marginal distribution of the covariates and the conditional distribution of the response given the covariates. In this paper, we introduce a broad family of CWMs in which the component conditional distributions are assumed to belong to the exponential family and the covariates are allowed to be of mixed-type. Under the assumption of Gaussian covariates, sufficient conditions for model identifiability are provided. Moreover, maximum likelihood parameter estimates are derived using the EM algorithm. Parameter recovery, classification assessment, and performance of some information criteria are investigated through a broad simulation design. An application to real data is finally presented, with the proposed model outperforming other well-established mixture-based approaches.","Cluster-weighted models, Model-based clustering, Generalized linear models, Mixed-type data",https://link.springer.com//article/10.1007/s00357-015-9175-1
A Note on Maximizing the Agreement Between Partitions: A Stepwise Optimal Algorithm and Some Properties,"Building on Brusco and Steinley (2008), a computationally efficient stepwise optimal heuristic is provided for maximizing the adjusted Rand index (Hubert and Arabie 1985). The proposed algorithm is different than other methods for estimating the maximum value for the adjusted Rand index (e.g., Messatfa 1992) in that it does not rely on mathematical programming; consequently, problems of much larger size can be handled. Using the proposed method, various characteristics of the adjusted Rand index are explored and presented.","Heuristic Algorithm, Total Sample Size, Monte Carlo Study, Rand Index, Hypergeometric Distribution",https://link.springer.com//article/10.1007/s00357-015-9169-z
Some Relationships Between Cronbach’s Alpha and the Spearman-Brown Formula,"Cronbach’s alpha is an estimate of the reliability of a test score if the items are essentially tau-equivalent. Several authors have derived results that provide alternative interpretations of alpha. These interpretations are also valid if essential tau-equivalency does not hold. For example, alpha is the mean of all split-half reliabilities if the test is split into two halves that are equal in size. This note presents several connections between Cronbach’s alpha and the Spearman-Brown formula. The results provide new interpretations of Cronbach’s alpha, the stepped down alpha, and standardized alpha, that are also valid in the case that essential tau-equivalency or parallel equivalency do not hold. The main result is that the stepped down alpha is a weighted average of the alphas of all subtests of a specific size, where the weights are the denominators of the subtest alphas. Thus, the stepped down alpha can be interpreted as an average subtest alpha. Furthermore, we may calculate the stepped down alpha without using the Spearman-Brown formula.","Stepped up alpha, Stepped down alpha, Standardized alpha, Reliability, Coefficient alpha, Psychometrics.",https://link.springer.com//article/10.1007/s00357-015-9168-0
Ward’s Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward’s Criterion?,"The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. Two algorithms are found in the literature and software, both announcing that they implement the Ward clustering method. When applied to the same distance matrix, they produce different results. One algorithm preserves Ward’s criterion, the other does not. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward’s hierarchical clustering method.","Hierarchical clustering, Ward, Lance-Williams, Minimum variance, Statistical software",https://link.springer.com//article/10.1007/s00357-014-9161-z
Functional PCA and Base-Line Logit Models,"In many statistical applications data are curves measured as functions of a continuous parameter as time. Despite of their functional nature and due to discrete-time observation, these type of data are usually analyzed with multivariate statistical methods that do not take into account the high correlation between observations of a single curve at nearby time points. Functional data analysis methodologies have been developed to solve these type of problems. In order to predict the class membership (multi-category response variable) associated to an observed curve (functional data), a functional generalized logit model is proposed. Base-line category logit formulations will be considered and their estimation based on basis expansions of the sample curves of the functional predictor and parameters. Functional principal component analysis will be used to get an accurate estimation of the functional parameters and to classify sample curves in the categories of the response variable. The good performance of the proposed methodology will be studied by developing an experimental study with simulated and real data.","Functional data analysis, Nominal logit regression, Principal components",https://link.springer.com//article/10.1007/s00357-014-9162-y
Robust Functional Supervised Classification for Time Series,"We propose using the integrated periodogram to classify time series. The method assigns a new time series to the group that minimizes the distance between the series integrated periodogram and the group mean of integrated periodograms. Local computation of these periodograms allows the application of this approach to nonstationary time series. Since the integrated periodograms are curves, we apply functional data depth-based techniques to make the classification robust, which is a clear advantage over other competitive procedures. The method provides small error rates for both simulated and real data. It improves existing approaches and presents good computational behavior.","Time series, Supervised classification, Integrated periodogram, Functional data depth",https://link.springer.com//article/10.1007/s00357-014-9163-x
An Optimal Algorithm To Recognize Robinsonian Dissimilarities,"A dissimilarity D on a finite set S is said to be Robinsonian if S can be totally ordered in such a way that, for every i < j < k, D (i, j) ≤ D (i, k) and D (j, k) ≤ D (i, k). Intuitively, D is Robinsonian if S can be represented by points on a line. Recognizing Robinsonian dissimilarities has many applications in seriation and classification. In this paper, we present an optimal O (n2) algorithm to recognize Robinsonian dissimilarities, where n is the cardinal of S. Our result improves the already known algorithms.","Robinsonian dissimilarities, Classification, Seriation, Interval graphs, PQ-Trees, Consecutive One’s Property, Partition refinement",https://link.springer.com//article/10.1007/s00357-014-9150-2
Classification of Asymmetric Proximity Data,"When clustering asymmetric proximity data, only the average amounts are often considered by assuming that the asymmetry is due to noise. But when the asymmetry is structural, as typically may happen for exchange flows, migration data or confusion data, this may strongly affect the search for the groups because the directions of the exchanges are ignored and not integrated in the clustering process. The clustering model proposed here relies on the decomposition of the asymmetric dissimilarity matrix into symmetric and skew-symmetric effects both decomposed in within and between cluster effects. The classification structures used here are generally based on two different partitions of the objects fitted to the symmetric and the skew-symmetric part of the data, respectively; the restricted case is also presented where the partition fits jointly both of them allowing for clusters of objects similar with respect to the average amounts and directions of the data. Parsimonious models are presented which allow for effective and simple graphical representations of the results.","Asymmetric dissimilarities, Partition, Skew-Symmetric matrix, Least-Squares",https://link.springer.com//article/10.1007/s00357-014-9159-6
Variable Selection for Clustering and Classification,"As data sets continue to grow in size and complexity, effective and efficient techniques are needed to target important features in the variable space. Many of the variable selection techniques that are commonly used alongside clustering algorithms are based upon determining the best variable subspace according to model fitting in a stepwise manner. These techniques are often computationally intensive and can require extended periods of time to run; in fact, some are prohibitively computationally expensive for high-dimensional data. In this paper, a novel variable selection technique is introduced for use in clustering and classification analyses that is both intuitive and computationally efficient. We focus largely on applications in mixture model-based learning, but the technique could be adapted for use with various other clustering/classification methods. Our approach is illustrated on both simulated and real data, highlighted by contrasting its performance with that of other comparable variable selection techniques on the real data sets.","Classification, Cluster analysis, High-dimensional data, Mixture models, Model-based clustering, Variable selection",https://link.springer.com//article/10.1007/s00357-013-9139-2
A Run Length Transformation for Discriminating Between Auto Regressive Time Series,"We describe a simple time series transformation to detect differences in series that can be accurately modelled as stationary autoregressive (AR) processes. The transformation involves forming the histogram of above and below the mean run lengths. The run length (RL) transformation has the benefits of being very fast, compact and updatable for new data in constant time. Furthermore, it can be generated directly from data that has already been highly compressed. We first establish the theoretical asymptotic relationship between run length distributions and AR models through consideration of the zero crossing probability and the distribution of runs. We benchmark our transformation against two alternatives: the truncated Autocorrelation function (ACF) transform and the AR transformation, which involves the standard method of fitting the partial autocorrelation coefficients with the Durbin-Levinson recursions and using the Akaike Information Criterion stopping procedure. Whilst optimal in the idealized scenario, representing the data in these ways is time consuming and the representation cannot be updated online for new data. We show that for classification problems the accuracy obtained through using the run length distribution tends towards that obtained from using the full fitted models. We then propose three alternative distance measures for run length distributions based on Gower’s general similarity coefficient, the likelihood ratio and dynamic time warping (DTW). Through simulated classification experiments we show that a nearest neighbour distance based on DTW converges to the optimal faster than classifiers based on Euclidean distance, Gower’s coefficient and the likelihood ratio. We experiment with a variety of classifiers and demonstrate that although the RL transform requires more data than the best performing classifier to achieve the same accuracy as AR or ACF, this factor is at worst non-increasing with the series length, m, whereas the relative time taken to fit AR and ACF increases with m. We conclude that if the data is stationary and can be suitably modelled by an AR series, and if time is an important factor in reaching a discriminatory decision, then the run length distribution transform is a simple and effective transformation to use.","Time series classification, Run length distribution, Auto regressive model approximation",https://link.springer.com//article/10.1007/s00357-013-9135-6
Corrected Zegers-ten Berge Coefficients Are Special Cases of Cohen’s Weighted Kappa,"It is shown that if cell weights may be calculated from the data the chance-corrected Zegers-ten Berge coefficients for metric scales are special cases of Cohen’s weighted kappa. The corrected coefficients include Pearson’s product-moment correlation, Spearman’s rank correlation and the intraclass correlation ICC(3, 1).","Inter-rater reliability, Inter-rater agreement, Cohen’s kappa, Cohen’s weighted kappa, Product-moment correlation, Intraclass correlation, ICC(2, 1), ICC(3,1), Spearman’s rank correlation",https://link.springer.com//article/10.1007/s00357-014-9156-9
Minkowski Generalizations of Ward’s Method in Hierarchical Clustering,"In this paper, we consider several generalizations of the popular Ward’s method for agglomerative hierarchical clustering. Our work was motivated by clustering software, such as the R function hclust, which accepts a distance matrix as input and applies Ward’s definition of inter-cluster distance to produce a clustering. The standard version of Ward’s method uses squared Euclidean distance to form the distance matrix. We explore the effect on the clustering of using other definitions of distance, such as the Minkowski distance.","Distance matrix, Ward’s method, Minkowski distance",https://link.springer.com//article/10.1007/s00357-014-9157-8
"Globally Optimal Clusterwise Regression By Column Generation Enhanced with Heuristics, Sequencing and Ending Subset Optimization","A column generation based approach is proposed for solving the cluster-wise regression problem. The proposed strategy relies firstly on several efficient heuristic strategies to insert columns into the restricted master problem. If these heuristics fail to identify an improving column, an exhaustive search is performed starting with incrementally larger ending subsets, all the while iteratively performing heuristic optimization to ensure a proper balance of exact and heuristic optimization. Additionally, observations are sequenced by their dual variables and by their inclusion in joint pair branching rules. The proposed strategy is shown to outperform the best known alternative (BBHSE) when the number of clusters is greater than three. Additionally, the current work further demonstrates and expands the successful use of the new paradigm of using incrementally larger ending subsets to strengthen the lower bounds of a branch and bound search as pioneered by Brusco's Repetitive Branch and Bound Algorithm (RBBA).","Global optimization, Combinatorial optimization, Clusterwise regression, Column generation, Branch and bound, Sequencing, Heuristics",https://link.springer.com//article/10.1007/s00357-014-9155-x
Using Generalized Procrustes Analysis for Multiple Imputation in Principal Component Analysis,"Multiple imputation is one of the most highly recommended procedures for dealing with missing data. However, to date little attention has been paid to methods for combining the results from principal component analyses applied to a multiply imputed data set. In this paper we propose Generalized Procrustes analysis for this purpose, of which its centroid solution can be used as a final estimate for the component loadings. Convex hulls based on the loadings of the imputed data sets can be used to represent the uncertainty due to the missing data. In two simulation studies, the performance of Generalized Procrustes approach is evaluated and compared with other methods. More specifically it is studied how these methods behave when order changes of components and sign reversals of component loadings occur, such as in case of near-equal eigenvalues, or data having almost as many counterindicative items as indicative items. The simulations show that other proposed methods either may run into serious problems or are not able to adequately assess the accuracy due to the presence of missing data. However, when the above situations do not occur, all methods will provide adequate estimates for the PCA loadings.","Convex hulls, Missing data, Multiple imputation, Principal component analysis, Generalized Procrustes analysis, Questionnaires",https://link.springer.com//article/10.1007/s00357-014-9154-y
"Canonical Analysis: Ranks, Ratios and Fits","Measurements of p variables for n samples are collected into a n×p matrix X, where the samples belong to one of k groups. The group means are separated by Mahalanobis distances. CVA optimally represents the group means of X in an r-dimensional space. This can be done by maximizing a ratio criterion (basically one- dimensional) or, more flexibly, by minimizing a rank-constrained least-squares fitting criterion (which is not confined to being one-dimensional but depends on defining an appropriate Mahalanobis metric). In modern n < p problems, where W is not of full rank, the ratio criterion is shown not to be coherent but the fit criterion, with an attention to associated metrics, readily generalizes. In this context we give a unified generalization of CVA, introducing two metrics, one in the range space of W and the other in the null space of W, that have links with Mahalanobis distance. This generalization is computationally efficient, since it requires only the spectral decomposition of a n×n matrix.","Canonical analysis, Ratio form, Fit form, Mahalonobis distance, Discriminant analysis",https://link.springer.com//article/10.1007/s00357-014-9146-y
Distinguishing and Classifying from n-ary Properties,"We present a hierarchical classification based on n-ary relations of the entities. Starting from the finest partition that can be obtained from the attributes, we distinguish between entities having the same attributes by using relations between entities. The classification that we get is thus a refinement of this finest partition. It can be computed in O(n + m2) space and O(n · p · m5/2) time, where n is the number of entities, p the number of classes of the resulting hierarchy (p is the size of the output; p < 2n) and m the maximum number of relations an entity can have (usually, m ≪ n). So we can treat sets with millions of entities.","Classification, Data analysis, Hierarchy, Ultrametric, Computational linguistics, Generation of referring expressions",https://link.springer.com//article/10.1007/s00357-014-9151-1
Adaptive Mixture Discriminant Analysis for Supervised Learning with Unobserved Classes,"In supervised learning, an important issue usually not taken into account by classical methods is that a class represented in the test set may have not been encountered earlier in the learning phase. Classical supervised algorithms will automatically label such observations as belonging to one of the known classes in the training set and will not be able to detect new classes. This work introduces a model-based discriminant analysis method, called adaptive mixture discriminant analysis (AMDA), which can detect several unobserved groups of points and can adapt the learned classifier to the new situation. Two EM-based procedures are proposed for parameter estimation and model selection criteria are used for selecting the actual number of classes. Experiments on artificial and real data demonstrate the ability of the proposed method to deal with complex and real-world problems. The proposed approach is also applied to the detection of unobserved communities in social network analysis.","Supervised classification, Unobserved classes, Adaptive learning, Multiclass novelty detection, Model-based classification, Social network analysis",https://link.springer.com//article/10.1007/s00357-014-9147-x
Rhetorical Strategy in Forensic Speeches: Multidimensional Statistics-Based Methodology,"Rhetorical strategy is relevant in the law domain, where language is a vital instrument. Textual statistics have much to offer for uncovering such a strategy. We propose a methodology that starts from a non-structured text; first, the breakpoints are automatically detected and lexically homogeneous parts are identified; then, the shape of the text through the trajectory of these parts and their hierarchical structure are uncovered; finally, the argument flow is tracked along. Several methods are combined. Chronological clustering of multidimensional count series detects the breakpoints; the shape of the text is revealed by applying correspondence analysis to the parts×words table while the progression of the argument is described by labelled time-constrained hierarchical clustering. This methodology is illustrated on a rhetoric forensic application, concretely a closing speech delivered by a prosecutor at Barcelona Criminal Court. This approach could also be useful in politics, communication and professional writing.","Chronological clustering, Hierarchical constrained clustering, Correspondence analysis, Closing speech for the prosecution, Textual statistics, Textual data",https://link.springer.com//article/10.1007/s00357-014-9148-9
The Canonical Analysis of Distance,"Canonical Variate Analysis (CVA) is one of the most useful of multivariate methods. It is concerned with separating between and within group variation among N samples from K populations with respect to p measured variables. Mahalanobis distance between the K group means can be represented as points in a (K - 1) dimensional space and approximated in a smaller space, with the variables shown as calibrated biplot axes. Within group variation may also be shown, together with circular confidence regions and other convex prediction regions, which may be used to discriminate new samples. This type of representation extends to what we term Analysis of Distance (AoD), whenever a Euclidean inter-sample distance is defined. Although the N × N distance matrix of the samples, which may be large, is required, eigenvalue calculations are needed only for the much smaller K × K matrix of distances between group centroids. All the ancillary information that is attached to a CVA analysis is available in an AoD analysis. We outline the theory and the R programs we developed to implement AoD by presenting two examples.","Analysis of distance, Biplot, Canonical variate analysis",https://link.springer.com//article/10.1007/s00357-014-9149-8
Constrained Multilevel Latent Class Models for the Analysis of Three-Way Three-Mode Binary Data,"Probabilistic feature models (PFMs) can be used to explain binary rater judgements about the associations between two types of elements (e.g., objects and attributes) on the basis of binary latent features. In particular, to explain observed object-attribute associations PFMs assume that respondents classify both objects and attributes with respect to a, usually small, number of binary latent features, and that the observed object-attribute association is derived as a specific mapping of these classifications. Standard PFMs assume that the object-attribute association probability is the same according to all respondents, and that all observations are statistically independent. As both assumptions may be unrealistic, a multilevel latent class extension of PFMs is proposed which allows objects and/or attribute parameters to be different across latent rater classes, and which allows to model dependencies between associations with a common object (attribute) by assuming that the link between features and objects (attributes) is fixed across judgements. Formal relationships with existing multilevel latent class models for binary three-way data are described. As an illustration, the models are used to study rater differences in product perception and to investigate individual differences in the situational determinants of anger-related behavior.","Multilevel latent class model, Latent feature, Three-way data",https://link.springer.com//article/10.1007/s00357-013-9141-8
Model Selection for the Trend Vector Model,"Model selection is an important component of data analysis. This study focuses on issues of model selection for the trend vector model, a model for the analysis of longitudinal multinomial outcomes. The trend vector model is a so-called marginal model, focusing on population averaged evolutions over time. A quasi-likelihood method is employed to obtain parameter estimates. Such an optimization function in theory invalidates likelihood-based statistics, such as the likelihood ratio statistic. Moreover, standard errors obtained from the Hessian are biased. In this paper, the performances of different model selection methods for the trend vector model are studied in detail. We specifically focused on two aspects of model selection: variable selection and dimensionality determination. Based on the quasi-likelihood function, selection criteria analogous to the likelihood ratio statistics, AIC and BIC, were employed. Additionally, Wald and resampling statistics were included as variable selection criteria. A series of simulations were carried out to evaluate the relative performance of these criteria. The results suggest that model selection can be best performed using either the quasi likelihood ratio statistic or the quasi-BIC. A special study on dimensionality selection found that the quasi-AIC also performs well for cases with degrees of freedom greater than 8. Another important finding is that the sandwich estimator for standard errors used in Wald statistics does not perform well. Even for larger sample sizes, the bias-correction procedure for the sandwich estimator is needed to give satisfactory results.","Model selection, Longitudinal multinomial data, Information criterion, Resampling methods, Sandwich variance estimator",https://link.springer.com//article/10.1007/s00357-013-9138-3
Cluster Differences Unfolding for Two-Way Two-Mode Preference Rating Data,"Classification and spatial methods can be used in conjunction to represent the individual information of similar preferences by means of groups. In the context of latent class models and using Simulated Annealing, the cluster-unfolding model for two-way two-mode preference rating data has been shown to be superior to a two-step approach of first deriving the clusters and then unfolding the classes. However, the high computational cost makes the procedure only suitable for small or medium-sized data sets, and the hypothesis of independent and normally distributed preference data may also be too restrictive in many practical situations. Therefore, an alternating least squares procedure is proposed, in which the individuals and the objects are partitioned into clusters, while at the same time the cluster centers are represented by unfolding. An enhanced Simulated Annealing algorithm in the least squares framework is also proposed in order to address the local optimum problem. Real and artificial data sets are analyzed to illustrate the performance of the model.","Unfolding, Cluster analysis, Least squares, Minimum distance, Interval level data, Preference ratings",https://link.springer.com//article/10.1007/s00357-013-9144-5
Graph Partitioning by Correspondence Analysis and Taxicab Correspondence Analysis,We consider correspondence analysis (CA) and taxicab correspondence analysis (TCA) of relational datasets that can mathematically be described as weighted loopless graphs. Such data appear in particular in network analysis. We present CA and TCA as relaxation methods for the graph partitioning problem. Examples of real datasets are provided.,"Adjacency matrix, Incidence matrix, Network analysis, Graph partitioning, Graph Laplacian matrix, Correspondence analysis, Taxicab correspondence analysis, NCut, RCut, MCut, Matrix norm, Centroid method",https://link.springer.com//article/10.1007/s00357-013-9145-4
Model Similarity and Rank-Order Based Classification of Bayesian Networks,"Suppose that we rank-order the conditional probabilities for a group of subjects that are provided from a Bayesian network (BN) model of binary variables. The conditional probability is the probability that a subject has a certain attribute given an outcome of some other variables and the classification is based on the rank-order. Under the condition that the class sizes are equal across the class levels and that all the variables in the model are positively associated with each other, we compared the classification results between models of binary variables which share the same model structure. In the comparison, we used a BN model, called a similar BN model, which was constructed under some rule based on a set of BN models satisfying certain conditions. Simulation results indicate that the agreement level of the classification between a set of BN models and their corresponding similar BN model is considerably high with the exact agreement for about half of the subjects or more and the agreement up to one-class-level difference for about 90% or more.","Agreement level, Bayesian network, Conditional probability, Model similarity, Positive association",https://link.springer.com//article/10.1007/s00357-013-9140-9
Optimal Quantization of the Support of a Continuous Multivariate Distribution based on Mutual Information,"Based on the notion of mutual information between the components of a random vector, we construct, for data reduction reasons, an optimal quantization of the support of its probability measure. More precisely, we propose a simultaneous discretization of the whole set of the components of the random vector which takes into account, as much as possible, the stochastic dependence between them. Examples are presented.","Divergence, Mutual information, Copula, Optimal quantization",https://link.springer.com//article/10.1007/s00357-013-9127-6
Using Neural Network Analysis to Define Methods of DINA Model Estimation for Small Sample Sizes,"The DINA model is a commonly used model for obtaining diagnostic information. Like many other Diagnostic Classification Models (DCMs), it can require a large sample size to obtain reliable item and examinee parameter estimation. Neural Network (NN) analysis is a classification method that uses a training dataset for calibration. As a result, if this training dataset is determined theoretically, as was the case in Gierl’s attribute hierarchical method (AHM), the NN analysis does not have any sample size requirements. However, a NN approach does not provide traditional item parameters of a DCM or allow for item responses to influence test calibration. In this paper, the NN approach will be implemented for the DINA model estimation to explore its effectiveness as a classification method beyond its use in AHM. The accuracy of the NN approach across different sample sizes, item quality and Q-matrix complexity is described in the DINA model context. Then, a Markov Chain Monte Carlo (MCMC) estimation algorithm and Joint Maximum Likelihood Estimation is used to extend the NN approach so that item parameters associated with the DINA model are obtained while allowing examinee responses to influence the test calibration. The results derived by the NN, the combination of MCMC and NN (NN MCMC) and the combination of JMLE and NN are compared with that of the well-established Hierarchical MCMC procedure and JMLE with a uniform prior on the attribute profile to illustrate their strength and weakness.","Neural network, Diagnostic Classification Model, MCMC",https://link.springer.com//article/10.1007/s00357-013-9134-7
Incorporating Student Covariates in Cognitive Diagnosis Models,"In educational measurement, cognitive diagnosis models have been developed to allow assessment of specific skills that are needed to perform tasks. Skill knowledge is characterized as present or absent and represented by a vector of binary indicators, or the skill set profile. After determining which skills are needed for each assessment item, a model is specified for the relationship between item responses and skill set profiles. Cognitive diagnosis models are often used for diagnosis, that is, for classifying students into the different skill set profiles. Generally, cognitive diagnosis models do not exploit student covariate information. However, investigating the effects of student covariates, such as gender, SES, or educational interventions, on skill knowledge mastery is important in education research, and covariate information may improve classification of students to skill set profiles. We extend a common cognitive diagnosis model, the DINA model, by modeling the relationship between the latent skill knowledge indicators and covariates. The probability of skill mastery is modeled as a logistic regression model, possibly with a student-level random intercept, giving a higher-order DINA model with a latent regression. Simulations show that parameter recovery is good for these models and that inclusion of covariates can improve skill diagnosis. When applying our methods to data from an online tutor, we obtain reasonable and interpretable parameter estimates that allow more detailed characterization of groups of students who differ in their predicted skill set profiles.","Cognitive diagnosis model, Collateral information, Concomitant variables, Covariates, DIF, DINA, Higher order model, Random effect, Skill diagnosis",https://link.springer.com//article/10.1007/s00357-013-9130-y
A Nonparametric Approach to Cognitive Diagnosis by Proximity to Ideal Response Patterns,"A trend in educational testing is to go beyond unidimensional scoring and provide a more complete profile of skills that have been mastered and those that have not. To achieve this, cognitive diagnosis models have been developed that can be viewed as restricted latent class models. Diagnosis of class membership is the statistical objective of these models. As an alternative to latent class modeling, a nonparametric procedure is introduced that only requires specification of an item-by-attribute association matrix, and classifies according to minimizing a distance measure between observed responses, and the ideal response for a given attribute profile that would be implied by the item-by-attribute association matrix. This procedure requires no statistical parameter estimation, and can be used on a sample size as small as 1. Heuristic arguments are given for why the nonparametric procedure should be effective under various possible cognitive diagnosis models for data generation. Simulation studies compare classification rates with parametric models, and consider a variety of distance measures, data generation models, and the effects of model misspecification. A real data example is provided with an analysis of agreement between the nonparametric method and parametric approaches.","Cognitive diagnosis, Nonparametric classification, Residual sum",https://link.springer.com//article/10.1007/s00357-013-9132-9
Measuring the Reliability of Diagnostic Classification Model Examinee Estimates,"Over the past decade, diagnostic classification models (DCMs) have become an active area of psychometric research. Despite their use, the reliability of examinee estimates in DCM applications has seldom been reported. In this paper, a reliability measure for the categorical latent variables of DCMs is defined. Using theory-and simulation-based results, we show how DCMs uniformly provide greater examinee estimate reliability than IRT models for tests of the same length, a result that is a consequence of the smaller range of latent variable values examinee estimates can take in DCMs. We demonstrate this result by comparing DCM and IRT reliability for a series of models estimated with data from an end-of-grade test, culminating with a discussion of how DCMs can be used to change the character of large scale testing, either by shortening tests that measure examinees unidimensionally or by providing more reliable multidimensional measurement for tests of the same length.","Diagnostic classification models, Cognitive diagnosis, Reliability, Classification, Psychometrics",https://link.springer.com//article/10.1007/s00357-013-9129-4
Multidimensional Item Response Theory Models with Collateral Information as Poisson Regression Models,"Multiple choice items on tests and Likert items on surveys are ubiquitous in educational, social and behavioral science research; however, methods for analyzing of such data can be problematic. Multidimensional item response theory models are proposed that yield structured Poisson regression models for the joint distribution of responses to items. The methodology presented here extends the approach described in Anderson, Verkuilen, and Peyton (2010) that used fully conditionally specified multinomial logistic regression models as item response functions. In this paper, covariates are added as predictors of the latent variables along with covariates as predictors of location parameters. Furthermore, the models presented here incorporate ordinal information of the response options thus allowing an empirical examination of assumptions regarding the ordering and the estimation of optimal scoring of the response options. To illustrate the methodology and flexibility of the models, data from a study on aggression in middle school (Espelage, Holt, and Henkel 2004) is analyzed. The models are fit to data using SAS.","Log-multiplicative association models, Ordinal response scales, Polytomous items, Covariates, Constrained optimization, Fully conditionally specified models",https://link.springer.com//article/10.1007/s00357-013-9131-x
Utilization of singularity exponent in nearest neighbor based classifier,"Classifiers serve as tools for classifying data into classes. They directly or indirectly take a distribution of data points around a given query point into account. To express the distribution of points from the viewpoint of distances from a given point, a probability distribution mapping function is introduced here. The approximation of this function in a form of a suitable power of the distance is presented. How to state this power—the distribution mapping exponent—is described. This exponent is used for probability density estimation in high-dimensional spaces and for classification. A close relation of the exponent to a singularity exponent is discussed. It is also shown that this classifier exhibits better behavior (classification accuracy) than other kinds of classifiers for some tasks.","Multivariate data, Probability density estimation, Classification, Probability distribution mapping function, Probability density mapping function, Power approximation",https://link.springer.com//article/10.1007/s00357-013-9121-z
Detecting Clusters in the Data from Variance Decompositions of Its Projections,"A new projection-pursuit index is used to identify clusters and other structures in multivariate data. It is obtained from the variance decompositions of the data’s one-dimensional projections, without assuming a model for the data or that the number of clusters is known. The index is affine invariant and successful with real and simulated data. A general result is obtained indicating that clusters’ separation increases with the data’s dimension. In simulations it is thus confirmed, as expected, that the performance of the index either improves or does not deteriorate when the data’s dimension increases, making it especially useful for “large dimension-small sample size” data. The efficiency of this index will increase with the continuously improved computer technology. Several applications are presented.","Analysis of variance, Classification, Clusters, Data structures",https://link.springer.com//article/10.1007/s00357-013-9124-9
Additive Biclustering: A Comparison of One New and Two Existing ALS Algorithms,"The additive biclustering model for two-way two-mode object by variable data implies overlapping clusterings of both the objects and the variables together with a weight for each bicluster (i.e., a pair of an object and a variable cluster). In the data analysis, an additive biclustering model is fitted to given data by means of minimizing a least squares loss function. To this end, two alternating least squares algorithms (ALS) may be used: (1) PENCLUS, and (2) Baier’s ALS approach. However, both algorithms suffer from some inherent limitations, which may hamper their performance. As a way out, based on theoretical results regarding optimally designing ALS algorithms, in this paper a new ALS algorithm will be presented. In a simulation study this algorithm will be shown to outperform the existing ALS approaches.","Biclustering, Additive clustering, PENCLUS, ALS algorithms, Simulation study, Simultaneous overlapping clusterings, Co-clustering, Two-mode clustering, Two-mode data",https://link.springer.com//article/10.1007/s00357-013-9120-0
Comparing Optimization Algorithms for Item Selection in Mokken Scale Analysis,"Mokken scale analysis uses an automated bottom-up stepwise item selection procedure that suffers from two problems. First, when selected during the procedure items satisfy the scaling conditions but they may fail to do so after the scale has been completed. Second, the procedure is approximate and thus may not produce the optimal item partitioning. This study investigates a variation on Mokken’s item selection procedure, which alleviates the first problem, and proposes a genetic algorithm, which alleviates both problems. The genetic algorithm is an approximation to checking all possible partitionings. A simulation study shows that the genetic algorithm leads to better scaling results than the other two procedures.","Item selection, Genetic algorithm, Mokken scaling, Test construction",https://link.springer.com//article/10.1007/s00357-013-9122-y
Combining Association Measures for Collocation Extraction Using Clustering of Receiver Operating Characteristic Curves,"This paper focuses on combining association measures using corresponding receiver operating characteristic curves. The approach is motivated by a problem of automatic bigram collocation extraction from the field of computational linguistics. It is based on supervised machine learning techniques and the fact that different association measures discover different collocation types. Clusters of equivalent ROC curves are first determined by a testing procedure. The paper’s major contribution is an investigation of the possibility of combining representatives of the clusters of equivalent association measures into more complex models, thus improving performance of the collocation extraction.","Receiver operating characteristic (ROC) curves, Binary classification, Clustering, Association measures, Bigram collocation extraction, Lexical classification",https://link.springer.com//article/10.1007/s00357-013-9123-x
A Thurstonian Ranking Model with Rank-Induced Dependencies,"A Thurstonian model for ranks is introduced in which rank-induced dependencies are specified through correlation coefficients among ranked objects that are determined by a vector of rank-induced parameters. The ranking model can be expressed in terms of univariate normal distribution functions, thus simplifying a previously computationally intensive problem. A theorem is proven that shows that the specification given in the paper for the dependencies is the only way that this simplification can be achieved under the process assumptions of the model. The model depends on certain conditional probabilities that arise from item orders considered by subjects as they make ranking decisions. Examples involving a complete set of ranks and a set with missing values are used to illustrate recovery of the objects’ scale values and the rank dependency parameters. Application of the model to ranks for gift items presented singly or as composite items is also discussed.","Ranks, Thurstonian, Probabilistic, Dependencies, Maximum likelihood",https://link.springer.com//article/10.1007/s00357-013-9125-8
Block-Relaxation Approaches for Fitting the INDCLUS Model,"A well-known clustering model to represent I × I × J data blocks, the J frontal slices of which consist of I × I object by object similarity matrices, is the INDCLUS model. This model implies a grouping of the I objects into a prespecified number of overlapping clusters, with each cluster having a slice-specific positive weight. An INDCLUS model is fitted to a given data set by means of minimizing a least squares loss function. The minimization of this loss function has appeared to be a difficult problem for which several algorithmic strategies have been proposed. At present, the best available option seems to be the SYMPRES algorithm, which minimizes the loss function by means of a block-relaxation algorithm. Yet, SYMPRES is conjectured to suffer from a severe local optima problem. As a way out, based on theoretical results with respect to optimally designing block-relaxation algorithms, five alternative block-relaxation algorithms are proposed. In a simulation study it appears that the alternative algorithms with overlapping parameter subsets perform best and clearly outperform SYMPRES in terms of optimization performance and cluster recovery.","INDCLUS, ADCLUS, Alternating least squares, Block-Relaxation algorithms, Additive clustering, Overlapping clusters, Proximity data, Three-Way data",https://link.springer.com//article/10.1007/s00357-012-9113-4
Lowdimensional Additive Overlapping Clustering,"To reveal the structure underlying two-way two-mode object by variable data, Mirkin (1987) has proposed an additive overlapping clustering model. This model implies an overlapping clustering of the objects and a reconstruction of the data, with the reconstructed variable profile of an object being a summation of the variable profiles of the clusters it belongs to. Grasping the additive (overlapping) clustering structure of object by variable data may, however, be seriously hampered in case the data include a very large number of variables. To deal with this problem, we propose a new model that simultaneously clusters the objects in overlapping clusters and reduces the variable space; as such, the model implies that the cluster profiles and, hence, the reconstructed data profiles are constrained to lie in a lowdimensional space. An alternating least squares (ALS) algorithm to fit the new model to a given data set will be presented, along with a simulation study and an illustrative example that makes use of empirical data.","Additive overlapping clustering, Dimensional reduction, Alternating least squares algorithm, Two-way two-mode data, Object by variable data",https://link.springer.com//article/10.1007/s00357-012-9112-5
Recognizing Treelike k-Dissimilarities,"A k-dissimilarity D on a finite set X, |X| ≥ k, is a map from the set of size k subsets of X to the real numbers. Such maps naturally arise from edgeweighted trees T with leaf-set X: Given a subset Y of X of size k, D(Y ) is defined to be the total length of the smallest subtree of T with leaf-set Y . In case k = 2, it is well-known that 2-dissimilarities arising in this way can be characterized by the so-called “4-point condition”. However, in case k > 2 Pachter and Speyer (2004) recently posed the following question: Given an arbitrary k-dissimilarity, how do we test whether this map comes from a tree? In this paper, we provide an answer to this question, showing that for k ≥ 3 a k-dissimilarity on a set X arises from a tree if and only if its restriction to every 2 k-element subset of X arises from some tree, and that 2 k is the least possible subset size to ensure that this is the case. As a corollary, we show that there exists a polynomial-time algorithm to determine when a k-dissimilarity arises from a tree. We also give a 6-point condition for determining when a 3-dissimilarity arises from a tree, that is similar to the aforementioned 4-point condition.","k-dissimilarity, Phylogenetic tree, Dissimilarity, Metric, 4-point condition, Ultrametric condition, Equidistant tree",https://link.springer.com//article/10.1007/s00357-012-9115-2
Mixtures of Autoregressions with an Improper Component for Panel Data,"An EM algorithm for fitting mixtures of autoregressions of low order is constructed and the properties of the estimators are explored on simulated and real datasets. The mixture model incorporates a component with an improper density, which is intended for outliers. The model is proposed as an alternative to the search for the order of a single-component autoregression. The methods can be adapted to other patterns of dependence in panel data. An application to the monthly records of income of the outlets of a retail company is presented.","Autoregression, EM algorithm, Improper component, Mixture, Outliers, Panel data",https://link.springer.com//article/10.1007/s00357-012-9111-6
Local Statistical Modeling via a Cluster-Weighted Approach with Elliptical Distributions,"Cluster-weighted modeling (CWM) is a mixture approach to modeling the joint probability of data coming from a heterogeneous population. Under Gaussian assumptions, we investigate statistical properties of CWM from both theoretical and numerical point of view; in particular, we show that Gaussian CWM includes mixtures of distributions and mixtures of regressions as special cases. Further, we introduce CWM based on Student-t distributions, which provides a more robust fit for groups of observations with longer than normal tails or noise data. Theoretical results are illustrated using some empirical studies, considering both simulated and real data. Some generalizations of such models are also outlined.","Cluster-weighted modeling, Mixture models, Model-based clustering",https://link.springer.com//article/10.1007/s00357-012-9114-3
"Fast, Linear Time Hierarchical Clustering using the Baire Metric","The Baire metric induces an ultrametric on a dataset and is of linear computational complexity, contrasted with the standard quadratic time agglomerative hierarchical clustering algorithm. In this work we evaluate empirically this new approach to hierarchical clustering. We compare hierarchical clustering based on the Baire metric with (i) agglomerative hierarchical clustering, in terms of algorithm properties; (ii) generalized ultrametrics, in terms of definition; and (iii) fast clustering through k-means partitioning, in terms of quality of results. For the latter, we carry out an in depth astronomical study. We apply the Baire distance to spectrometric and photometric redshifts from the Sloan Digital Sky Survey using, in this work, about half a million astronomical objects. We want to know how well the (more costly to determine) spectrometric redshifts can predict the (more easily obtained) photometric redshifts, i.e. we seek to regress the spectrometric on the photometric redshifts, and we use clusterwise regression for this.","Hierarchical clustering, Ultrametric, Redshift, k-means, p-adic, m-adic, Baire, Longest common prefix",https://link.springer.com//article/10.1007/s00357-012-9106-3
Dealing with Distances and Transformations for Fuzzy C-Means Clustering of Compositional Data,"Clustering techniques are based upon a dissimilarity or distance measure between objects and clusters. This paper focuses on the simplex space, whose elements—compositions—are subject to non-negativity and constant-sum constraints. Any data analysis involving compositions should fulfill two main principles: scale invariance and subcompositional coherence. Among fuzzy clustering methods, the FCM algorithm is broadly applied in a variety of fields, but it is not well-behaved when dealing with compositions. Here, the adequacy of different dissimilarities in the simplex, together with the behavior of the common log-ratio transformations, is discussed in the basis of compositional principles. As a result, a well-founded strategy for FCM clustering of compositions is suggested. Theoretical findings are accompanied by numerical evidence, and a detailed account of our proposal is provided. Finally, a case study is illustrated using a nutritional data set known in the clustering literature.","Fuzzy clustering, FCM, Compositional data, Closed data, Simplex space, Aitchison distance",https://link.springer.com//article/10.1007/s00357-012-9105-4
FINDCLUS: Fuzzy INdividual Differences CLUStering,"ADditive CLUStering (ADCLUS) is a tool for overlapping clustering of two-way proximity matrices (objects × objects). In Simple Additive Fuzzy Clustering (SAFC), a variant of ADCLUS is introduced providing a fuzzy partition of the objects, that is the objects belong to the clusters with the so-called membership degrees ranging from zero (complete non-membership) to one (complete membership). INDCLUS (INdividual Differences CLUStering) is a generalization of ADCLUS for handling three-way proximity arrays (objects × objects × subjects). Here, we propose a fuzzified alternative to INDCLUS capable to offer a fuzzy partition of the objects by generalizing in a three-way context the idea behind SAFC. This new model is called Fuzzy INdividual Differences CLUStering (FINDCLUS). An algorithm is provided for fitting the FINDCLUS model to the data. Finally, the results of a simulation experiment and some applications to synthetic and real data are discussed.","Three-way analysis, Clustering, Proximity data, INDCLUS, Fuzzy approach",https://link.springer.com//article/10.1007/s00357-012-9109-0
A New Class of Weighted Similarity Indices Using Polytomous Variables,"We introduce new similarity measures between two subjects, with reference to variables with multiple categories. In contrast to traditionally used similarity indices, they also take into account the frequency of the categories of each attribute in the sample. This feature is useful when dealing with rare categories, since it makes sense to differently evaluate the pairwise presence of a rare category from the pairwise presence of a widespread one. A weighting criterion for each category derived from Shannon’s information theory is suggested. There are two versions of the weighted index: one for independent categorical variables and one for dependent variables. The suitability of the proposed indices is shown in this paper using both simulated and real world data sets.","Chi-square distance, Cluster analysis, Variable weighting, Information theory",https://link.springer.com//article/10.1007/s00357-012-9107-2
Accurate Tree-based Missing Data Imputation and Data Fusion within the Statistical Learning Paradigm,"Framework of this paper is statistical data editing, specifically how to edit or impute missing or contradictory data and how to merge two independent data sets presenting some lack of information. Assuming a missing at random mechanism, this paper provides an accurate tree-based methodology for both missing data imputation and data fusion that is justified within the Statistical Learning Theory of Vapnik. It considers both an incremental variable imputation method to improve computational efficiency as well as boosted trees to gain in prediction accuracy with respect to other methods. As a result, the best approximation of the structural risk (also known as irreducible error) is reached, thus reducing at minimum the generalization (or prediction) error of imputation. Moreover, it is distribution free, it holds independently of the underlying probability law generating missing data values. Performance analysis is discussed considering simulation case studies and real world applications.","Data editing, Tree-based methods, Boosting algorithm, FAST algorithm, Incremental imputation, Generalization error",https://link.springer.com//article/10.1007/s00357-012-9108-1
A Combinatorial Approach to Assess the Separability of Clusters,"Separability of clusters is an issue that arises in many different areas, and is often used in a rather vague and subjective manner. We introduce a combinatorial notion of interiority to derive a global view on separability of a set of entities. We develop this approach further to evaluate the overall separability of a partition in the context of cluster analysis. Our approach captures combinatorial and geometrical aspects of data and provides, in addition to numerical evaluations, graphical representations particularly useful when data are not easily visualized. We illustrate the methodology on some real and simulated datasets.","Cluster analysis, Combinatorics, Convex hulls;Minimum spanning forests",https://link.springer.com//article/10.1007/s00357-012-9098-z
A Criterion Based on the Mahalanobis Distance for Cluster Analysis with Subsampling,"A two-level data set consists of entities of a higher level (say populations), each one being composed of several units of the lower level (say individuals). Observations are made at the individual level, whereas population characteristics are aggregated from individual data. Cluster analysis with subsampling of populations is a cluster analysis based on individual data that aims at clustering populations rather than individuals. In this article, we extend existing optimality criteria for cluster analysis with subsampling of populations to deal with situations where population characteristics are not the mean of individual data. A new criterion that depends on the Mahalanobis distance is also defined. The criteria are compared using simulated examples and an ecological data set of tree species in a tropical rain forest.","Cluster analysis, Mahalanobis distance, Model-based clustering, Species classification, Subsampling, Variance criterion",https://link.springer.com//article/10.1007/s00357-012-9100-9
A Copula-Based Algorithm for Discovering Patterns of Dependent Observations,"The main aim of this work is the study of clustering dependent data by means of copula functions. Copulas are popular multivariate tools whose importance within clustering methods has not been investigated yet in detail. We propose a new algorithm (CoClust in brief) that allows to cluster dependent data according to the multivariate structure of the generating process without any assumption on the margins. Moreover, the approach does not require either to choose a starting classification or to set a priori the number of clusters; in fact, the CoClust selects them by using a criterion based on the log–likelihood of a copula fit. We test our proposal on simulated data for different dependence scenarios and compare it with a model–based clustering technique. Finally, we show applications of the CoClust to real microarray data of breast-cancer patients.","Clustering methods, CoClust algorithm, Copula functions, Model–based clustering, Microarray data",https://link.springer.com//article/10.1007/s00357-012-9099-y
A Property of the CHAID Partitioning Method for Dichotomous Randomized Response Data and Categorical Predictors,"In this paper, we present empirical and theoretical results on classification trees for randomized response data. We considered a dichotomous sensitive response variable with the true status intentionally misclassified by the respondents using rules prescribed by a randomized response method. We assumed that classification trees are grown using the Pearson chi-square test as a splitting criterion, and that the randomized response data are analyzed using classification trees as if they were not perturbed. We proved that classification trees analyzing observed randomized response data and estimated true data have a one-to-one correspondence in terms of ranking the splitting variables. This is illustrated using two real data sets.","Pearson chi-square, Forced response method, Prevalence estimation, CHAID method",https://link.springer.com//article/10.1007/s00357-011-9094-8
Handling Missing Values with Regularized Iterative Multiple Correspondence Analysis,"A common approach to deal with missing values in multivariate exploratory data analysis consists in minimizing the loss function over all non-missing elements, which can be achieved by EM-type algorithms where an iterative imputation of the missing values is performed during the estimation of the axes and components. This paper proposes such an algorithm, named iterative multiple correspondence analysis, to handle missing values in multiple correspondence analysis (MCA). The algorithm, based on an iterative PCA algorithm, is described and its properties are studied. We point out the overfitting problem and propose a regularized version of the algorithm to overcome this major issue. Finally, performances of the regularized iterative MCA algorithm (implemented in the R-package named missMDA) are assessed from both simulations and a real dataset. Results are promising with respect to other methods such as the missing-data passive modified margin method, an adaptation of the missing passive method used in Gifi’s Homogeneity analysis framework.","Multiple correspondence analysis, Categorical data, Missing values, Imputation, Regularization",https://link.springer.com//article/10.1007/s00357-012-9097-0
Directed Binary Hierarchies and Directed Ultrametrics,"Directed binary hierarchies have been introduced in order to give a graphical reduced representation of a family of association rules. This type of structure extends the classical binary hierarchical classification in a very specific way. In this paper an accurate formalization of this new structure is studied. A directed hierarchy is defined as a set of ordered pairs of subsets of the initial individual set satisfying specific conditions. A new notion of directed ultrametricity is studied. The main result consists in establishing a bijective correspondence between a directed ultrametric space and a directed binary hierarchy. Finally, an algorithm is proposed in order to transform a directed ultrametric structure into a graphical representation associated with a directed binary hierarchy.","Directed hierarchical classification, Association rule representation",https://link.springer.com//article/10.1007/s00357-011-9091-y
On the Schoenberg Transformations in Data Analysis: Theory and Illustrations,"The class of Schoenberg transformations, embedding Euclidean distances into higher dimensional Euclidean spaces, is presented, and derived from theorems on positive definite and conditionally negative definite matrices. Original results on the arc lengths, angles and curvature of the transformations are proposed, and visualized on artificial data sets by classical multidimensional scaling. A distance-based discriminant algorithm and a robust multidimensional centroid estimate illustrate the theory, closely connected to the Gaussian kernels of Machine Learning.","Bernstein functions, Conditionally negative definite matrices, Discriminant analysis, Euclidean distances, Huygens principle, Isometric embedding, helix, Kernels, Menger curvature, Multidimensional scaling, Rectifiable curves, Robust centroids, Robust PCA",https://link.springer.com//article/10.1007/s00357-011-9092-x
Between-Group Metrics,"In canonical analysis with more variables than samples, it is shown that, as well as the usual canonical means in the range-space of the within-groups dispersion matrix, canonical means may be defined in its null space. In the range space we have the usual Mahalanobis metric; in the null space explicit expressions are given and interpreted for a new metric.","Between-group distances, Canonical analysis, Mahalanobis distance",https://link.springer.com//article/10.1007/s00357-011-9090-z
Classification of Multivariate Objects Using Interval Quantile Classes,"The paper contains a proposal of interval data clustering related to given social and economic objects characterized by many interval variables. This multivariate approach is based on an original conception of interval quantiles constructed using a special definition derived from the notion of the Hausdorff distance. In order to improve the quality of classification, the obtained interval quantile classes can be next aggregated into larger merged classes. The efficiency of our method can be assessed using especially defined indices of entropy and volume coefficients. The second notion replaces the classical concept of area, which is not applicable in this case.","Clustering, Interval data, Interval quantile",https://link.springer.com//article/10.1007/s00357-011-9088-6
The Real-Valued Model of Hierarchical Classes,"We propose a non-negative real-valued model of hierarchical classes (HICLAS) for two-way two-mode data. Like the other members of the HICLAS family, the non-negative real-valued model (NNRV-HICLAS) implies simultaneous hierarchically organized classifications of all modes involved in the data. A distinctive feature of the novel model is that it yields continuous, non-negative real-valued reconstructed data, which considerably expands the application range of the HICLAS family. The expansion implies a major algorithmic challenge as it involves a move from the typical discrete optimization problems in HICLAS to a mixed discrete-continuous one. To solve this mixed discrete-continuous optimization problem, a two-stage algorithm combining a simulated annealing and an alternating local descent stage is proposed. Subsequently it is evaluated in a simulation study. Finally, the NNRVHICLAS model is applied to an empirical data set on anger.","HICLAS modeling, Real-valued HICLAS, Two-mode clustering, Simulated annealing, Alternating least squares",https://link.springer.com//article/10.1007/s00357-011-9089-5
TreeOfTrees Method to Evaluate the Congruence Between Gene Trees,"A new method, TreeOfTrees, is proposed to compare X-tree structures obtained from several sets of aligned gene sequences of the same taxa. Its aim is to detect genes or sets of genes having different evolutionary histories. The comparison between sets of trees is based on several tree metrics, leading to a unique tree labelled by the gene trees. The robustness values of its edges are estimated by bootstrapping and consensus procedures that allow detecting subsets of genes having differently evolved. Simulations are performed under various evolutionary conditions to test the efficiency of the method and an application on real data is described. Tests of arboricity and various consensus algorithms are also discussed. A corresponding software package is available.","Congruence, Gene trees, Phylogeny, Tree of trees",https://link.springer.com//article/10.1007/s00357-011-9093-9
Some New Results on Orthogonally Constrained Candecomp,"The use of Candecomp to fit scalar products in the context of Indscal is based on the assumption that, due to the symmetry of the data matrices involved, two components matrices will become equal when Candecomp converges. Bennani Dosse and Ten Berge (2008) have shown that, in the single component case, the assumption can only be violated at saddle points in the case of Gramian matrices. This paper again considers Candecomp applied to symmetric matrices, but with an orthonormality constraint on the components. This constrained version of Candecomp, when applied to symmetric matrices, has long been known under the acronym Indort. When the data matrices are positive definite, or have become positive semidefinite due to double centering, and the saliences are nonnegative – by chance or by constraint –, the component matrices resulting from Indort are shown to be equal. Because Indort is also free from so-called degeneracy problems, it is a highly attractive alternative to Candecomp in the present context. We also consider a well-known successive approach to the orthogonally constrained Indscal problem and we compare, from simulated and real data sets, its results with those given by the simultaneous (Indort) approach.","Simultaneous approach, Successive approach, Indort, Three-way Data, Candecomp, Indscal",https://link.springer.com//article/10.1007/s00357-011-9086-8
Clustering of Distributions: A Case of Patent Citations,"Often the data units are described with discrete distributions (work described with citation distribution over time, population pyramid described as age-sex distribution etc.).When the set of such units is very large, appropriate clustering methods can reveal the typical patterns hidden in the data.In this paper we present an adapted leaders method combined with a compatible adapted agglomerative hierarchical method that are based on relative error measure between a unit and the corresponding cluster representative–leader. The proposed approach is illustrated on citation distributions derived from the data set of US patents from 1980 to 1999. These new methods were developed because clustering of units, described with distributions, with classical k-means method reveals patterns with single high peaks which correspond to a single year. These patterns prevail over other distribution shapes also present in the data. Compared with centers in k-means method, clusters’ representatives obtained with the proposed new methods better detect typical distribution shapes of units. The obtained main cluster types for different sets of units show three main patterns: patents with early or late peak of importance to the community, and patents where the importance is slowly increasing throughout the time period.","Clustering, Distribution, Leaders method, k-means method, Agglomerative hierarchical clustering method, Temporal citation distribution, Citation network, Relative error measure, Patents",https://link.springer.com//article/10.1007/s00357-011-9084-x
MCS: A Method for Finding the Number of Clusters,"This paper proposes a maximum clustering similarity (MCS) method for determining the number of clusters in a data set by studying the behavior of similarity indices comparing two (of several) clustering methods. The similarity between the two clusterings is calculated at the same number of clusters, using the indices of Rand (R), Fowlkes and Mallows (FM), and Kulczynski (K) each corrected for chance agreement. The number of clusters at which the index attains its maximum is a candidate for the optimal number of clusters. The proposed method is applied to simulated bivariate normal data, and further extended for use in circular data. Its performance is compared to the criteria discussed in Tibshirani, Walther, and Hastie (2001). The proposed method is not based on any distributional or data assumption which makes it widely applicable to any type of data that can be clustered using at least two clustering algorithms.","Similarity index, Clustering algorithm, Circular data, Bivariate normal mixture, Correction for chance agreement, Gap statistic, Number of clusters, Comparing partitions",https://link.springer.com//article/10.1007/s00357-010-9069-1
A New Dimension Reduction Method: Factor Discriminant K-means,"Reduced K-means (RKM) and Factorial K-means (FKM) are two data reduction techniques incorporating principal component analysis and K-means into a unified methodology to obtain a reduced set of components for variables and an optimal partition for objects. RKM finds clusters in a reduced space by maximizing the between-clusters deviance without imposing any condition on the within-clusters deviance, so that clusters are isolated but they might be heterogeneous. On the other hand, FKM identifies clusters in a reduced space by minimizing the within-clusters deviance without imposing any condition on the between-clusters deviance. Thus, clusters are homogeneous, but they might not be isolated. The two techniques give different results because the total deviance in the reduced space for the two methodologies is not constant; hence the minimization of the within-clusters deviance is not equivalent to the maximization of the between-clusters deviance. In this paper a modification of the two techniques is introduced to avoid the afore mentioned weaknesses. It is shown that the two modified methods give the same results, thus merging RKM and FKM into a new methodology. It is called Factor Discriminant K-means (FDKM), because it combines Linear Discriminant Analysis and K-means. The paper examines several theoretical properties of FDKM and its performances with a simulation study. An application on real-world data is presented to show the features of FDKM.","Cluster analysis, Dimension reduction, K-Means, Principal Component Analysis",https://link.springer.com//article/10.1007/s00357-011-9085-9
Multiclass Functional Discriminant Analysis and Its Application to Gesture Recognition,"We consider applying a functional logistic discriminant procedure to the analysis of handwritten character data. Time-course trajectories corresponding to the X and Y coordinate values of handwritten characters written in the air with one finger are converted into a functional data set via regularized basis expansion. We then apply functional logistic modeling to classify the functions into several classes. In order to select the values of adjusted parameters involved in the functional logistic model, we derive a model selection criterion for evaluating models estimated by the method of regularization. Results indicate the effectiveness of our modeling strategy in terms of prediction accuracy.","Functional data analysis, Model selection, Pattern recognition",https://link.springer.com//article/10.1007/s00357-011-9082-z
Classification and Categorical Inputs with Treed Gaussian Process Models,"Recognizing the successes of treed Gaussian process (TGP) models as an interpretable and thrifty model for nonparametric regression, we seek to extend the model to classification. Both treed models and Gaussian processes (GPs) have, separately, enjoyed great success in application to classification problems. An example of the former is Bayesian CART. In the latter, real-valued GP output may be utilized for classification via latent variables, which provide classification rules by means of a softmax function. We formulate a Bayesian model averaging scheme to combine these two models and describe a Monte Carlo method for sampling from the full posterior distribution with joint proposals for the tree topology and the GP parameters corresponding to latent variables at the leaves. We concentrate on efficient sampling of the latent variables, which is important to obtain good mixing in the expanded parameter space. The tree structure is particularly helpful for this task and also for developing an efficient scheme for handling categorical predictors, which commonly arise in classification problems. Our proposed classification TGP (CTGP) methodology is illustrated on a collection of synthetic and real data sets. We assess performance relative to existing methods and thereby show how CTGP is highly flexible, offers tractable inference, produces rules that are easy to interpret, and performs well out of sample.","Treed models, Gaussian process, Bayesian model averaging, Latent variable",https://link.springer.com//article/10.1007/s00357-011-9083-y
The Academic Journal Ranking Problem: A Fuzzy-Clustering Approach,"The Academic Journal Ranking Problem consists in formulating a formal assessment of scientific journals. An outcome variable must be constructed that allows valid journal comparison, either as a set of tiers (ordered classes) or as a numerical index. But part of the problem is also to devise a procedure to get this outcome, that is, how to get and use relevant data coming from expert opinions or from citations database. We propose a novel approach to the problem that applies fuzzy cluster analysis to peer reviews and opinion surveys. The procedure is composed of two steps: the first is to collect the most relevant qualitative assessments from international organizations (for example, the ones available in the Harzing database) and, as inductive analysis, to apply fuzzy clustering to determine homogeneous journal classes; the second deductive step is to determine the hidden logical rules that underlies the classification, using a classification tree to reproduce the same patterns of the first step.Our approach is applied to the classification of 138 academic journals that were selected by members of AMASES, an Italian mathematics association, as the most prominent journals of our field. The clusters that are determined by our method show that rankings are affected by two hidden dimensions: one is the academic prestige of a publication, but the other is the disciplinary diffusion of a mathematics subfield. In particular, mathematics journals that are close to finance or economics are usually ranked better than journals dealing with linear algebra or systems dynamics.","Fuzzy clustering model, Classification tree, Journal quality problem, Harzing data base, Impact Factor",https://link.springer.com//article/10.1007/s00357-011-9072-1
Heterogeneity Measures in Customer Satisfaction Analysis,"In this paper we deal with the problem of identifying a valid way to characterize heterogeneity in the analysis of customer satisfaction observing the phenomenon through a new perspective. In the literature, the variability of a Customer Satisfaction index is measured by the standard deviation or the coefficient of variation. In this way, heterogeneity among customers may be masked. To overcome this drawback, we provide a new approach to the construction of a multi-dimensional measure of heterogeneity of the Customer Satisfaction index not depending on the choice of a particular heterogeneity index. The approach is based on heterogeneity profiles which lead to a more detailed description of heterogeneity than alternative measures. Moreover, a latent class model is used for classifying individuals into distinct groups based on responses to a set of items. Once groups are formed, Customer Satisfaction researchers can make conclusions about the level of satisfaction and the characteristics of groups in terms of heterogeneity.","Response pattern, Heterogeneity, Heterogeneity profile, Latent class model",https://link.springer.com//article/10.1007/s00357-011-9075-y
Complementary Use of Rasch Models and Nonlinear Principal Components Analysis in the Assessment of the Opinion of Europeans About Utilities,"Two non-standard techniques – the Rasch Model and Nonlinear Principal Components Analysis – originally proposed in other fields are presented and discussed to measure the opinion of European citizens about utilities. The Eurobarometer survey data are thus considered. The potential of both methods and their complementary use are highlighted; in particular the methods allow the questionnaire to be calibrated, the role of different services and aspects of service to be established, and the consumer satisfaction to be assessed and compared among European countries, different years and services.","Customer satisfaction, Eurobarometer, Ordinal variables, Service quality",https://link.springer.com//article/10.1007/s00357-011-9081-0
Correspondence Analysis with Linear Constraints of Ordinal Cross-Classifications,"Within the non-iterative procedures for performing a correspondence analysis with linear constraints, a strategy is proposed to impose linear constraints in analyzing a contingency table with one or two ordered sets of categories. At the heart of the approach is the partition of the Pearson chi-squared statistics which involves terms that summarize the association between the nominal/ordinal variables using bivariate moments based on orthogonal polynomials. Linear constraints are then included directly in suitable matrices reflecting the most important components, overcoming also the problem of imposing linear constraints based on subjective decisions.","Correspondence analysis, Ordinal correspondence analysis, Linear constraints, External information, Orthogonal polynomials",https://link.springer.com//article/10.1007/s00357-011-9070-3
On the Imputation of Missing Data in Surveys with Likert-Type Scales,"Starting from the problem of missing data in surveys with Likert-type scales, the aim of this paper is to evaluate a possible improvement for the imputation procedure proposed by Lavori, Dawson, and Shera (1995) here called Approximate Bayesian bootstrap with Propensity score (ABP).We propose an imputation procedure named Approximate Bayesian bootstrap with Propensity score and Nearest neighbour (ABPN), which, after the “propensity score step” of ABP, randomly selects a donor in the nonrespondent’s neighbourhood, which includes cases with response patterns similar to the one of the nonrespondent to be imputed.A preliminary simulation study with single imputation on missing data in two Likerttype scales from a real data set shows that ABPN: (a) performed better than the ABP imputation, and (b) can be considered as a serious competitor of other procedures used in this context.","Item nonresponse, Imputation, Approximate Bayesian Bootstrap, Missing data mechanisms, Latent trait, Ordinal variables",https://link.springer.com//article/10.1007/s00357-011-9074-z
Iterative Design of Experiments by Non-Linear PLS Models. A Case Study: The Reservoir Simulator Data to Forecast Oil Production,"In this paper we present a way of conducting design of experiments by Multivariate Additive Partial Least-Squares Splines models, in short MAPLSS. In the framework of optimal experimental design based on small samples, in order to select the most informative MAPLSS model, we process an adaptive incremental selection of observations by a particular bootstrap procedure. Why MAPLSS models? Because they inherit the advantages of the PLS regression that permits to capture additively non-linear main effects and relevant interactions in the difficult framework of small samples. The effectiveness of this approach is illustrated on the reservoir simulator data used to forecast oil production.","L2 Boosting regression, Multivariate additive PLS splines, Bootstrap, Iterative design of experiments",https://link.springer.com//article/10.1007/s00357-011-9071-2
GME Estimation of Spatial Structural Equations Models,"The objective of this paper is to develop a GME formulation for the class of spatial structural equations models (S-SEM). In this respect, two innovatory aspects are introduced: (i) the formalization of the GME estimation approach for structural equations models that account for spatial heterogeneity and spatial dependence; (ii) the extension of the methodology to a panel data framework. We also present an application of the method to real data finalized to investigate disparities of unemployment rates in OECD countries over the period 1998-2006.",GME estimation of S-SEM,https://link.springer.com//article/10.1007/s00357-011-9073-0
Structural Similarity: Spectral Methods for Relaxed Blockmodeling,"In this paper we propose the concept of structural similarity as a relaxation of blockmodeling in social network analysis. Most previous approaches attempt to relax the constraints on partitions, for instance, that of being a structural or regular equivalence to being approximately structural or regular, respectively. In contrast, our approach is to relax the partitions themselves: structural similarities yield similarity values instead of equivalence or non-equivalence of actors, while strictly obeying the requirement made for exact regular equivalences. Structural similarities are based on a vector space interpretation and yield efficient spectral methods that, in a more restrictive manner, have been successfully applied to difficult combinatorial problems such as graph coloring. While traditional blockmodeling approaches have to rely on local search heuristics, our framework yields algorithms that are provably optimal for specific data-generation models. Furthermore, the stability of structural similarities can be well characterized making them suitable for the analysis of noisy or dynamically changing network data.","Social network analysis, Blockmodeling, Spectral graph partitioning, Conflict networks, Dynamic network visualization",https://link.springer.com//article/10.1007/s00357-010-9062-8
Dimensionality Reduction on the Cartesian Product of Embeddings of Multiple Dissimilarity Matrices,"We consider the problem of combining multiple dissimilarity representations via the Cartesian product of their embeddings. For concreteness, we choose the inferential task at hand to be classification. The high dimensionality of this Cartesian product space implies the necessity of dimensionality reduction before training a classifier. We propose a supervised dimensionality reduction method, which utilizes the class label information, to help achieve a favorable combination. The simulation and real data results show that our approach can improve classification accuracy compared to the alternatives of principal components analysis and no dimensionality reduction at all.","Dissimilarity representation, Multidimensional scaling, Dimensionality reduction, Principal components analysis, Linear discriminant analysis",https://link.springer.com//article/10.1007/s00357-010-9059-3
A Formal Proof of a Paradox Associated with Cohen’s Kappa,"Suppose two judges each classify a group of objects into one of several nominal categories. It has been observed in the literature that, for fixed observed agreement between the judges, Cohen’s kappa penalizes judges with similar marginals compared to judges who produce different marginals. This paper presents a formal proof of this phenomenon.","Inter-rater reliability, Nominal agreement, Rearrangement inequality, Marginal homogeneity, Marginal asymmetry",https://link.springer.com//article/10.1007/s00357-010-9060-x
Comparing Several Parametric and Nonparametric Approaches to Time Series Clustering: A Simulation Study,"One key point in cluster analysis is to determine a similarity or dissimilarity measure between data objects. When working with time series, the concept of similarity can be established in different ways. In this paper, several non-parametric statistics originally designed to test the equality of the log-spectra of two stochastic processes are proposed as dissimilarity measures between time series data. Their behavior in time series clustering is analyzed throughout a simulation study, and compared with the performance of several model-free and model-based dissimilarity measures. Up to three different classification settings were considered: (i) to distinguish between stationary and non-stationary time series, (ii) to classify different ARMA processes and (iii) to classify several non-linear time series models. As it was expected, the performance of a particular dissimilarity metric strongly depended on the type of processes subjected to clustering. Among all the measures studied, the nonparametric distances showed the most robust behavior.","Time series clustering, Dissimilarity measures, Stationary and non-stationary processes, ARMA processes, Non-linear processes, Local linear regression",https://link.springer.com//article/10.1007/s00357-010-9064-6
Dimensionally Reduced Model-Based Clustering Through Mixtures of Factor Mixture Analyzers,"Dimensionally reduced model-based clustering methods are recently receiving a wide interest in statistics as a tool for performing simultaneously clustering and dimension reduction through one or more latent variables. Among these, Mixtures of Factor Analyzers assume that, within each component, the data are generated according to a factor model, thus reducing the number of parameters on which the covariance matrices depend. In Factor Mixture Analysis clustering is performed through the factors of an ordinary factor analysis which are jointly modelled by a Gaussian mixture. The two approaches differ in genesis, parameterization and consequently clustering performance. In this work we propose a model which extends and combines them. The proposed Mixtures of Factor Mixture Analyzers provide a unified class of dimensionally reduced mixture models which includes the previous ones as special cases and could offer a powerful tool for modelling non-Gaussian latent variables.","Gaussian mixture models, Factor analysis, EM-algorithm",https://link.springer.com//article/10.1007/s00357-010-9063-7
A Probability for Classification Based on the Dirichlet Process Mixture Model,In this paper we provide an explicit probability distribution for classification purposes when observations are viewed on the real line and classifications are to be based on numerical orderings. The classification model is derived from a Bayesian nonparametric mixture of Dirichlet process model; with some modifications. The resulting approach then more closely resembles a classical hierarchical grouping rule in that it depends on sums of squares of neighboring values. The proposed probability model for classification relies on a numerical procedure based on a reversible Markov chain Monte Carlo (MCMC) algorithm for determining the probabilities. Some numerical illustrations comparing with alternative ideas for classification are provided.,"Classification, MCMC sampling, MDP model",https://link.springer.com//article/10.1007/s00357-010-9061-9
Minimum Tree Cost Quartet Puzzling,"We present a new distance based quartet method for phylogenetic tree reconstruction, called Minimum Tree Cost Quartet Puzzling. Starting from a distance matrix computed from natural data, the algorithm incrementally constructs a tree by adding one taxon at a time to the intermediary tree using a cost function based on the relaxed 4-point condition for weighting quartets. Different input orders of taxa lead to trees having distinct topologies which can be evaluated using a maximum likelihood or weighted least squares optimality criterion. Using reduced sets of quartets and a simple heuristic tree search strategy we obtain an overall complexity of O(n5 log2n) for the algorithm. We evaluate the performances of the method through comparative tests and show that our method outperforms NJ when a weighted least squares optimality criterion is employed. We also discuss the theoretical boundaries of the algorithm.","Quartet method, Distance method, Phylogenetic tree reconstruction",https://link.springer.com//article/10.1007/s00357-010-9053-9
An Algorithm for Computing Cutpoints in Finite Metric Spaces,"The theory of the tight span, a cell complex that can be associated to every metric D, offers a unifying view on existing approaches for analyzing distance data, in particular for decomposing a metric D into a sum of simpler metrics as well as for representing it by certain specific edge-weighted graphs, often referred to as realizations of D. Many of these approaches involve the explicit or implicit computation of the so-called cutpoints of (the tight span of) D, such as the algorithm for computing the “building blocks” of optimal realizations of D recently presented by A. Hertz and S. Varone. The main result of this paper is an algorithm for computing the set of these cutpoints for a metric D on a finite set with n elements in O(n3) time. As a direct consequence, this improves the run time of the aforementioned O(n6)-algorithm by Hertz and Varone by “three orders of magnitude”.","Metric, Cutpoint, Realization, Tight span, Decomposition, Block",https://link.springer.com//article/10.1007/s00357-010-9055-7
n-Way Metrics,"We study a family of n-way metrics that generalize the usual two-way metric. The n-way metrics are totally symmetric maps from En into \( {\mathbb{R}_{ \geqslant 0}} \). The three-way metrics introduced by Joly and Le Calvé (1995) and Heiser and Bennani (1997) and the n-way metrics studied in Deza and Rosenberg (2000) belong to this family. It is shown how the n-way metrics and n-way distance measures are related to (n − 1)-way metrics, respectively, (n − 1)-way distance measures.","n-Way distance measure, Triangle inequality, Tetrahedron inequality;Polyhedron inequality, Parametrized inequality",https://link.springer.com//article/10.1007/s00357-010-9052-x
Multiple Correspondence Analysis via Polynomial Transformations of Ordered Categorical Variables,"We present an alternative approach to Multiple Correspondence Analysis (MCA) that is appropriate when the data consist of ordered categorical variables. MCA displays objects (individuals, units) and variables as individual points and sets of category points in a low-dimensional space. We propose a hybrid decomposition on the basis of the classical indicator super-matrix, using the singular value decomposition, and the bivariate moment decomposition by orthogonal polynomials. When compared to standard MCA, the hybrid decomposition will give the same representation of the categories of the variables, but additionally, we obtain a clear association interpretation among the categories in terms of linear, quadratic and higher order components. Moreover, the graphical display of the individual units will show an automatic clustering.","Multiple correspondence analysis, Ordered categorical variables, Singular value decomposition, Bivariate moment decomposition, Hybrid decomposition, Automatic clustering of objects",https://link.springer.com//article/10.1007/s00357-010-9056-6
Functional Cluster Analysis via Orthonormalized Gaussian Basis Expansions and Its Application,"We propose functional cluster analysis (FCA) for multidimensional functional data sets, utilizing orthonormalized Gaussian basis functions. An essential point in FCA is the use of orthonormal bases that yield the identity matrix for the integral of the product of any two bases. We construct orthonormalized Gaussian basis functions using Cholesky decomposition and derive a property of Cholesky decomposition with respect to Gram-Schmidt orthonormalization. The advantages of the functional clustering are that it can be applied to the data observed at different time points for each subject, and the functional structure behind the data can be captured by removing the measurement errors. Numerical experiments are conducted to investigate the effectiveness of the proposed method, as compared to conventional discrete cluster analysis. The proposed method is applied to three-dimensional (3D) protein structural data that determine the 3D arrangement of amino acids in individual protein.","Cholesky decomposition, Functional data, Gram-Schmidt orthonormalization, k-means, Protein structure, Radial basis functions, Self-Organizing Maps",https://link.springer.com//article/10.1007/s00357-010-9054-8
Wavelet-based Fuzzy Clustering of Time Series,"Traditional procedures for clustering time series are based mostly on crisp hierarchical or partitioning methods. Given that the dynamics of a time series may change over time, a time series might display patterns that may enable it to belong to one cluster over one period while over another period, its pattern may be more consistent with those in another cluster. The traditional clustering procedures are unable to identify the changing patterns over time. However, clustering based on fuzzy logic will be able to detect the switching patterns from one time period to another thus enabling some time series to simultaneously belong to more than one cluster. In particular, this paper proposes a fuzzy approach to the clustering of time series based on their variances through wavelet decomposition. We will show that this approach will distinguish between time series with different patterns in variability as well identifying time series with switching patterns in variability.","Stationary and non stationary time series, Switching time series, Wavelet variance, Crisp clustering, Fuzzy clustering, Developed and emerging markets",https://link.springer.com//article/10.1007/s00357-010-9058-4
Intelligent Choice of the Number of Clusters in K-Means Clustering: An Experimental Study with Different Cluster Spreads,"The issue of determining “the right number of clusters” in K-Means has attracted considerable interest, especially in the recent years. Cluster intermix appears to be a factor most affecting the clustering results. This paper proposes an experimental setting for comparison of different approaches at data generated from Gaussian clusters with the controlled parameters of between- and within-cluster spread to model cluster intermix. The setting allows for evaluating the centroid recovery on par with conventional evaluation of the cluster recovery. The subjects of our interest are two versions of the “intelligent” K-Means method, ik-Means, that find the “right” number of clusters by extracting “anomalous patterns” from the data one-by-one. We compare them with seven other methods, including Hartigan’s rule, averaged Silhouette width and Gap statistic, under different between- and within-cluster spread-shape conditions. There are several consistent patterns in the results of our experiments, such as that the right K is reproduced best by Hartigan’s rule – but not clusters or their centroids. This leads us to propose an adjusted version of iK-Means, which performs well in the current experiment setting.","K-Means clustering, Number of clusters, Anomalous pattern, Hartigan’s rule, Gap statistic",https://link.springer.com//article/10.1007/s00357-010-9049-5
Selection of a Representative Sample,"Sometimes a larger dataset needs to be reduced to just a few points, and it is desirable that these points be representative of the whole dataset. If the future uses of these points are not fully specified in advance, standard decision-theoretic approaches will not work. We present here methodology for choosing a small representative sample based on a mixture modeling approach.","Bayesian statistics, Clustering, Mixture model, Data reduction.",https://link.springer.com//article/10.1007/s00357-010-9044-x
A Fuzzy Clustering Model for Multivariate Spatial Time Series,"Clustering of multivariate spatial-time series should consider: 1) the spatial nature of the objects to be clustered; 2) the characteristics of the feature space, namely the space of multivariate time trajectories; 3) the uncertainty associated to the assignment of a spatial unit to a given cluster on the basis of the above complex features. The last aspect is dealt with by using the Fuzzy C-Means objective function, based on appropriate measures of dissimilarity between time trajectories, by distinguishing the cross-sectional and longitudinal aspects of the trajectories. In order to take into account the spatial nature of the statistical units, a spatial penalization term is added to the above function, depending on a suitable spatial proximity/ contiguity matrix. A tuning coefficient takes care of the balance between, on one side, discriminating according to the pattern of the time trajectories and, on the other side, ensuring an approximate spatial homogeneity of the clusters. A technique for determining an optimal value of this coefficient is proposed, based on an appropriate spatial autocorrelation measure. Finally, the proposed models are applied to the classification of the Italian provinces, on the basis of the observed dynamics of some socio-economical indicators.","Array of space time data, Fuzzy clustering, Spatial autocorrelation function, Spatial penalization term, Dissimilarity measures between multivariate time trajectories",https://link.springer.com//article/10.1007/s00357-010-9043-y
Parsimonious Classification Via Generalized Linear Mixed Models,"We devise a classification algorithm based on generalized linear mixed model (GLMM) technology. The algorithm incorporates spline smoothing, additive model-type structures and model selection. For reasons of speed we employ the Laplace approximation, rather than Monte Carlo methods. Tests on real and simulated data show the algorithm to have good classification performance. Moreover, the resulting classifiers are generally interpretable and parsimonious.","Akaike Information Criterion, Feature selection, Generalized additive models, Penalized splines, Supervised learning, Model selection, Rao statistics, Variance components",https://link.springer.com//article/10.1007/s00357-010-9045-9
Anisotropic Orthogonal Procrustes Analysis,"The aim of this paper is to analyze two scaling extensions of the Orthogonal Procrustes Problem (OPP) called the pre-scaling and the post-scaling approaches. We also discuss some problems related to these extensions and propose two new algorithms to find optimal solutions. These algorithms, which are based on the majorization principle, are shown to be monotonically convergent and their performance is examined.","Procrustes analysis, Pre-scaling approach, Post-scaling approach, Majorization algorithms",https://link.springer.com//article/10.1007/s00357-010-9046-8
The Remarkable Simplicity of Very High Dimensional Data: Application of Model-Based Clustering,"An ultrametric topology formalizes the notion of hierarchical structure. An ultrametric embedding, referred to here as ultrametricity, is implied by a hierarchical embedding. Such hierarchical structure can be global in the data set, or local. By quantifying extent or degree of ultrametricity in a data set, we show that ultrametricity becomes pervasive as dimensionality and/or spatial sparsity increases. This leads us to assert that very high dimensional data are of simple structure. We exemplify this finding through a range of simulated data cases. We discuss also application to very high frequency time series segmentation and modeling.","Multivariate data analysis, Cluster analysis, Hierarchy, Ultrametric, p-Adic, Dimensionality",https://link.springer.com//article/10.1007/s00357-009-9037-9
Seriation in the Presence of Errors: NP-Hardness of l∞ -Fitting Robinson Structures to Dissimilarity Matrices,"In this paper, we establish that the following fitting problem is NP-hard: given a finite set X and a dissimilarity measure d on X (d is a symmetric function from X2 to the nonnegative real numbers and vanishing on the diagonal), we wish to find a Robinsonian dissimilarity dR on X minimizing the l∞-error ||d − dR||∞ = maxx,y∈X{|d(x, y) − dR(x, y)|} between d and dR. Recall that a dissimilarity dR on X is called monotone (or Robinsonian) if there exists a total order ≺ on X such that x ≺ z ≺ y implies that d(x, y) ≥ max{d(x, z), d(z, y)}. The Robinsonian dissimilarities appear in seriation and clustering problems, in sparse matrix ordering and DNA sequencing.","Robinsonian dissimilarity, NP-hardness, Fitting problem, l∞-norm",https://link.springer.com//article/10.1007/s00357-009-9041-0
Market Segmentation Using Brand Strategy Research: Bayesian Inference with Respect to Mixtures of Log-Linear Models,"This paper presents a Bayesian model based clustering approach for dichotomous item responses that deals with issues often encountered in model based clustering like missing data, large data sets and within cluster dependencies. The approach proposed will be illustrated using an example concerning Brand Strategy Research.","Bayesian computational statistics, Model based clustering, Log-linear modeling, Market segmentation, Brand strategy research, Markov Chain Monte Carlo methods, Missing data",https://link.springer.com//article/10.1007/s00357-009-9040-1
Unfolding Incomplete Data: Guidelines for Unfolding Row-Conditional Rank Order Data with Random Missings,"Unfolding creates configurations from preference information. In this paper, it is argued that not all preference information needs to be collected and that good solutions are still obtained, even when more than half of the data is missing. Simulation studies are conducted to compare missing data treatments, sources of missing data, and designs for the specification of missing data. Guidelines are provided and used in actual practice.","Unfolding, Incomplete data, Missing data, BIBD, PREFSCAL",https://link.springer.com//article/10.1007/s00357-009-9039-7
Incremental Tree-Based Missing Data Imputation with Lexicographic Ordering,"In the framework of incomplete data analysis, this paper provides a nonparametric approach to missing data imputation based on Information Retrieval. In particular, an incremental procedure based on the iterative use of tree-based method is proposed and a suitable Incremental Imputation Algorithm is introduced. The key idea is to define a lexicographic ordering of cases and variables so that conditional mean imputation via binary trees can be performed incrementally. A simulation study and real data applications are carried out to describe the advantages and the performance with respect to standard approaches.","Missing data, Classification and regression tree, FAST splitting algorithm, Lexicographic order, Nonparametric imputation, Data editing",https://link.springer.com//article/10.1007/s00357-009-9038-8
Structural Classification Analysis of Three-Way Dissimilarity Data,"The paper presents a methodology for classifying three-way dissimilarity data, which are reconstructed by a small number of consensus classifications of the objects each defined by a sum of two order constrained distance matrices, so as to identify both a partition and an indexed hierarchy.Specifically, the dissimilarity matrices are partitioned in homogeneous classes and, within each class, a partition and an indexed hierarchy are simultaneously fitted.The model proposed is mathematically formalized as a constrained mixed-integer quadratic problem to be fitted in the least-squares sense and an alternating least-squares algorithm is proposed which is computationally efficient.Two applications of the methodology are also described together with an extensive simulation to investigate the performance of the algorithm.","Dissimilarity, Three-Way data, Classification, Hierarchy, Partition",https://link.springer.com//article/10.1007/s00357-009-9033-0
Optimization Strategies for Two-Mode Partitioning,"Two-mode partitioning is a relatively new form of clustering that clusters both rows and columns of a data matrix. In this paper, we consider deterministic two-mode partitioning methods in which a criterion similar to k-means is optimized. A variety of optimization methods have been proposed for this type of problem. However, it is still unclear which method should be used, as various methods may lead to non-global optima. This paper reviews and compares several optimization methods for two-mode partitioning. Several known methods are discussed, and a new fuzzy steps method is introduced. The fuzzy steps method is based on the fuzzy c-means algorithm of Bezdek (1981) and the fuzzy steps approach of Heiser and Groenen (1997) and Groenen and Jajuga (2001). The performances of all methods are compared in a large simulation study. In our simulations, a two-mode k-means optimization method most often gives the best results. Finally, an empirical data set is used to give a practical example of two-mode partitioning.","Two-mode partitioning, Optimization methods, Meta-heuristics",https://link.springer.com//article/10.1007/s00357-009-9031-2
A Procedure for Estimating the Number of Clusters in Logistic Regression Clustering,"This paper studies the problem of estimating the number of clusters in the context of logistic regression clustering. The classification likelihood approach is employed to tackle this problem. A model-selection based criterion for selecting the number of logistic curves is proposed and its asymptotic property is also considered. The small sample performance of the proposed criterion is studied by Monto Carlo simulation. In addition, a real data example is presented.","Asymptotics, Logistic regression clustering, Model selection, Penalty",https://link.springer.com//article/10.1007/s00357-009-9035-y
Bayes Discriminant Rules with Ordered Predictors,We propose and discuss improved Bayes rules to discriminate between two populations using ordered predictors. To address the problem we propose an alternative formulation using a latent space that allows to introduce the information about the order in the theoretical rules. The rules are first defined when the marginal densities are fully known and then under normality when the parameters are unknown and training samples are available. Several numerical examples and simulations in the paper illustrate the methodology and show that the new rules handle the information appropriately. We compare the new rules with the classical Bayes and Fisher rules in these examples and we show that the misclassification probability is smaller for the new rules. The method is also applied to data from a diabetes study where we again show that the new rules improve over the usual Fisher rule.,"Discriminant analysis, Latent space, Misclassification probability, Order restrictions, Restricted estimation",https://link.springer.com//article/10.1007/s00357-009-9034-z
k-Adic Similarity Coefficients for Binary (Presence/Absence) Data,"k-Adic formulations (for groups of objects of size k) of a variety of 2-adic similarity coefficients (for pairs of objects) for binary (presence/absence) data are presented. The formulations are not functions of 2-adic similarity coefficients. Instead, the main objective of the the paper is to present k-adic formulations that reflect certain basic characteristics of, and have a similar interpretation as, their 2-adic versions. Two major classes are distinguished. The first class is referred to as Bennani-Heiser similarity coefficients, which contains all coefficients that can be defined using just the matches, the number of attributes that are present and that are absent in k objects, and the total number of attributes. The coefficients in the second class can be formulated as functions of Dice’s association indices.","Indices of association, Resemblance measures, Simple matching coefficient, Jaccard coefficient, Dice/Sørenson coefficient, Rand index, Global order equivalence",https://link.springer.com//article/10.1007/s00357-009-9032-1
Assessing Congruence Among Ultrametric Distance Matrices,"Recently, a test of congruence among distance matrices (CADM) has been developed. The null hypothesis is the incongruence among all data matrices. It has been shown that CADM has a correct type I error rate and good power when applied to independently-generated distance matrices. In this study, we investigate the suitability of CADM to compare ultrametric distance matrices. We tested the type I error rate and power of CADM with randomly generated dendrograms and their associated ultrametric distance matrices. We show that the test has correct type I error rates and good power. To obtain the significance level of the statistic, a single (as in the Mantel test) or a double (as in the double permutation test, DPT) permutation procedure was used. The power of CADM remained identical when the two permutation methods were compared. This study clearly demonstrates that CADM can be used to determine whether different dendrograms convey congruent information.","Incongruence, Power, Statistical test, Simulation, Type I error rate",https://link.springer.com//article/10.1007/s00357-009-9028-x
Classifying Time Series Data: A Nonparametric Approach,"A general nonparametric approach to identify similarities in a set of simultaneously observed time series is proposed. The trends are estimated via local polynomial regression and classified according to standard clustering procedures. The equality of the trends is checked using several nonparametric test statistics whose asymptotic distributions are approximated by a bootstrap procedure. Once the estimated trends are removed from the model, the residual series are grouped by means of a nonparametric cluster method specifically designed for time series. Such a method is based on a disparity measure between local linear smoothers of the spectra of the series. The performance of the proposed methodology is illustrated by means of its application to a particular financial data example. The dependence of the observations is a crucial factor in this work and is taken into account throughout the study.","Nonparametric methods, Cluster analysis, Hypothesis testing, Time series",https://link.springer.com//article/10.1007/s00357-009-9030-3
"Distributional Equivalence and Subcompositional Coherence in the Analysis of Compositional Data, Contingency Tables and Ratio-Scale Measurements","We consider two fundamental properties in the analysis of two-way tables of positive data: the principle of distributional equivalence, one of the cornerstones of correspondence analysis of contingency tables, and the principle of subcompositional coherence, which forms the basis of compositional data analysis. For an analysis to be subcompositionally coherent, it suffices to analyze the ratios of the data values. A common approach to dimension reduction in compositional data analysis is to perform principal component analysis on the logarithms of ratios, but this method does not obey the principle of distributional equivalence. We show that by introducing weights for the rows and columns, the method achieves this desirable property and can be applied to a wider class of methods. This weighted log-ratio analysis is theoretically equivalent to “spectral mapping”, a multivariate method developed almost 30 years ago for displaying ratio-scale data from biological activity spectra. The close relationship between spectral mapping and correspondence analysis is also explained, as well as their connection with association modeling. The weighted log-ratio methodology is used here to visualize frequency data in linguistics and chemical compositional data in archeology.","Association models, Biplot, Correspondence analysis, Log-ratio analysis, Singular value decomposition, Spectral mapping",https://link.springer.com//article/10.1007/s00357-009-9027-y
Some Interpretative Tools for Non-Symmetrical Correspondence Analysis,"Non-symmetrical correspondence analysis (NSCA) is a very practical statistical technique for the identification of the structure of association between asymmetrically related categorical variables forming a contingency table. This paper considers some tools that can be used to numerically and graphically explore in detail the association between these variables and include the use of confidence regions, the establishment of the link between NSCA and the analysis of variance of categorical variables, and the effect of imposing linear constraints on a variable.","CATANOVA, Confidence circles, Goodman-Kruskal tau index, Linear constraints, Non-symmetrical correspondence analysis",https://link.springer.com//article/10.1007/s00357-009-9025-0
Robust Double Clustering: A Method Based on Alternating Concentration Steps,"We propose two algorithms for robust two-mode partitioning of a data matrix in the presence of outliers. First we extend the robust k-means procedure to the case of biclustering, then we slightly relax the definition of outlier and propose a more flexible and parsimonious strategy, which anyway is inherently less robust. We discuss the breakdown properties of the algorithms, and illustrate the methods with simulations and three real examples.","Biclustering, Double clustering, Microarrays, Robustness, Outliers",https://link.springer.com//article/10.1007/s00357-009-9026-z
A Characterization of Majority Rule for Hierarchies,"The majority rule has been a popular method for producing a consensus classification from several different classifications, when the classifications are all on the same set of objects and are structured as hierarchies. In this note, a new axiomatic characterization is proved for this consensus method on hierarchies.","Hierarchy, Consensus, Majority rule",https://link.springer.com//article/10.1007/s00357-008-9012-x
The Metric Cutpoint Partition Problem,"Let G = (V, E,w) be a graph with vertex and edge sets V and E, respectively, and w: E → \( \mathbb{R}^{+} \) a function which assigns a positive weight or length to each edge of G. G is called a realization of a finite metric space (M, d), with M = {1, ..., n} if and only if {1, ..., n} ⊆ V and d(i, j) is equal to the length of the shortest chain linking i and j in G ∀i, j = 1, ..., n. A realization G of (M, d), is called optimal if the sum of its weights is minimal among all the realizations of (M, d). A cutpoint in a graph G is a vertex whose removal strictly increases the number of connected components of G. The Metric Cutpoint Partition Problem is to determine if a finite metric space (M, d) has an optimal realization containing a cutpoint. We prove in this paper that this problem is polynomially solvable. We also describe an algorithm that constructs an optimal realization of (M, d) from optimal realizations of subspaces that do not contain any cutpoint.","Metric spaces, Optimal realizations, Cutpoint, Bridge, Polynomial algorithm",https://link.springer.com//article/10.1007/s00357-008-9016-6
On the Equivalence of Cohen’s Kappa and the Hubert-Arabie Adjusted Rand Index,"It is shown that one can calculate the Hubert-Arabie adjusted Rand index by first forming the fourfold contingency table counting the number of pairs of objects that were placed in the same cluster in both partitions, in the same cluster in one partition but in different clusters in the other partition, and in different clusters in both, and then computing Cohen’s κ on this fourfold table.","Correction for chance agreement, Partitions, Clustering method, Matching table, Simple matching coefficient, Similarity indices, Resemblance measures",https://link.springer.com//article/10.1007/s00357-008-9023-7
A Binary Integer Program to Maximize the Agreement Between Partitions,"This research note focuses on a problem where the cluster sizes for two partitions of the same object set are assumed known; however, the actual assignments of objects to clusters are unknown for one or both partitions. The objective is to find a contingency table that produces maximum possible agreement between the two partitions, subject to constraints that the row and column marginal frequencies for the table correspond exactly to the cluster sizes for the partitions. This problem was described by H. Messatfa (Journal of Classification, 1992, pp. 5–15), who provided a heuristic procedure based on the linear transportation problem. We present an exact solution procedure using binary integer programming. We demonstrate that our proposed method efficiently obtains optimal solutions for problems of practical size.","Partition agreement, Contingency table, Binary integer programming",https://link.springer.com//article/10.1007/s00357-008-9013-9
Bounds of Resemblance Measures for Binary (Presence/Absence) Variables,"Bounds of association coefficients for binary variables are derived using the arithmetic-geometric-harmonic mean inequality. More precisely, it is shown which presence/absence coefficients are bounds with respect to each other. Using the new bounds it is investigated whether a coefficient is in general closer to either its upper or its lower bound.","Association coefficients, Similarity coefficients, 2 × 2 table, Minimum value, Harmonic mean, Geometric mean, Arithmetic mean, Maximum value",https://link.springer.com//article/10.1007/s00357-008-9024-6
On the Discrepancy Measures for the Optimal Equal Probability Partitioning in Bayesian Multivariate Micro-Aggregation,"Data holders, such as statistical institutions and financial organizations, have a very serious and demanding task when producing data for official and public use. It’s about controlling the risk of identity disclosure and protecting sensitive information when they communicate data-sets among themselves, to governmental agencies and to the public. One of the techniques applied is that of micro-aggregation. In a Bayesian setting, micro-aggregation can be viewed as the optimal partitioning of the original data-set based on the minimization of an appropriate measure of discrepancy, or distance, between two posterior distributions, one of which is conditional on the original data-set and the other conditional on the aggregated data-set. Assuming d-variate normal data-sets and using several measures of discrepancy, it is shown that the asymptotically optimal equal probability m-partition of \( \mathbb{R}^{d} \), with m1/d ∈ \( \mathbb{N} \), is the convex one which is provided by hypercubes whose sides are formed by hyperplanes perpendicular to the canonical axes, no matter which discrepancy measure has been used. On the basis of the above result, a method that produces a sub-optimal partition with a very small computational cost is presented.","Asymptotic theory, Convex partition, Discrepancy measures, Identity protection, Multivariate micro-aggregation, Probabilistic distances",https://link.springer.com//article/10.1007/s00357-008-9014-8
Identifiability of Finite Mixtures of Multinomial Logit Models with Varying and Fixed Effects,"Unique parametrizations of models are very important for parameter interpretation and consistency of estimators. In this paper we analyze the identifiability of a general class of finite mixtures of multinomial logits with varying and fixed effects, which includes the popular multinomial logit and conditional logit models. The application of the general identifiability conditions is demonstrated on several important special cases and relations to previously established results are discussed. The main results are illustrated with a simulation study using artificial data and a marketing dataset of brand choices.","Conditional logit, Finite mixture, Identifiability, Multinomial logit, Unobserved heterogeneity",https://link.springer.com//article/10.1007/s00357-008-9022-8
Classification Based on Depth Transvariations,"Suppose y, a d-dimensional (d ≥ 1) vector, is drawn from a mixture of k (k ≥ 2) populations, given by ∏1, ∏2,…,∏k. We wish to identify the population that is the most likely source of the point y. To solve this classification problem many classification rules have been proposed in the literature. In this study, a new nonparametric classifier based on the transvariation probabilities of data depth is proposed. We compare the performance of the newly proposed nonparametric classifier with classical and maximum depth classifiers using some benchmark and simulated data sets.","Transvariation probabilities, Nonparametric classifier, Depth function, Multivariate ranks, Misclassification error rate",https://link.springer.com//article/10.1007/s00357-008-9015-7
Selecting Among Multi-Mode Partitioning Models of Different Complexities: A Comparison of Four Model Selection Criteria,"Multi-mode partitioning models for N-way N-mode data reduce each of the N modes in the data to a small number of clusters that are mutually exclusive. Given a specific N-mode data set, one may wonder which multi-mode partitioning model (i.e., with which numbers of clusters for each mode) yields the most useful description of this data set and should therefore be selected. In this paper, we address this issue by investigating four possible model selection heuristics: multi-mode extensions of Calinski and Harabasz’s (1974) and Kaufman and Rousseeuw’s (1990) indices for one-mode k-means clustering and multi-mode partitioning versions of Timmerman and Kiers’s (2000) DIFFIT and Ceulemans and Kiers’s (2006) numerical convex hull based model selection heuristic for three-mode principal component analysis. The performance of these four heuristics is systematically compared in a simulation study, which shows that the DIFFIT and numerical convex hull heuristics perform satisfactory in the two-mode partitioning case and very good in the threemode partitioning case.","Multi-mode partitioning, Model selection, Number of clusters",https://link.springer.com//article/10.1007/s00357-008-9005-9
Probabilistic D-Clustering,"We present a new iterative method for probabilistic clustering of data. Given clusters, their centers and the distances of data points from these centers, the probability of cluster membership at any point is assumed inversely proportional to the distance from (the center of) the cluster in question. This assumption is our working principle.The method is a generalization, to several centers, of theWeiszfeld method for solving the Fermat–Weber location problem. At each iteration, the distances (Euclidean, Mahalanobis, etc.) from the cluster centers are computed for all data points, and the centers are updated as convex combinations of these points, with weights determined by the above principle. Computations stop when the centers stop moving.Progress is monitored by the joint distance function, a measure of distance from all cluster centers, that evolves during the iterations, and captures the data in its low contours.The method is simple, fast (requiring a small number of cheap iterations) and insensitive to outliers.","Clustering, Probabilistic clustering, Mahalanobis distance, Harmonic mean, Joint distance function, Weiszfeld method, Similarity matrix",https://link.springer.com//article/10.1007/s00357-008-9002-z
Degenerating Families of Dendrograms,"Dendrograms used in data analysis are ultrametric spaces, hence objects of nonarchimedean geometry. It is known that there exist p-adic representations of dendrograms. Completed by a point at infinity, they can be viewed as subtrees of the Bruhat-Tits tree associated to the p-adic projective line. The implications are that certain moduli spaces known in algebraic geometry are in fact p-adic parameter spaces of dendrograms, and stochastic classification can also be handled within this framework. At the end, we calculate the topology of the hidden part of a dendrogram.","Dendrograms, p-adic numbers, Bruhat-Tits tree, Moduli spaces",https://link.springer.com//article/10.1007/s00357-008-9009-5
Solving Non-Uniqueness in Agglomerative Hierarchical Clustering Using Multidendrograms,"In agglomerative hierarchical clustering, pair-group methods suffer from a problem of non-uniqueness when two or more distances between different clusters coincide during the amalgamation process. The traditional approach for solving this drawback has been to take any arbitrary criterion in order to break ties between distances, which results in different hierarchical classifications depending on the criterion followed. In this article we propose a variable-group algorithm that consists in grouping more than two clusters at the same time when ties occur. We give a tree representation for the results of the algorithm, which we call a multidendrogram, as well as a generalization of the Lance andWilliams’ formula which enables the implementation of the algorithm in a recursive way.","Agglomerative methods, Cluster analysis, Hierarchical classification, Lance and Williams’ formula, Ties in proximity",https://link.springer.com//article/10.1007/s00357-008-9004-x
Mining Supervised Classification Performance Studies: A Meta-Analytic Investigation,"There have been many comparative studies of classification methods in which real datasets are used as a gauge to assess the relative performance of the methods. Since these comparisons often yield inconclusive or limited results on how methods perform, it is often believed that a broader approach combining these studies would shed some light on this difficult question. This paper describes such an attempt: we have sampled the available literature and created a dataset of 5807 classification results. We show that one of the possible ways to analyze the resulting data is an overall assessment of the classification methods, and we present methods for that particular aim. The merits and demerits of such an approach are discussed, and conclusions are drawn which may assist future research: we argue that the current state of the literature hardly allows large-scale investigations.","Classification rules, Supervised classification, Neural networks, Tree classifiers, Logistic regression, Nearest neighbor method, Bradley-Terry, Meta-analysis, Data mining",https://link.springer.com//article/10.1007/s00357-008-9003-y
A Simple Identification Proof for a Mixture of Two Univariate Normal Distributions,A simple proof of the identification of a mixture of two univariate normal distributions is given. The proof is based on the equivalence of local identification with positive definiteness of the information matrix and the equivalence of the latter to a condition on the score vector that is easily checked for this model. Two extensions using the same line of proof are also given.,"Finite mixtures, Information matrix, Exponential family, Mixture regression",https://link.springer.com//article/10.1007/s00357-008-9008-6
On the Indeterminacy of Resemblance Measures for Binary (Presence/Absence) Data,Many similarity coefficients for binary data are defined as fractions. For certain resemblance measures the denominator may become zero. If the denominator is zero the value of the coefficient is indeterminate. It is shown that the seriousness of the indeterminacy problem differs with the resemblance measures. Following Batagelj and Bren (1995) we remove the indeterminacies by defining appropriate values in critical cases.,"Association coefficients, Indeterminate values, Critical cases",https://link.springer.com//article/10.1007/s00357-008-9006-8
Bayesian Regularization for Normal Mixture Estimation and Model-Based Clustering,"Normal mixture models are widely used for statistical modeling of data, including cluster analysis.However maximum likelihood estimation (MLE) for normal mixtures using the EM algorithm may fail as the result of singularities or degeneracies. To avoid this, we propose replacing the MLE by a maximum a posteriori (MAP) estimator, also found by the EM algorithm. For choosing the number of components and the model parameterization, we propose a modified version of BIC, where the likelihood is evaluated at the MAP instead of the MLE. We use a highly dispersed proper conjugate prior, containing a small fraction of one observation's worth of information. The resulting method avoids degeneracies and singularities, but when these are not present it gives similar results to the standard method using MLE, EM and BIC.","Bayesian Information Criterion, American Statistical Association, Normal Mixture, Finite Mixture Model, Posterior Mode",https://link.springer.com//article/10.1007/s00357-007-0004-5
Separating Latent Classes by Information Criteria,"This study evaluates performance of information criteria used to separate latent classes. In the evaluations, various numbers of latent classes, sample sizes, parameter structures and latent-class complexities were designed to simulate datasets. The average accuracy rates of information criteria in selecting the designed numbers of latent classes were the core results in this experiment. The study revealed that widely used information criteria, e.g., AIC, BIC, CAIC, could perform poorly under some circumstances. By including a sample size adjustment (Rissanen, 1978), the unsatis-factory performances could be improved considerably. The sample size adjustment provides a plausible solution for separating latent classes. Guidelines are provided to help achieve optimum use of the model fit indices.","Latent Classis, Latent Class Analysis, Royal Statistical Society, Latent Class Model, Finite Mixture Model",https://link.springer.com//article/10.1007/s00357-007-0010-1
Incremental Classification with Generalized Eigenvalues,"Supervised learning techniques are widely accepted methods to analyze data for scientific and real world problems. Most of these problems require fast and continuous acquisition of data, which are to be used in training the learning system. Therefore, maintaining such systems updated may become cumbersome. Various techniques have been devised in the field of machine learning to solve this problem. In this study, we propose an algorithm to reduce the training data to a substantially small subset of the original training data to train a generalized eigenvalue classifier. The proposed method provides a constructive way to understand the influence of new training data on an existing classification function. We show through numerical experiments that this technique prevents the overfitting problem of the earlier generalized eigenvalue classifiers, while promising a comparable performance in classification with respect to the state-of-the-art classification methods.","Support Vector Machine, Generalize Eigenvalue Problem, Support Vector Machine Algorithm, Support Vector Machine Method, Supervise Learning Technique",https://link.springer.com//article/10.1007/s00357-007-0012-z
Free Knot Splines for Supervised Classification,Data in many different fields come to practitioners through a process naturally described as functional. We propose a classification procedure of oxidation curves. Our algorithm is based on two stages: fitting the functional data by linear splines with free knots and classifying the estimated knots which estimate useful oxidation parameters. A real data set on 57 oxidation curves is used to illustrate our approach.,"Prediction Error, Spline Function, Query Point, Friction Parameter, Functional Data Analysis",https://link.springer.com//article/10.1007/s00357-007-0013-y
The Metric Bridge Partition Problem: Partitioning of a Metric Space into Two Subspaces Linked by an Edge in Any Optimal Realization,"Let G = (V,E,w) be a graph with vertex and edge sets V and E, respectively, and w:E → R + a function which assigns a positive weight or length to each edge of G. G is called a realization of a finite metric space (M,d), with M = { 1,...,n} if and only if { 1,...,n} ⫅ V and d(i,j) is equal to the length of the shortest chain linking i and j in G ∀ i,j = 1,...,n. A realization G of (M,d), is said optimal if the sum of its weights is minimal among all the realizations of (M,d). Consider a partition of M into two nonempty subsets K and L, and let e be an edge in a realization G of (M,d); we say that e is a bridge linking K with L if e belongs to all chains in G linking a vertex of K with a vertex of L. The Metric Bridge Partition Problem is to determine if the elements of a finite metric space (M,d) can be partitioned into two nonempty subsets K and L such that all optimal realizations of (M,d) contain a bridge linking K with L. We prove in this paper that this problem is polynomially solvable. We also describe an algorithm that constructs an optimal realization of (M,d) from optimal realizations of (K,d|K) and (L,d|L).","Distance Matrix, Nonempty Subset, Decomposition Algorithm, Positive Weight, Swiss National Science Foundation",https://link.springer.com//article/10.1007/s00357-007-0011-0
Algorithms for ℓ1-Embeddability and Related Problems,"Assouad has shown that a real-valued distance d = (dij)1 ≤ i < j ≤ n is isometrically embeddable in ℓ1space if and only if it belongs to the cut cone on n points.  Determining if this condition holds is NP-complete. We use Assouad's result in a constructive column generation algorithm for ℓ1-embeddability. The subproblem is an unconstrained 0-1 quadratic program, solved by Tabu Search and Variable Neighborhood Search heuristics as well as by an exact enumerative algorithm.  Computational results are reported. Several ways to approximate a distance which is not ℓ1-embeddable by another one which is are also studied.","Column Generation, Master Problem, Variable Neighborhood Search, Feasibility Problem, Isometric Embedding",https://link.springer.com//article/10.1007/s00357-007-0014-x
Global Optimization in Any Minkowski Metric: A Permutation-Translation Simulated Annealing Algorithm for Multidimensional Scaling,"It is well known that considering a non-Euclidean Minkowski metric in Multidimensional Scaling, either for the distance model or for the loss function, increases the computational problem of local minima considerably. In this paper, we propose an algorithm in which both the loss function and the composition rule can be considered in any Minkowski metric, using a multivariate randomly alternating Simulated Annealing procedure with permutation and translation phases. The algorithm has been implemented in Fortran and tested over classical and simulated data matrices with sizes up to 200 objects. A study has been carried out with some of the common loss functions to determine the most suitable values for the main parameters. The experimental results confirm the theoretical expectation that Simulated Annealing is a suitable strategy to deal by itself with the optimization problems in Multidimensional Scaling, in particular for City-Block, Euclidean and Infinity metrics.","Simulated Annealing, Global Optimization, Loss Function, Multidimensional Scaling, Simulated Annealing Algorithm",https://link.springer.com//article/10.1007/s00357-007-0020-1
The Haar Wavelet Transform of a Dendrogram,"We describe a new wavelet transform, for use on hierarchies or binary rooted trees. The theoretical framework of this approach to data analysis is described. Case studies are used to further exemplify this approach. A first set of application studies  deals with data array smoothing, or filtering. A second set of application studies relates to hierarchical tree condensation. Finally, a third study explores the wavelet decomposition, and the  reproducibility of data sets such as text, including a new perspective on the generation or computability of such data objects.","Hierarchical Cluster, Binary Tree, Wavelet Transform, Haar Wavelet, Ultrametric Space",https://link.springer.com//article/10.1007/s00357-007-0007-9
On a General Transformation Making a Dissimilarity Matrix Euclidean,"When a dissimilarity matrix cannot be represented in a Euclidean space, it is possible to make it Euclidean by means of suitable transformations of the original dissimilarity values. In this paper we discuss some interesting properties of a class of transformations based on adding a specific squared Euclidean distance to the initial dissimilarity.","Distance Matrix, Multidimensional Scaling, Additive Constant, Dissimilarity Measure, General Transformation",https://link.springer.com//article/10.1007/s00357-007-0005-y
Default Priors for Neural Network Classification,"Feedforward neural networks are a popular tool for classification, offering a method for fully flexible modeling. This paper looks at the underlying probability model, so as to understand statistically what is going on in order to facilitate an intelligent choice of prior for a fully Bayesian analysis. The parameters turn out to be difficult or impossible to interpret, and yet a coherent prior requires a quantification of this inherent uncertainty. Several approaches are discussed, including flat priors, Jeffreys priors and reference priors.","Neural Network, Class Membership, Fisher Information Matrix, Sepal Length, Reference Prior",https://link.springer.com//article/10.1007/s00357-007-0001-2
Simultaneous Component and Clustering Models for Three-way Data: Within and Between Approaches,"In this paper two techniques for units clustering and factorial dimensionality reduction of variables and occasions of a three-mode data set are discussed. These techniques can be seen as the simultaneous version of two procedures based on the sequential application of k-means and Tucker2 algorithms and vice versa. The two techniques, T3Clus and 3Fk-means, have been compared theoretically and empirically by a simulation study. In the latter, it has been noted that neither T3Clus nor 3Fk-means outperforms the other in every case. From these results rises the idea to combine the two techniques in a unique general model, named CT3Clus, having T3Clus and 3Fk-means as special cases. A simulation study follows to show the effectiveness of the proposal.","Cluster Model, Component Score, Cluster Structure, Noise Variable, Cluster Dimension",https://link.springer.com//article/10.1007/s00357-007-0006-x
Initializing K-means Batch Clustering: A Critical Evaluation of Several Techniques,"K-means clustering is arguably the most popular technique for partitioning data. Unfortunately, K-means suffers from the well-known problem of locally optimal solutions. Furthermore, the final partition is dependent upon the initial configuration, making the choice of starting partitions all the more important. This paper evaluates 12 procedures proposed in the literature and provides recommendations for best practices.","Initial Seed, Initialization Procedure, Initialization Strategy, Random Initialization, Multivariate Behavioral Research",https://link.springer.com//article/10.1007/s00357-007-0003-0
Asymmetric Agglomerative Hierarchical Clustering Algorithms and Their Evaluations,"This paper presents asymmetricagglomerative hierarchical clustering algorithms in an extensive view point. First, we develop a new updating formula for these algorithms, proposing a general framework to incorporate many algorithms. Next we propose measures to evaluate the fit of asymmetric clustering results to data. Then we demonstrate numerical examples with real data, using the new updating formula and the indices of fit. Discussing empirical findings, through the demonstrative examples, we show new insights into the asymmetric clustering.","Cluster Result, Linkage Algorithm, Brand Switching, Asymmetric Data, Combine Distance",https://link.springer.com//article/10.1007/s00357-007-0002-1
Recent Advances in Predictive (Machine) Learning,Prediction involves estimating the unknown value of an attribute of a system under study given the values of other measured attributes. In prediction (machine) learning the prediction rule is derived from data consisting of previously solved cases. Most methods for predictive learning were originated many years ago at the dawn of the computer age. Recently two new techniques have emerged that have revitalized the field. These are support vector machines and boosted decision trees. This paper provides an introduction to these two new methods tracing their respective ancestral roots to standard kernel methods and ordinary decision trees.,"Support Vector Machine, Target Function, Kernel Method, Boost Decision Tree, Training Sample Size",https://link.springer.com//article/10.1007/s00357-006-0012-4
Confidence in Classification: A Bayesian Approach,"Bayesian classification is currently of considerable interest. It provides a strategy for eliminating the uncertainty associated with a particular choice of classifiermodel parameters, and is the optimal decision-theoretic choice under certain circumstances when there is no single “true” classifier for a given data set. Modern computing capabilities can easily support the Markov chain Monte Carlo sampling that is necessary to carry out the calculations involved, but the information available in these samples is not at present being fully utilised. We show how it can be allied to known results concerning the “reject option” in order to produce an assessment of the confidence that can be ascribed to particular classifications, and how these confidence measures can be used to compare the performances of classifiers. Incorporating these confidence measures can alter the apparent ranking of classifiers as given by straightforward success or error rates. Several possible methods for obtaining confidence assessments are described, and compared on a range of data sets using the Bayesian probabilistic nearest-neighbour classifier.","Posterior Probability, Posterior Distribution, Markov Chain Monte Carlo, Bayesian Approach, Markov Chain Monte Carlo Sample",https://link.springer.com//article/10.1007/s00357-006-0013-3
PARAMAP vs. Isomap: A Comparison of Two Nonlinear Mapping Algorithms,"Dimensionality reduction techniques are used for representing higher dimensional data by a more parsimonious and meaningful lower dimensional structure. In this paper we will study two such approaches, namely Carroll’s Parametric Mapping (abbreviated PARAMAP) (Shepard and Carroll, 1966) and Tenenbaum’s Isometric Mapping (abbreviated Isomap) (Tenenbaum, de Silva, and Langford, 2000). The former relies on iterative minimization of a cost function while the latter applies classical MDS after a preprocessing step involving the use of a shortest path algorithm to define approximate geodesic distances. We will develop a measure of congruence based on preservation of local structure between the input data and the mapped low dimensional embedding, and compare the different approaches on various sets of data, including points located on the surface of a sphere, some data called the ""Swiss Roll data"", and truncated spheres.","Agreement Rate, Nonlinear Dimensionality Reduction, Input Configuration, Regular Sphere, Output Configuration",https://link.springer.com//article/10.1007/s00357-006-0014-2
On the Performance of Simulated Annealingfor Large-Scale L2 Unidimensional Scaling,"In this research note, I present a modified version of G. De Soete, L. Hubert, and P. Arabie’s (1988) simulated annealing approach for the problem of L2 unidimensional scaling via maximization of the Defays criterion. The modifications include efficient storage and computation methods that facilitate rapid evaluation of trial solutions. The results of two experimental studies indicate that the enhanced simulated annealing algorithm is competitive with A. Murillo, J.F. Vera, and W.J. Heiser’s (2005) recently published pertsaus2 procedure in terms of solution quality and computation time. Both Fortran and MatLab versions of this modified simulated annealing implementation are available from the author.","Simulated Annealing, Test Problem, Simulated Annealing Algorithm, Trial Solution, Efficient Storage",https://link.springer.com//article/10.1007/s00357-006-0015-1
"CLUSCALE (""CLUstering and multidimensional SCAL[E]ing""): A Three-Way Hybrid Model Incorporating Overlapping Clustering and Multidimensional Scaling Structure","Traditional techniques of perceptual mapping hypothesize that stimuli are differentiated in a common perceptual space of quantitative attributes. This paper enhances traditional perceptual mapping techniques such as multidimensional scaling (MDS) which assume only continuously valued dimensions by presenting a model and methodology called CLUSCALE for capturing stimulus differentiation due to perceptions that are qualitative, in addition to quantitative or continuously varying perceptual attributes or dimensions. It provides models and OLS parameter estimation procedures for both a two-way and a three-way version of this general model. Since the two-way version of the model and method has already been discussed by Chaturvedi and Carroll (2000), and a stochastic variant discussed by Navarro and Lee (2003), we shall deal in this paper almost entirely with the three-way version of this model. We recommend the use of the three-way approach over the two-way approach, since the three-way approach both accounts for and takes advantage of the heterogeneity in subjects’ perceptions of stimuli to provide maximal information; i.e., it explicitly deals with individual differences among subjects.","Multidimensional Scaling, Quantitative Dimension, Perceptual Mapping, Discrete Feature, Proximity Data",https://link.springer.com//article/10.1007/s00357-006-0016-0
On Similarity Indices and Correction for Chance Agreement,"Similarity indices can be used to compare partitions (clusterings) of a data set. Many such indices were introduced in the literature over the years. We are showing that out of 28 indices we were able to track, there are 22 different ones. Even though their values differ for the same clusterings compared, after correcting for agreement attributed to chance only, their values become similar and some of them even become equivalent. Consequently, the problem of choice of the index to be used for comparing different clusterings becomes less important.","Cluster Size, Similarity Index, American Statistical Association, Chance Agreement, Similarity Table",https://link.springer.com//article/10.1007/s00357-006-0017-z
Generation of Random Clusters with Specified Degree of Separation,"We propose a random cluster generation algorithm that has the desired features: (1) the population degree of separation between clusters and the nearest neighboring clusters can be set to a specified value, based on a separation index; (2) no constraint is imposed on the isolation among clusters in each dimension; (3) the covariance matrices correspond to different shapes, diameters and orientations; (4) the full cluster structures generally could not be detected simply from pair-wise scatterplots of variables; (5) noisy variables and outliers can be imposed to make the cluster structures harder to be recovered. This algorithm is an improvement on the method used in Milligan (1985).","Covariance Matrice, Cluster Structure, Random Cluster, Neighboring Cluster, Factorial Experiment Design",https://link.springer.com//article/10.1007/s00357-006-0018-y
The Practice of Cluster Analysis,"Cluster analysis is one of the main methodologies for analyzing multivariatedata. Its use is widespread and growing rapidly. The goal of this article is to documentthis growth, characterize current usage, illustrate the breadth of applications viaexamples, highlight both good and risky practices, and suggest some research priorities.","Cluster Analysis, Hierarchical Cluster Analysis, Social Science Citation Index, Science Citation Index Expand, Humanity Citation Index",https://link.springer.com//article/10.1007/s00357-006-0002-6
Astrocladistics: A Phylogenetic Analysis of Galaxy Evolution I. Character Evolutions and Galaxy Histories,"This series of papers is intended to present astrocladistics in some detail and evaluate this methodology in reconstructing phylogenies of galaxies. Being based on the evolution of all the characters describing galaxies, it is an objective way of understanding galaxy diversity through evolutionary relationships. In this first paper, we present the basic steps of a cladistic analysis and show both theoretically and practically that it can be applied to galaxies. For illustration, we use a sample of 50 simulated galaxies taken from the GALICS database, which are described by 91 observables (dynamics, masses and luminosities). These 50 simulated galaxies are indeed 10 different galaxies taken at 5 cosmological epochs, and they are free of merger events. The astrocladistic analysis easily reconstructs the true chronology of evolution relationships within this sample. It also demonstrates that burst characters are not relevant for galaxy evolution as a whole. A companion paper is devoted to the formalization of the concepts of formation and diversification in galaxy evolution.","Retention Index, Consistency Index, Parsimonious Tree, Cladistic Analysis, Character Evolution",https://link.springer.com//article/10.1007/s00357-006-0003-5
Astrocladistics: A Phylogenetic Analysis of Galaxy Evolution II. Formation and Diversification of Galaxies,"This series of papers is intended to evaluate astrocladistics in reconstructing phylogenies of galaxies. The objective of this second paper is to formalize the concept of galaxy formation and to identify the processes of diversification. We show that galaxy diversity can be expected to organize itself in a hierarchy. In order to better understand the role of mergers, we have selected a sample of 43 galaxies from the GALICS database built from simulations with a hybrid model for galaxy formation studies. These simulated galaxies, described by 119 characters and considered as representing still undefined classes, have experienced different numbers of merger events during evolution. Our cladistic analysis yields a robust tree that proves the existence of a hierarchy. Mergers, like interactions (not taken into account in the GALICS simulations), are probably a strong driver for galaxy diversification. Our result shows that mergers participate in a branching type of evolution, but do not seem to play the role of an evolutionary clock.","Cladistic Analysis, Star Formation Rate, Galaxy Formation, Basic Constituent, Galaxy Evolution",https://link.springer.com//article/10.1007/s00357-006-0004-4
On the Complexity of Ordinal Clustering,"Given a set of pairwise distances on a set of n points, constructing an edgeweighted tree whose leaves are these n points such that the tree distances would mimic the original distances under some criteria is a fundamental problem. One such criterion is to preserve the ordinal relation between the pairwise distances. The ordinal relation can be of the form of total order on the distances or it can be some partial order specified on the pairwise distances. We show that the problem of finding a weighted tree, if it exists, which would preserve the total order on pairwise distances is NP-hard. We also show the NP-hardness of the problem of finding a weighted tree which would preserve a particular kind of partial order called a triangle order, one of the most fundamental partial orders considered in computational biology.","Partial Order, Pairwise Distance, Total Order, Weighted Tree, Distance Pair",https://link.springer.com//article/10.1007/s00357-006-0005-3
Maximum Transfer Distance Between Partitions,"In this paper, we study a distance defined over the partitions of a finite set. Given two partitions P and Q, this distance is defined as the minimum number of transfers of an element from one class to another, required to transform P into Q. We recall the algorithm to evaluate this distance and we give some formulae for the maximum distance value between two partitions having exactly or at most p and q classes, for given p and q.","Perfect Match, Assignment Problem, Graph Partitioning, Rand Index, Transfer Distance",https://link.springer.com//article/10.1007/s00357-006-0006-2
On Classification and Regression Trees for Multiple Responses and Its Application,"In many application fields, multivariate approaches that simultaneously consider the correlation between responses are needed. The tree method can be extended to multivariate responses, such as repeated measure and longitudinal data, by modifying the split function so as to accommodate multiple responses. Recently, researchers have constructed some decision trees for multiple continuous longitudinal response and multiple binary responses using Mahalanobis distance and a generalized entropy index. However, these methods have limitations according to the type of response, that is, those that are only continuous or binary. In this paper, we will modify the tree for univariate response procedure and suggest a new tree-based method that can analyze any type of multiple responses by using GEE (generalized estimating equations) techniques. To compare the performance of trees, simulation studies on selection probability of true split variable will be shown. Finally, applications using epileptic seizure data and WWW data are introduced.","Regression Tree, Epileptic Seizure, Generalize Estimate Equation, Terminal Node, Multiple Response",https://link.springer.com//article/10.1007/s00357-006-0007-1
On a Transvariation Based Measure of Group Separability,"In this paper, the potentialities of transvariation (Gini, 1959) in measuring the separation between two groups of multivariate observations are explored. With this aim, a modified version of Gini’s notion of multidimensional transvariation is proposed. According to Gini (1959), two groups G1 and G2 are said to transvary on the k-dimensional variable X = (X1,...,Xh,...,Xk) if there exists at least one pair of units, belonging to different groups, such that for h = 1,...,k the sign of the difference between their Xh values is opposite to that of m1h −m2h, where m1h and m2h are the corresponding group mean values of Xh. We introduce a modification that allows us to derive a measure of group separation, which can be profitably used in discriminating between two groups. The performance of the measure is tested through simulation experiments. The results show that the proposed measure is not sensitive to distributional assumptions and highlight its robustness against outliers.","Income Distribution, Mahalanobis Distance, Group Separability, Linear Discriminant Function, Separability Measure",https://link.springer.com//article/10.1007/s00357-006-0008-0
OCLUS: An Analytic Method for Generating Clusters with Known Overlap,"The primary method for validating cluster analysis techniques is throughMonteCarlo simulations that rely on generating data with known cluster structure (e.g., Milligan1996). This paper defines two kinds of data generation mechanisms with cluster overlap,marginal and joint; current cluster generation methods are framed within these definitions.An algorithm generating overlapping clusters based on shared densities from several differentmultivariate distributions is proposed and shown to lead to an easily understandablenotion of cluster overlap. Besides outlining the advantages of generating clusters withinthis framework, a discussion is given of how the proposed data generation technique canbe used to augment research into current classification techniques such as finite mixturemodeling, classification algorithm robustness, and latent profile analysis.","Cluster Generation, Joint Distribution, American Statistical Association, Skewed Data, Triangular Distribution",https://link.springer.com//article/10.1007/s00357-005-0015-6
Rotation in Correspondence Analysis,"In correspondence analysis rows and columns of a nonnegative data matrix aredepicted as points in a, usually, two-dimensional plot. Although such a two-dimensionalplot often provides a reasonable approximation, the situation can occur that an approximationof higher dimensionality is required. This is especially the case when the datamatrix is large. In such instances it may become difficult to interpret the solution. Similarto what is done in principal component analysis and factor analysis the correspondenceanalysis solution can be rotated to increase the interpretability. However, due to the variousscaling options encountered in correspondence analysis, there are several alternativeoptions for rotating the solutions. In this paper we consider two options for rotation incorrespondence analysis. An example is provided so that the benefits of rotation becomeapparent.","Correspondence Analysis, Simple Structure, Attribute Category, Absolute Contribution, Coordinate Matrix",https://link.springer.com//article/10.1007/s00357-005-0016-5
A Comparison of Two Methods for Fitting the INDCLUS Model,"Chaturvedi and Carroll have proposed the SINDCLUS method for fittingthe INDCLUS model. It is based on splitting the two appearances of the cluster matrixin the least squares fit function and relying on convergence to a solution where bothcluster matrices coincide. Kiers has proposed an alternative method which preservesequality of the cluster matrices throughout. This paper shows that the latter method isgenerally to be preferred. However, because the method has a serious local minimumproblem, alternative approaches should be contemplated.","Global Minimum, Additive Constant, Average Computation Time, Local Minimum Problem, Cluster Matrix",https://link.springer.com//article/10.1007/s00357-005-0017-4
On the Uniqueness of the Selection Criterion in Neighbor-Joining,"The Neighbor-Joining (NJ) method of Saitou and Nei is the most widely useddistance based method in phylogenetic analysis. Central to the method is the selectioncriterion, the formula used to choose which pair of objects to amalgamate next. Herewe analyze the NJ selection criterion using an axiomatic approach. We show that anyselection criterion that is linear, permutation equivariant, statistically consistent and basedsolely on distance data will give the same trees as those created by NJ.","Phylogenetic Analysis, Selection Criterion, Base Method, Axiomatic Approach, Distance Data",https://link.springer.com//article/10.1007/s00357-005-0003-x
A Hierarchical Methodology for Class Detection Problems with Skewed Priors,"We describe a novel extension to the Class-Cover-Catch-Digraph (CCCD)classifier, specifically tuned to detection problems. These are two-class classificationproblems where the natural priors on the classes are skewed by several orders of magnitude.The emphasis of the proposed techniques is in computationally efficient classificationfor real-time applications. Our principal contribution consists of two boosted classi-fiers built upon the CCCD structure, one in the form of a sequential decision process andthe other in the form of a tree. Both of these classifiers achieve performances comparableto that of the original CCCD classifiers, but at drastically reduced computational expense.An analysis of classification performance and computational cost is performed using datafrom a face detection application. Comparisons are provided with Support Vector Machines(SVM) and reduced SVMs. These comparisons show that while some SVMs mayachieve higher classification performance, their computational burden can be so high as tomake them unusable in real-time applications. On the other hand, the proposed classifierscombine high detection performance with extremely fast classification.","Support Vector Machine, Classification Performance, Face Detection, Detection Problem, Computational Expense",https://link.springer.com//article/10.1007/s00357-005-0004-9
Minimal Sample Size in the Group Classification Problem,"Bayes classification procedure for a group of independent vectors treated as awhole is considered. When the distributions are not specified, we obtain the bounds of theminimal sample size based on the Chernoff and the Bhattacharyya distances between thepopulations. The case of the normal distribution is also discussed.","Normal Distribution, Classification Problem, Minimal Sample, Group Classification, Minimal Sample Size",https://link.springer.com//article/10.1007/s00357-005-0005-8
Extensions of Biplot Methodology to Discriminant Analysis,"In this paper we show how biplot methodology can be combined withvarious forms of discriminant analyses leading to highly informative visual displays ofthe respective class separations. It is demonstrated that the concept of distance asapplied to discriminant analysis provides a unified approach to a wide variety ofdiscriminant analysis procedures that can be accommodated by just changing to anappropriate distance metric. These changes in the distance metric are crucial for theconstruction of appropriate biplots. Several new types of biplots viz. quadraticdiscriminant analysis biplots for use with heteroscedastic stratified data, discriminantsubspace biplots and flexible discriminant analysis biplots are derived and their useillustrated. Advantages of the proposed procedures are pointed out. Although biplotmethodology is in particular well suited for complementing J > 2 classes discriminationproblems its use in 2-class problems is also illustrated.","Analysis Procedure, Discriminant Analysis, Unify Approach, Visual Display, Class Separation",https://link.springer.com//article/10.1007/s00357-005-0006-7
Combinatorial Representations of Token Sequences,"This paper presents new representations of token sequences, with and withoutassociated quantities, in Euclidean space. The representations are free of assumptionsabout the nature of the sequences or the processes that generate them. Algorithms andapplications from the domains of structured interviews and life histories are discussed.","Life History, Euclidean Space, Structure Interview, Combinatorial Representation, Token Sequence",https://link.springer.com//article/10.1007/s00357-005-0007-6
A Permutation-Translation Simulated Annealing Algorithm for L1 and L2 Unidimensional Scaling,"Given a set of objects and a symmetric matrix of dissimilarities between them,Unidimensional Scaling is the problem of finding a representation by locating points on acontinuum. Approximating dissimilarities by the absolute value of the difference betweencoordinates on a line constitutes a serious computational problem. This paper presents analgorithm that implements Simulated Annealing in a new way, via a strategy based on aweighted alternating process that uses permutations and point-wise translations to locatethe optimal configuration. Explicit implementation details are given for least squares lossfunctions and for least absolute deviations. The weighted, alternating process is shownto outperform earlier implementations of Simulated Annealing and other optimizationstrategies for Unidimensional Scaling in run time efficiency, in solution quality, or inboth.","Simulated Annealing, Absolute Deviation, Optimization Strategy, Loss Function, Symmetric Matrix",https://link.springer.com//article/10.1007/s00357-005-0008-5
"On Ultrametricity, Data Coding, and Computation","The triangular inequality is a defining property of a metric space, while thestronger ultrametric inequality is a defining property of an ultrametric space. Ultrametricdistance is defined from p-adic valuation. It is known that ultrametricity is a naturalproperty of spaces in the sparse limit. The implications of this are discussed in this article.Experimental results are presented which quantify how ultrametric a given metric spaceis. We explore the practical meaningfulness of this property of a space being ultrametric.In particular, we examine the computational implications of widely prevalent and perhapsubiquitous ultrametricity.","Natural Property, Data Code, Ultrametric Space, Practical Meaningfulness, Triangular Inequality",https://link.springer.com//article/10.1007/s00357-004-0015-y
"Percept Variance, Subadditivity and the Metric Classification of Similarity, and Dissimilarity Data",Percept variance is shown to change the additive property of city-blockdistances and make city-block distances more subadditive than Euclidean distances.Failure to account for percept variance will result in the misclassification of city-blockdata as Euclidean. A maximum likelihood estimation procedure is proposed for themultidimensional scaling of similarity data characterized by percept variance. MonteCarlo and empirical experiments are used to evaluate the proposed approach.,"Euclidean Distance, Maximum Likelihood Estimation, Likelihood Estimation, Estimation Procedure, Similarity Data",https://link.springer.com//article/10.1007/s00357-004-0016-x
Improving Dynamic Programming Strategies for Partitioning,"Improvements to the dynamic programming (DP) strategy for partitioning (nonhierarchical classification) as discussed in Hubert, Arabie, and Meulman (2001) are proposed. First, it is shown how the number of evaluations in the DP process can be decreasedwithout affecting generality. Both a completely nonredundant and a quasi-nonredundantmethod are proposed. Second, an efficient implementation of both approaches is discussed.This implementation is shown to have a dramatic increase in speed over the original program.The flexibility of the approach is illustrated by analyzing three data sets.","Dynamic Programming, Efficient Implementation, Original Program, Program Strategy, Dynamic Program Strategy",https://link.springer.com//article/10.1007/s00357-004-0017-9
Model-Based Clustering for Image Segmentation and Large Datasets via Sampling,"The rapid increase in the size of data sets makes clustering all the more importantto capture and summarize the information, at the same time making clustering moredifficult to accomplish. If model-based clustering is applied directly to a large data set, itcan be too slow for practical application. A simple and common approach is to first clustera random sample of moderate size, and then use the clustering model found in this wayto classify the remainder of the objects. We show that, in its simplest form, this methodmay lead to unstable results. Our experiments suggest that a stable method with better performance can be obtained with two straightforward modifications to the simple samplingmethod: several tentative models are identified from the sample instead of just one, andseveral EM steps are used rather than just one E step to classify the full data set. We findthat there are significant gains from increasing the size of the sample up to about 2,000,but not from further increases. These conclusions are based on the application of severalalternative strategies to the segmentation of three different multispectral images, and toseveral simulated data sets.","Simulated Data, Large Data, Image Segmentation, Large Dataset, Alternative Strategy",https://link.springer.com//article/10.1007/s00357-004-0018-8
Oscillation Heuristics for the Two-group Classification Problem,"We propose a new nonparametric family of oscillation heuristics for improvinglinear classifiers in the two-group discriminant problem. The heuristics are motivated bythe intuition that the classification accuracy of a separating hyperplane can be improvedthrough small perturbations to its slope and position, accomplished by substituting trainingobservations near the hyperplane for those used to generate it. In an extensive simulationstudy, using data generated from multivariate normal distributions under a variety of conditions,the oscillation heuristics consistently improve upon the classical linear and logisticdiscriminant functions, as well as two published linear programming-based heuristics anda linear Support Vector Machine. Added to any of the methods above, they approach, andfrequently attain, the best possible accuracy on the training samples, as determined by amixed-integer programming (MIP) model, at a much smaller computational cost. Theyalso improve expected accuracy on the overall populations when the populations overlapsignificantly and the heuristics are trained with large samples, at least in situations wherethe data conditions do not explicitly favor a particular classifier.","Support Vector Machine, Classification Accuracy, Discriminant Function, Extensive Simulation, Multivariate Normal Distribution",https://link.springer.com//article/10.1007/s00357-004-0019-7
K-modes Clustering,"We norm (defined as the limit of an Lp norm as p approaches zero).In Monte Carlo simulations, both K-modes and the latent class procedures (e.g., Goodman 1974) performed with equal efficiency in recovering a known underlying cluster structure. However, K-modes is an order of magnitude faster than  the latent class procedure in speed and suffers from fewer problems of local optima than do the latent class procedures.  For data sets involving a large number of categorical variables, latent class procedures become computationally extremly slow and hence infeasible.  We conjecture that, although in some cases latent class procedures might perform better than K-modes, it could out-perform latent class procedures in other cases. Hence, we recommend that these two approaches be used as ""complementary"" procedures in performing cluster analysis. We also present an empirical comparison of K-modes and latent class, where the former method prevails.     ",,https://link.springer.com//article/10.1007/s00357-001-0004-3
An Evaluation of Two Algorithms for Hierarchical Classes Analysis ,"In this procedure, a least-squares loss function in terms of discrepancies between D and M is minimized. The present paper describes the original hierarchical classes algorithm proposed by De Boeck and Rosenberg (1988), which is based on an alternating greedy heuristic, and proposes a new algorithm, based on an alternating branch-and-bound procedure. An extensive simulation study is reported in which both algorithms are evaluated and compared according to goodness-of-fit to the data and goodness-of-recovery of the underlying true structure. Furthermore, three heuristics for selecting models of different ranks for a given D are presented and compared.  The simulation results show that the new algorithm yields models with slightly higher goodness-of-fit and goodness-of-recovery values.",,https://link.springer.com//article/10.1007/s00357-001-0005-2
GLIMMIX: Software for Estimating Mixtures and Mixtures of Generalized Linear Models,"GLIMMIX is a commercial WINDOWS-based computer program that implements the EM algorithm (Dempster, Laird and Rubin 1977) for the estimation of finite mixtures and mixtures of generalized linear models. The program allows for the specification of a number of distributions in the exponential family, including the normal, gamma, binomial, Poisson, and multinomial distributions. For each of those distributions, a variety of link functions can be specified to relate the expectation of the dependent variable to a linear predictor. Several statistics, including AIC, CAlC and BIC are computed to aid in model selection (cf. Akaike 1974; Bozdogan 1987), missing values are accommodated, and posterior membership probabilities are computed for cases, included or not included in the analysis. Simple discriminant type models dealing with concomitant variables to describe the classes are supported, and a random responder class can be added to the model. Various graphs are provided. A demonstration version ofthe program can be obtained from http://www/ganuna.rug.nl. Before providing some details on the GLIMMIX software, a brief review of a few relevant issues in Mixture modelling are provided.",,https://link.springer.com//article/10.1007/s0357-001-0008-z
Morph-Based Local-Search Heuristics for Large-Scale Combinatorial Data Analysis,"There are a variety of data analysis techniques in the social and behavioral sciences that require the solution of NP-complete optimization problems. Unfortunately, optimal solution methods are generally intractable for problems of practical size and thus there has been an emphasis on the development of heuristic procedures. Although local-search procedures, such as simulated annealing, have been tested on several combinatorial data analysis problems, they have frequently been criticized as computationally inefficient and therefore impractical for large problems. This paper presents a process called ‘morphing’ that can substantially increase the efficiency and effectiveness of local-search heuristics. The new procedure is compared to replications of a heuristic battery of local-operations across a set of large metric unidimensional scaling (seriation) problems. Generalizations of the morphing process to other problems in combinatorial data analysis are also discussed.",Key words: Local-search methods; Combinatorial data analysis; Unidimensional scaling; Seriation.,https://link.springer.com//article/10.1007/s003579900052
Nonmetric Linear Biplots,"A methodology is developed for constructing linear biplots for a class of nonmetric multidimensional scaling methods for multivariate data. The nonlinear transformations of nonmetric scaling manifest themselves in irregularly spaced calibration markers. Two approaches are examined, one based on Procrustean embedding, the other on a modification of the popular regression method. The widespread use of an unmodified regression method in association with nonlinear transformations is questioned. An example is given. The methodology presented here could potentially be developed to give an optimal represention of a matrix in fewer geometric dimensions than its rank.","Biplots, calibration, Distance-based multivariate analysis, Least-squares scaling, Matrix approximation, Non-linear multivariate analysis, Non-metric multidimensional scaling, Ordination, Orthogonal Procrustes analysis, Principal components analysis, Regression method for biplots",https://link.springer.com//article/10.1007/s003579900053
Shape Statistics: Procrustes Superimpositions and Tangent Spaces,"The shape of a set of labeled points corresponds to those attributes of the configuration that are invariant to the effects of translation, rotation, and scale. Procrustes distance may be used to compare different shapes and also serve as a metric that may be used to define multidimensional shape spaces. This paper demonstrates that the preshape space of planar triangles Procrustes aligned to a reference triangle corresponds to a unit hemisphere. An overview of methods used as linear approximations of D. G. Kendall's non-Euclidean shape space is given, and the equivalence of several methods based on orthogonal projections is shown. Some problems with approximations based on stereo graphic projections are also discussed. A simple example using artificial data is included.",Key words: Kendall shape space; Preshape space; Tangent space; Partial warps; Multivariate analysis.,https://link.springer.com//article/10.1007/s003579900054
Global Optimization in Least-Squares Multidimensional Scaling by Distance Smoothing,"Least-squares multidimensional scaling is known to have a serious problem of local minima, especially if one dimension is chosen, or if city-block distances are involved. One particular strategy, the smoothing strategy proposed by Pliner (1986, 1996), turns out to be quite successful in these cases. Here, we propose a slightly different approach, called distance smoothing. We extend distance smoothing for any Minkowski distance. In addition, we extend the majorization approach to multidimensional scaling to have a one-step update for Minkowski parameters larger than 2 and use the results for distance smoothing. We present simple ideas for finding quadratic majorizing functions. The performance of distance smoothing is investigated in several examples, including two simulation studies.",Key words: Multidimensional scaling; Minkowski distances; Global optimization; Smoothing; Majorization.,https://link.springer.com//article/10.1007/s003579900055
An Extraction and Regularization Approach to Additive Clustering,"Additive clustering provides a conceptually simple similarity model which is, nevertheless, capable of accommodating arbitrary similarity structures. The discrete nature of the clusters, coupled with the general flexibility of the model, however, means that the derivation of additive clustering models from given similarity data is difficult. After reviewing a number of previously developed algorithms, a new two stage algorithm for generating additive cluster models is developed. In the first stage, an extraction process generates a manageable number of candidate clusters which, in the second stage, are subject to a regularization process. The number of clusters included in the derived model is controlled by a parameter specifying the target level of variance to be accounted for by the final model. Several applications of the proposed algorithm are presented, including three involving previously examined data sets that facilitate an evaluation of performance relative to several other algorithms. It is argued that the proposed algorithm exhibits comparable performance in relation to these previous algorithms, and has the advantage of being developed within a framework that potentially allows the optimization of the tradeoff between goodness-of-fit and model parsimony.",Key words: Overlapping clustering; Additive clustering; ADCLUS.,https://link.springer.com//article/10.1007/s003579900056
Fitting a Mixture Model to Three-Mode Three-Way Data with Categorical and Continuous Variables,"The mixture likelihood approach to clustering is most often used with two-mode two-way data to cluster one of the modes (e.g., the entities) into homogeneous groups on the basis of the other mode (e.g., the attributes). In this case, the attributes can either be continuous or categorical. When the data set consists of a three-mode three-way array (e.g., attributes measured on entities in different situations), an analogous procedure is needed to enable the clustering of the entities (i.e., one of the modes) on the basis of both of the other modes simultaneously (i.e., the attributes measured in different situations). In this paper, it is shown that the finite mixture approach to clustering can be extended to analyze three-mode threeway data where some of the attributes are continuous and some are categorical. The methodology is illustrated by clustering the genotypes in a three-way soybean data set where various attributes were measured on genotypes grown in several environments.",Key words: Clustering; Finite mixture models; Mixed data.,https://link.springer.com//article/10.1007/s003579900057
Wedding the Wavelet Transform and Multivariate Data Analysis," We discuss the use of orthogonal wavelet transforms in preprocessing multivariate data for subsequent analysis, e.g., by clustering the dimensionality reduction.  Wavelet transforms allow us to introduce multiresolution approximation, and multiscale nonparametric regression or smoothing, in a natural and integrated way into the data analysis.  As will be explained in the first part of the paper, this approach is of greatest interest for multivariate data analysis when we use (i) datasets with ordered variables, e.g., time series, and (ii) object dimensionalities which are not too small, e.g., 16 and upwards.  In the second part of the paper, a different type of wavelet decomposition is used.  Applications illustrate the powerfulness of this new perspective on data analysis.","Data Analysis, Time Series, Dimensionality Reduction, Wavelet Transform, Multivariate Data",https://link.springer.com//article/10.1007/s003579900029
Construction of Efficient Identification Schemes Using Batches of Discrete-Valued Tests," Well established methods are available for identifying a specimen from a known set of taxa by applying either a single set of tests, or a hierarchical sequence of single tests.  These are relevant respectively when the cost of the tests or the time required for identification is most important.  Often, however, a mixed strategy is required which aims to reduce the cost of identification while avoiding taking too much time.  Methods are derived for constructing efficient identification schemes which involve the sequential use of batches of tests.  Also, the special case is considered where an initial batch of tests is used, followed by a second batch whose composition depends on the results observed for the initial batch.","Identification Scheme, Mixed Strategy, Single Test, Efficient Identification, Initial Batch",https://link.springer.com//article/10.1007/s003579900030
Metric Models for Random Graphs," Many problems entail the analysis of data that are independent and identically distributed random graphs.  Useful inference requires flexible probability models for such random graphs; these models should have interpretable location and scale parameters, and support the establishment of confidence regions, maximum likelihood estimates, goodness-of-fit tests, Bayesian inference, and an appropriate analogue of linear model theory.  Banks and Carley (1994) develop a simple probability model and sketch some analyses; this paper extends that work so that analysts are able to choose models that reflect application-specific metrics on the set of graphs.  The strategy applies to graphs, directed graphs, hypergraphs, and trees, and often extends to objects in countable metric spaces.","Linear Model, Maximum Likelihood Estimate, Probability Model, Likelihood Estimate, Scale Parameter",https://link.springer.com//article/10.1007/s003579900031
Mixture Model Analysis of Complex Samples, We investigate the effects of a complex sampling design on the estimation of mixture models.  An approximate or pseudo likelihood approach is proposed to obtain consistent estimates of class-specific parameters when the sample arises from such a complex design.  The effects of ignoring the sample design are demonstrated empirically in the context of an international value segmentation study in which a multinomial mixture model is applied to identify segment-level value rankings.  The analysis reveals that ignoring the sample design results in both an incorrect number of segments as identified by information criteria and biased estimates of segment-level parameters.,"Information Criterion, Mixture Model, Sample Design, Complex Sample, Consistent Estimate",https://link.springer.com//article/10.1007/s003579900032
Joint Orthomax Rotation of the Core and Component Matrices Resulting from Three-mode Principal Components Analysis," The analysis of a three-way data set using three-mode principal components analysis yields component matrices for all three modes of the data, and a three-way array called the core, which relates the components for the different modes to each other.  To exploit rotational freedom in the model, one may rotate the core array (over all three modes) to an optimally simple form, for instance by three-mode orthomax rotation.  However, such a rotation of the core may inadvertently detract from the simplicity of the component matrices.  One remedy is to rotate the core only over those modes in which no simple solution for the component matrices is desired or available, but this approach may in turn reduce the simplicity of the core to an unacceptable extent.  In the present paper, a general approach is developed, in which a criterion is optimized that not only takes into account the simplicity of the core, but also, to any desired degree, the simplicity of the component matrices.  This method (in contrast to methods for either core or component matrix rotation) can be used to find solutions in which the core and the component matrices are all reasonably simple.","Principal Component Analysis, Simple Form, Component Matrix, Simple Solution, Matrix Rotation",https://link.springer.com//article/10.1007/s003579900033
Partitions of Partitions," The paper presents methodology for analyzing a set of partitions of the same set of objects, by dividing them into classes of partitions that are similar to one another.  Two different definitions are given for the consensus partition which summarizes each class of partitions.  The classes are obtained using either constrained or unconstrained clustering algorithms.  Two applications of the methodology are described.","Cluster Algorithm, Consensus Partition, Unconstrained Cluster",https://link.springer.com//article/10.1007/s003579900034
A Nonlinear Programming Approach to Metric Unidimensional Scaling," Classical unidimensional scaling provides a difficult combinatorial task.  A procedure formulated as a nonlinear programming (NLP) model is proposed to solve this problem.  The new method can be implemented with standard mathematical programming software.  Unlike the traditional procedures that minimize either the sum of squared error (L2 norm) or the sum pf absolute error (L1 norm), the proposed method can minimize the error based on any Lp norm for 1 ≤p < ∞. Extensions of the NLP formulation to address a multidimensional scaling problem under the city-block model are also discussed.","Absolute Error, Mathematical Programming, Programming Software, Multidimensional Scaling, Programming Approach",https://link.springer.com//article/10.1007/s003579900017
A New Formulation of the Nonmetric Strain Problem in Multidimensional Scaling," A natural extension of classical metric multidimensional scaling is proposed.  The result is a new formulation of nonmetric multidimensional scaling in which the strain criterion is minimized subject to order constraints on the disparity variables.  Innovative features of the new formulation include: the parametrization of the p-dimensional distance matrices by the positive semidefinite matrices of rank ≤p; optimization of the (squared) disparity variables, rather than the configuration coordinate variables; and a new nondegeneracy constraint, which restricts the set of (squared) disparities rather than the set of distances.  Solutions are obtained using an easily implemented gradient projection method for numerical optimization.  The method is applied to two published data sets.","Multidimensional Scaling, Projection Method, Natural Extension, Strain Criterion, Numerical Optimization",https://link.springer.com//article/10.1007/s003579900018
Minimum Sum of Squares Clustering in a Low Dimensional Space," Clustering with a criterion which minimizes the sum of squared distances to cluster centroids is usually done in a heuristic way.  An exact polynomial algorithm, with a complexity in O(Np+1 logN), is proposed for minimum sum of squares hierarchical divisive clustering of points in a p-dimensional space with small p.  Empirical complexity is one order of magnitude lower. Data sets with N = 20000 for p = 2, N = 1000 for p = 3, and N = 200 for p = 4 are clustered in a reasonable computing time. ","Computing Time, Dimensional Space, Polynomial Algorithm, Reasonable Computing, Reasonable Computing Time",https://link.springer.com//article/10.1007/s003579900019
Self-Consistent Patterns for Symmetric Multivariate Distributions," The set of k points that optimally represent a distribution in terms of mean squared error have been called principal points (Flury 1990).  Principal points are a special case of self-consistent points.  Any given set of k distinct points in Rp induce a partition of Rp into Voronoi regions or domains of attraction according to minimal distance.  A set of k points are called self-consistent for a distribution if each point equals the conditional mean of the distribution over its respective Voronoi region.  For symmetric multivariate distributions, sets of self-consistent points typically form symmetric patterns.  This paper investigates the optimality of different symmetric patterns of self-consistent points for symmetric multivariate distributions and in particular for the bivariate normal distribution.  These results are applied to the problem of estimating principal points.","Normal Distribution, Minimal Distance, Distinct Point, Multivariate Distribution, Bivariate Normal Distribution",https://link.springer.com//article/10.1007/s003579900020
Subspace Projection for Multivariate Selection Problems," A new set of derived variables is proposed for exhibiting grouped multivariate data in a small number of dimensions, in such a way as to highlight `extremeness' of one or more groups relative to the rest of the data.  Such display can provide a useful exploratory tool in multivariate ranking and selection problems. We explore four possible measures of `extremeness', and suggest which one is best for practical application.  We show that the technique can be used to derive either orthogonal or uncorrelated dimensions for any type of input data, and we give an illustrative example of its use.","Input Data, Selection Problem, Multivariate Data, Exploratory Tool, Subspace Projection",https://link.springer.com//article/10.1007/s003579900021
Indexed Dendrograms on random Dissimilarities," This paper studies the random indexed dendograms produced by agglomerative hierarchical algorithms under the non-classifiability hypothesis of independent identically distributed (i.i.d.) dissimilarities.  New tests for classifiability are deduced.  The corresponding test statistics are random variables attached to the indexed dendrograms, such as the indices, the survival time of singletons, the value of the ultrametric between two given points, or the size of classes in the different levels of the dendogram.  For an indexed dendogram produced by the Single Link method on i.i.d. dissimilarities, the distribution of these random variables is computed, thus leading to explicit tests.  For the case of the Average and Complete Link methods, some asymptotic results are presented.  The proofs rely essentially on the theory of random graphs.","Survival Time, Random Graph, Asymptotic Result, Link Method, Explicit Test",https://link.springer.com//article/10.1007/s003579900022
An Unsupervised and Nonparametric Classification Procedure Based on Mixtures with Known Weights," I consider a new problem of classification into n(n ≥ 2) disjoint classes based on features of unclassified data.  It is assumed that the data are grouped into m(M ≥ n) disjoint sets and within each set the distribution of features is a mixture of distributions corresponding to particular classes.  Moreover, the mixing proportions should be known and form a matrix of rank n.  The idea of solution is, first, to estimate feature densities in all the groups, then to solve the linear system for component densities.  The proposed classification method is asymptotically optimal, provided a consistent method of density estimation is used.  For illustration, the method is applied to determining perfusion status in myocardial infarction patients, using creatine kinase measurements. ","Myocardial Infarction, Linear System, Creatine, Creatine Kinase, Density Estimation",https://link.springer.com//article/10.1007/s003579900023
Asymmetric multidimensional scaling of two-mode three-way proximities,"An asymmetric multidimensional scaling model and an associated nonmetric algorithm to analyze two-mode three-way proximities (object × object × source) are introduced. The model consists of a common object configuration and two kinds of weights, i.e., for both symmetry and asymmetry. In the common object configuration, each object is represented by a point and a circle (sphere, hypersphere) in a Euclidean space. The common object configuration represents pairwise proximity relationships between pairs of objects for the ‘group’ of all sources. Each source has its own symmetry weight and a set of asymmetry weights. Symmetry weights represent individual differences among sources of data in symmetric proximity relationships, and asymmetry weights represent individual differences among sources in asymmetric proximity relationships. The associated nonmetric algorithm, based on Kruskal’s (1964b) nonmetric multidimensional scaling algorithm, is an extension of the algorithm for the asymmetric multidimensional scaling of one mode two-way proximities developed earlier (Okada and Imaizumi 1987). As an illustrative example, we analyze intergenerational occupational mobility from 1955 to 1985 in Japan among eight occupational categories.","Asymmetric proximity, Multidimensional scaling, Two-mode three-way data",https://link.springer.com//article/10.1007/s003579900010
Latent class DEDICOM,A probabilistic DEDICOM model was proposed for mobility tables. The model attempts to explain observed transition probabilities by a latent mobility table and a set of transition probabilities from latent classes to observed classes. The model captures asymmetry in observed mobility tables by asymmetric latent mobility tables. It may be viewed as a special case of both the latent class model and DEDICOM with special constraints. A maximum penalized likelihood (MPL) method was developed for parameter estimation. The EM algorithm was adapted for the MPL estimation. Two examples were given to illustrate the proposed method.,"Mobility tables, Latent class models, Maximum penalized likelihood (MPL) method, EM algorithm, RIC",https://link.springer.com//article/10.1007/s003579900011
Radial basis functions for exploratory data analysis: An iterative majorisation approach for Minkowski distances based on multidimensional scaling,This paper considers the use of radial basis functions for exploratory data analysis. These are used to model a transformation from a high-dimensional observation space to a low-dimensional one. The parameters of the model are determined by optimising a loss function defined to be the stress function in multidimensional scaling. The metric for the low-dimensional space is taken to be the Minkowski metric with order parameter 1<-p<-2. A scheme based on iterative majorisation is proposed.,"Nonlinear transformation, Radial basis functions, Minkowski distances, Iterative majorisation, Multidimensional scaling, K-means clustering, Feature extraction, Nonlinear principal components",https://link.springer.com//article/10.1007/s003579900012
Concordance between two linear orders: The Spearman and Kendall coefficients revisited,"This paper discusses the two classic measures of concordance between two linear orders L and L′, the Kendall tau and the Spearman rho, equivalently, the Kendall and Spearman distances between such orders. We give an expression for ρ(L,L′)−τ(L,L′) as a function of the parameters of the partial order L∪L′, which allows the determination of extremal values for this difference and an investigation of when tau and rho are equal. This expression for ρ(L,L′)−τ(L,L′) is derived from a relation between the Kendall and Spearman distances between linear orders that is equivalent to both the Guilbaud (1980) formula linking rho, tau, and a third coefficient sigma, and Daniels’s (1950) inequality. We also prove an apparently new monotonicity property of rho. In the conclusion we point out possible extensions and add general historical comments.","Daniels’s inequality, Kendall’s tau, Linear order, Metric, Permutation, Spearman’s rho",https://link.springer.com//article/10.1007/s003579900013
A modification of the SINDCLUS algorithm for fitting the ADCLUS and INDCLUS models,"The SINDCLUS algorithm for fitting the ADCLUS and INDCLUS models deals with a parameter matrix that occurs twice in the model by considering the two occurrences as independent parameter matrices. This procedure has been justified empirically by the observation that upon convergence of the algorithm to the global optimum, the two independently treated parameter matrices turn out to be equal. In the present paper, results are presented that contradict this finding, and a modification of SINDCLUS is presented which obviates the need for independently treating two occurrences of the same parameter matrix.","Alternating least squares, Cluster analysis, Three-way proximity data",https://link.springer.com//article/10.1007/s003579900014
Recognition of Robinsonian dissimilarities,"We present an O(n3)-time, O(n2)-space algorithm to test whether a dissimilarity d on an n-object set X is Robinsonian, i.e., X admits an ordering such that i≤j≤k implies that d(xi,xk)≥max {d(xi,xj),d(xj,xk)}.","Robinsonian dissimilarities, Order compatible with a dissimilarity, Divide-and-conquer algorithm",https://link.springer.com//article/10.1007/s003579900015
An entropy criterion for assessing the number of clusters in a mixture model,"In this paper, we consider an entropy criterion to estimate the number of clusters arising from a mixture model. This criterion is derived from a relation linking the likelihood and the classification likelihood of a mixture. Its performance is investigated through Monte Carlo experiments, and it shows favorable results compared to other classical criteria.","Cluster analysis, Gaussian mixture, Entropy, Bayesian criteria",https://link.springer.com//article/10.1007/BF01246098
Numerical taxonomy and the principle of maximum entropy,The standard procedure in numerical classification and identification of micro-organisms based on binary features is given a justification based on the principle of maximum entropy. This principle also strongly supports the assumption that all characteristics upon which the classification is based are equally important and the use of polythetic taxa. The relevance of the principle of maximum entropy in connection with taxonomic structures based on clustering and maximal predictivity is discussed. A result on asymptotic separateness of maximum entropy distributions has implications for minimizing identification errors.,"Bhattacharyya coefficient, Clustering and identification of binary vectors, Hamming distance, Hellinger distance, Hypothetical mean organism, Maximal predictive classification, Mixture of multivariate Bernoulli distributions, Polythetic and monothetic classes",https://link.springer.com//article/10.1007/BF01246099
The weighted sum of split and diameter clustering,"In this paper, we propose a bicriterion objective function for clustering a given set ofN entities, which minimizes [αd−(1−α)s], where 0≤α≤1, andd ands are the diameter and the split of the clustering, respectively. When α=1, the problem reduces to minimum diameter clustering, and when α=0, maximum split clustering. We show that this objective provides an effective way to compromise between the two often conflicting criteria. While the problem is NP-hard in general, a polynomial algorithm with the worst-case time complexityO(N2) is devised to solve the bipartition version. This algorithm actually gives all the Pareto optimal bipartitions with respect to diameter and split, and it can be extended to yield an efficient divisive hierarchical scheme. An extension of the approach to the objective [α(d1+d2)−2(1−α)s] is also proposed, whered1 andd2 are diameters of the two clusters of a bipartition.","Diameter, Split, Bicriteria clustering, Complexity, Polynomial algorithm, Divisive hierarchical clustering",https://link.springer.com//article/10.1007/BF01246100
Fitting a distance model to homogeneous subsets of variables: Points of view analysis of categorical data,"An approach is presented for analyzing a heterogeneous set of categorical variables assumed to form a limited number of homogeneous subsets. The variables generate a particular set of proximities between the objects in the data matrix, and the objective of the analysis is to represent the objects in lowdimensional Euclidean spaces, where the distances approximate these proximities. A least squares loss function is minimized that involves three major components: a) the partitioning of the heterogeneous variables into homogeneous subsets; b) the optimal quantification of the categories of the variables, and c) the representation of the objects through multiple multidimensional scaling tasks performed simultaneously. An important aspect from an algorithmic point of view is in the use of majorization. The use of the procedure is demonstrated by a typical example of possible application, i.e., the analysis of categorical data obtained in a free-sort task. The results of points of view analysis are contrasted with a standard homogeneity analysis, and the stability is studied through a Jackknife analysis.","Categorical data, Multidimensional scaling, Homogeneity analysis, Optimal scaling, Majorization, Clustering of variables",https://link.springer.com//article/10.1007/BF01246101
A spherical representation of a correlation matrix,"It is common practice to perform a principal component analysis (PCA) on a correlation matrix to represent graphically the relations among numerous variables. In such a situation, the variables may be considered as points on the unit hypersphere of an Euclidean space, and PCA provides a sort of best fit of these points within a subspace. Taking into account their particular position, this paper suggests to represent the variables on an optimal three-dimensional unit sphere.","Principal component analysis, Principal curves, Nonlinear multivariate analysis, Three-dimensional representation, Constrained multidimensional scaling, Two-mode two-way data",https://link.springer.com//article/10.1007/BF01246102
Averaging over decision trees,"Pruning a decision tree is considered by some researchers to be the most important part of tree building in noisy domains. While there are many approaches to pruning, the alternative of averaging over decision trees has not received as much attention. The basic idea of tree averaging is to produce a weighted sum of decisions. We consider the set of trees used for the averaging process, and how weights should be assigned to each tree in this set. We define the concept of afanned set for a tree, and examine how the Minimum Message Length paradigm of learning may be used to average over decision trees. We perform an empirical evaluation of two averaging approaches, and a Minimum Message Length approach.","Decision trees, Classification trees, Averaging, Minimum message length, Bayesian trees",https://link.springer.com//article/10.1007/BF01246103
Constrained clustering and Kohonen Self-Organizing Maps,"The Self-Organizing Feature Maps (SOFM; Kohonen 1984) algorithm is a well-known example of unsupervised learning in connectionism and is a clustering method closely related to the k-means. Generally the data set is available before running the algorithm and the clustering problem can be approached by an inertia criterion optimization. In this paper we consider the probabilistic approach to this problem. We propose a new algorithm based on the Expectation Maximization principle (EM; Dempster, Laird, and Rubin 1977). The new method can be viewed as a Kohonen type of EM and gives a better insight into the SOFM according to constrained clustering. We perform numerical experiments and compare our results with the standard Kohonen approach.","EM algorithm, Gaussian mixture, Kohonen maps, Constrained clustering",https://link.springer.com//article/10.1007/BF01246104
Measuring the influence of individual data points in a cluster analysis,"The problem of measuring the impact of individual data points in a cluster analysis is examined. The purpose is to identify those data points that have an influence on the resulting cluster partitions. Influence of a single data point is considered present when different cluster partitions result from the removal of the element from the data set. The Hubert and Arabie (1985) corrected Rand index was used to provide numerical measures of influence of a data point. Simulated data sets consisting of a variety of cluster structures and error conditions were generated to validate the influence measures. The results showed that the measure of internal influence was 100% accurate in identifying those data elements exhibiting an influential effect. The nature of the influence, whether beneficial or detrimental to the clustering, can be evaluated with the use of the gamma and point-biserial statistics.","Hierarchical clustering, Jackknife, Corrected Rand index, Gamma statistic, Clustering stability",https://link.springer.com//article/10.1007/BF01246105
Metric unidimensional scaling and global optimization,"For the problem of metric unidimensional scaling, the number of local minima is estimated. For locating the globally optimal solution we develop an approach, called the “smoothing technique.” Although not guaranteed inevitably to locate the global optimum, the smoothing technique did so in all computational experiments where the global optimum was known.","Unidimensional scaling, Seriation, Local minima, Global optimization, Smoothing technique, Multidimensional scaling",https://link.springer.com//article/10.1007/BF01202579
Unidimensional scaling: A linear programming approach minimizing absolute deviations,A Mixed Integer Programming formulation can be developed for the classical unidimensional scaling problem when the measure of goodness-of-fit is thel1 norm of the discrepancies rather than the sum of the squares of the discrepancies.,"Unidimensional Scaling, Seriation, Linear Programming, Mixed Integer Programming",https://link.springer.com//article/10.1007/BF01202580
Quantitative properties of the evolution and classification of languages,"Statistical analyses of a published phylogenetic classification of languages show some properties attributable to taxonomic methods and others that reflect the nature of linguistic evolution. The inferred phylogenetic tree is less well resolved and more asymmetric at the highest taxonomic ranks, where the tree is constructed mainly by phenetic methods. At lower ranks, where cladistic methods are more prevalent, the asymmetry of well resolved parts of the tree is consistent with a stochastic birth and death process in which languages originate and become extinct at constant rates, although poorly resolved parts of the tree are still more asymmetric than predicted. Other tests applied to a sample of historically recorded languages reveal substantial fluctuations in the rates of origination and extinction, with both rates temporarily reduced when languages enter the historical record. For languages in general, the average origination rate is estimated to be only slightly higher than the average extinction rate, which in turn corresponds to an average lifetime of about 500 years or less.","Phylogenetic tree, Cladistics, Language evolution, Birth and death process, Evolutionary rates",https://link.springer.com//article/10.1007/BF01202581
The analysis of free-sorting data: Beyond pairwise cooccurrences,"Free-sorting data are obtained when subjects are given a set of objects and are asked to divide them into subsets. Such data are usually reduced by counting for each pair of objects, how many subjects placed both of them into the same subset. The present study examines the utility of a group of additional statistics. the cooccurrences of sets of three objects. Because there are dependencies among the pair and triple cooccurrences, adjusted triple similarity statistics are developed. Multidimensional scaling and cluster analysis — which usually use pair similarities as their input data — can be modified to operate on three-way similarities to create representations of the set of objects. Such methods are applied to a set of empirical sorting data: Rosenberg and Kim's (1975) fifteen kinship terms.","Multidimensional scaling, Cluster analysis, Three-way proximity",https://link.springer.com//article/10.1007/BF01202582
Unfolding a symmetric matrix,"Graphical displays which show inter-sample distances are important for the interpretation and presentation of multivariate data. Except when the displays are two-dimensional, however, they are often difficult to visualize as a whole. A device, based on multidimensional unfolding, is described for presenting some intrinsically high-dimensional displays in fewer, usually two, dimensions. This goal is achieved by representing each sample by a pair of points, sayRi andri, so that a theoretical distance between thei-th andj-th samples is represented twice, once by the distance betweenRi andrj and once by the distance betweenRj andri. Selfdistances betweenRi andri need not be zero. The mathematical conditions for unfolding to exhibit symmetry are established. Algorithms for finding approximate fits, not constrained to be symmetric, are discussed and some examples are given.","Dimensionality reduction, Distances, Graphics, Multidimensional scaling, Symmetric matrices, Unfolding",https://link.springer.com//article/10.1007/BF01202583
Espaliers: A generalization of dendrograms,"Dendrograms are widely used to represent graphically the clusters and partitions obtained with hierarchical clustering schemes. Espaliers are generalized dendrograms in which the length of horizontal lines is used in addition to their level in order to display the values of two characteristics of each cluster (e.g., the split and the diameter) instead of only one. An algorithm is first presented to transform a dendrogram into an espalier without rotation of any part of the former. This is done by stretching some of the horizontal lines to obtain a diagram with vertical and horizontal lines only, the cutting off by diagonal lines the parts of the horizontal lines exceeding their prescribed length. The problem of finding if, allowing rotations, no diagonal lines are needed is solved by anO(N2) algorithm whereN is the number of entities to be classified. This algorithm is the generalized to obtain espaliers with minimum width and, possibly, some diagonal lines.","Dendrogram, Espalier, Hierarchical clustering, Diagram, Polynomial algorithm",https://link.springer.com//article/10.1007/BF01202584
A reduction algorithm for approximating a (nonmetric) dissimilarity by a tree distance,"We propose a development stemming from Roux (1988). The principle is progressively to modify the dissimilarities so that every quadruple satisfies not only the additive inequality, as in Roux's method, but also all triangle inequalities. Our method thus ensures that the results are tree distances even when the observed dissimilarities are nonmetric. The method relies on the analytic solution of the least-squares projection onto a tree distance of the dissimilarities attached to a single quadruple. This goal is achieved by using geometric reasoning which also enables an easy proof of algorithm's convergence. This proof is simpler and more complete than that of Roux (1988) and applies to other similar reduction methods based on local least-squares projection. The method is illustrated using Case's (1978) data. Finally, we provide a comparative study with simulated data and show that our method compares favorably with that of Studier and Keppler (1988) which follows in the ADDTREE tradition (Sattath and Tversky 1977). Moreover, this study seems to indicate that our method's results are generally close to the global optimum according to variance accounted for.","Tree distance, Heuristic algorithm, Least-squares projection, Convex polyhedral cones, Computer simulations",https://link.springer.com//article/10.1007/BF01202585
Space-conserving agglomerative algorithms,"This paper evaluates a general, infinite family of clustering algorithms, called the Lance and Williams algorithms, with respect to the space-conserving criterion. An admissible clustering criterion is defined using the space conserving idea. Necessary and sufficient conditions for Lance and Williams clustering algorithms to satisfy space-conserving admissibility are provided. Space-dilating, space-contracting, and well-structured clustering algorithms are also discussed.","Space-conserving clustering, Agglomerative clustering, Lance and Williams algorithm, Admissible, Well-structured clustering",https://link.springer.com//article/10.1007/BF01202586
The Kohonen self-organizing map method: An assessment,"The “self-organizing map” method, due to Kohonen, is a well-known neural network method. It is closely related to cluster analysis (partitioning) and other methods of data analysis. In this article, we explore some of these close relationships. A number of properties of the technique are discussed. Comparisons with various methods of data analysis (principal components analysis, k-means clustering, and others) are presented.","Partitioning, Optimization, Dimensionality reduction, Data display, Exploratory data analysis",https://link.springer.com//article/10.1007/BF03040854
Three-way distances,"In this paper, dissimilarity relations are defined on triples rather than on dyads. We give a definition of a three-way distance analogous to that of the ordinary two-way distance. It is shown, as a straightforward generalization, that it is possible to define three-way ultrametric, three-way star, and three-way Euclidean distances. Special attention is paid to a model called the semi-perimeter model.We construct new methods analogous to the existing ones for ordinary distances, for example: principal coordinates analysis, the generalized Prim (1957) algorithm, hierarchical cluster analysis.","Dissimilarity indices, Three-way data, Metric spaces, Clustering",https://link.springer.com//article/10.1007/BF03040855
Minimum spanning trees for tree metrics: abridgements and adjustments,"Two properties of tree metrics are already known in the literature: tree metrics on a setX withn elements have 2n−3 degrees of freedom; a tree metric has Robinson form with regard to its minimum spanning tree (MST), or to any such MST if several of them exist. Starting from these results, we prove that a tree metrict is entirely defined by its restriction to some setB of 2n−3 entries. This set is easily determined from the table oft and includes then−1 entries of an MST. A fast method for the adjustment of a tree metric to any given metricd is then obtained. This method extends to dissimilarities.","Tree metric, Ultrametric, Robinson dissimilarity, Dissimilarity, Graph, Tree, Minimum spanning tree, Fitting algorithm, Combinatorial data analysis",https://link.springer.com//article/10.1007/BF03040856
Additive two-mode clustering: The error-variance approach revisited,"The additive clustering approach is applied to the problem of two-mode clustering and compared with the recent error-variance approach of Eckes and Orlik (1993). Although the schemes of the computational algorithms look very similar in both of the approaches, the additive clustering has been shown to have several advantages. Specifically, two technical limitations of the error-variance approach (see Eckes and Orlik 1993, p. 71) have been overcome in the framework of the additive clustering.","Two-mode clustering, Additive clustering, Correspondence analysis, Addition/delection algorithm",https://link.springer.com//article/10.1007/BF03040857
Comparison tests for dendrograms: A comparative evaluation,"Classifications are generally pictured in the form of hierarchical trees, also called dendrograms. A dendrogram is the graphical representation of an ultrametric (=cophenetic) matrix; so dendrograms can be compared to one another by comparing their cophenetic matrices. Three methods used in testing the correlation between matrices corresponding to dendrograms are evaluated. The three permutational procedures make use of different aspects of the information to compare dendrograms: the Mantel procedure permutes label positions only; the binary tree methods randomize the topology as well; the double-permutation procedure is based on all the information included in a dendrogram, that is: topology, label positions, and cluster heights. Theoretical and empirical investigations of these methods are carried out to evaluate their relative performance. Simulations show that the Mantel test is too conservative when applied to the comparison of dendrograms; the methods of binary tree comparisons do slightly better; only the doublepermutation test provides unbiased type I error.","Binary tree, Dendrogram, Classification, Permutation test, Ultrametric tree",https://link.springer.com//article/10.1007/BF03040858
"A graph-theoretic method for organizing overlapping clusters into trees, multiple trees, or extended trees","A clustering that consists of a nested set of clusters may be represented graphically by a tree. In contrast, a clustering that includes non-nested overlapping clusters (sometimes termed a “nonhierarchical” clustering) cannot be represented by a tree. Graphical representations of such non-nested overlapping clusterings are usually complex and difficult to interpret. Carroll and Pruzansky (1975, 1980) suggested representing non-nested clusterings with multiple ultrametric or additive trees. Corter and Tversky (1986) introduced the extended tree (EXTREE) model, which represents a non-nested structure as a tree plus overlapping clusters that are represented by marked segments in the tree. We show here that the problem of finding a nested (i.e., tree-structured) set of clusters in an overlapping clustering can be reformulated as the problem of finding a clique in a graph. Thus, clique-finding algorithms can be used to identify sets of clusters in the solution that can be represented by trees. This formulation provides a means of automatically constructing a multiple tree or extended tree representation of any non-nested clustering. The method, called “clustrees”, is applied to several non-nested overlapping clusterings derived using the MAPCLUS program (Arabie and Carroll 1980).","Overlapping clustering, Ultrametric, Additive tree, Extended tree, multiple trees, Graph, Clique, Proximities",https://link.springer.com//article/10.1007/BF03040859
The majorization approach to multidimensional scaling for Minkowski distances,The majorization method for multidimensional scaling with Kruskal's STRESS has been limited to Euclidean distances only. Here we extend the majorization algorithm to deal with Minkowski distances with 1≤p≤2 and suggest an algorithm that is partially based on majorization forp outside this range. We give some convergence proofs and extend the zero distance theorem of De Leeuw (1984) to Minkowski distances withp>1.,"Multidimensional scaling, Distance analysis, Majorization, Minkowski distances",https://link.springer.com//article/10.1007/BF01202265
A mixture likelihood approach for generalized linear models,"A mixture model approach is developed that simultaneously estimates the posterior membership probabilities of observations to a number of unobservable groups or latent classes, and the parameters of a generalized linear model which relates the observations, distributed according to some member of the exponential family, to a set of specified covariates within each Class. We demonstrate how this approach handles many of the existing latent class regression procedures as special cases, as well as a host of other parametric specifications in the exponential family heretofore not mentioned in the latent class literature. As such we generalize the McCullagh and Nelder approach to a latent class framework. The parameters are estimated using maximum likelihood, and an EM algorithm for estimation is provided. A Monte Carlo study of the performance of the algorithm for several distributions is provided, and the model is illustrated in two empirical applications.","Mixture models, Generalized linear models, EM algorithm, Maximum likelihood estimation",https://link.springer.com//article/10.1007/BF01202266
Fitting an extended INDSCAL model to three-way proximity data,"The INDSCAL individual differences scaling model is extended by assuming dimensions specific to each stimulus or other object, as well as dimensions common to all stimuli or objects. An “alternating maximum likelihood” procedure is used to seek maximum likelihood estimates of all parameters of this EXSCAL (Extended INDSCAL) model, including parameters of monotone splines assumed in a “quasi-nonmetric” approach. The rationale for and numerical details of this approach are described and discussed, and the resulting EXSCAL method is illustrated on some data on perception of musical timbres.","Weighted Euclidean model, INDSCAL, Multidimensional scaling, Specificities, Monotone splines",https://link.springer.com//article/10.1007/BF01202267
Comparing resemblance measures,"In the paper some types of equivalences over resemblance measures and some basic results about them are given. Based on induced partial orderings on the set of unordered pairs of units a dissimilarity between two resemblance measures over finite sets of units can be defined. As an example, using this dissimilarity standard association coefficients between binary vectors are compared both theoretically and computationally.","Dissimilarity spaces, Metric spaces, Association coefficients, Profile measures of resemblance",https://link.springer.com//article/10.1007/BF01202268
An algorithm to find agreement subtrees,"Given two binary trees, a largest subtree contained in both of the original trees that has been obtained by pruning vertices is called an agreement subtree. An exact algorithm for finding an agreement subtree is presented.","Pruning vertices, Agreement subtree",https://link.springer.com//article/10.1007/BF01202269
An efficient algorithm for supertrees,"Givenk rooted binary treesA1, A2, ..., Ak, with labeled leaves, we generateC, a unique system of lineage constraints on common ancestors. We then present an algorithm for constructing the set of rooted binary treesB, compatible with all ofA1, A2, ..., Ak. The running time to obtain one such supertree isO(k2 n2), wheren is the number of distinct leaves in all of the treesA1, A2, ..., Ak.","Tree compatibility, Constraints on trees, Supertrees, Consensus trees",https://link.springer.com//article/10.1007/BF01202270
Weighting and selection of variables for cluster analysis,One of the thorniest aspects of cluster analysis continues to be the weighting and selection of variables. This paper reports on the performance of nine methods on eight “leading case” simulated and real sets of data. The results demonstrate shortcomings of weighting based on the standard deviation or range as well as other more complex schemes in the literature. Weighting schemes based upon carefully chosen estimates of within-cluster and between-cluster variability are generally more effective. These estimates do not require knowledge of the cluster structure. Additional research is essential: worry-free approaches do not yet exist.,"Clustering, Variable selection, Feature selection, Variable weighting, Variable importance, Pattern recognition, Discriminant analysis",https://link.springer.com//article/10.1007/BF01202271
An alternating combinatorial optimization approach to fitting the INDCLUS and generalized INDCLUS models,"This paper presents a general approach for fitting the ADCLUS (Shepard and Arabie 1979; Arabie, Carroll, DeSarbo, and Wind 1981), INDCLUS (Carroll and Arabie 1983), and potentially a special case of the GENNCLUS (DeSarbo 1982) models. The proposed approach, based largely on a separability property observed for the least squares loss function being optimized, offers increased efficiency and other advantages over existing approaches like MAPCLUS (Arabie and Carroll 1980) for fitting the ADCLUS model, and the INDCLUS method for fitting the INDCLUS model. The new procedure (called “SINDCLUS”) is applied to three sets of empirical data to demonstrate the effectiveness of the SINDCLUS methodology. Finally, some potentially useful extensions are discussed.","Overlapping clustering, Cluster analysis, ADCLUS, INDCLUS, MAPCLUS, Separability",https://link.springer.com//article/10.1007/BF01195676
Pyramidal classification based on incomplete dissimilarity data,Two algorithms for pyramidal classification — a generalization of hierarchical classification — are presented that can work with incomplete dissimilarity data. These approaches — a modification of the pyramidal ascending classification algorithm and a least squares based penalty method — are described and compared using two different types of complete dissimilarity data in which randomly chosen dissimilarities are assumed missing and the non-missing ones are subjected to random error. We also consider relationships between hierarchical classification and pyramidal classification solutions when both are based on incomplete dissimilarity data.,"Cluster analysis, Missing values, Monte Carlo evaluation, Penalty approach, Pyramidal classification",https://link.springer.com//article/10.1007/BF01195677
"Ordination in the presence of group structure, for general multivariate data","A low-dimensional representation of multivariate data is often sought when the individuals belong to a set ofa-priori groups and the objective is to highlight between-group variation relative to that within groups. If all the data are continuous then this objective can be achieved by means of canonical variate analysis, but no corresponding technique exists when the data are categorical or mixed continuous and categorical. On the other hand, if there is noa-priori grouping of the individuals, then ordination of any form of data can be achieved by use of metric scaling (principal coordinate analysis). In this paper we consider a simple extension of the latter approach to incorporate grouped data, and discuss to what extent this method can be viewed as a generalization of canonical variate analysis. Some illustrative examples are also provided.","Canonical variate analysis, Categorical and mixed data, Distances, Diversity coefficients, Metric scaling",https://link.springer.com//article/10.1007/BF01195679
A tree · a window · a hill; generalization of nearest-neighbor interchange in phylogenetic optimization,"The method of nearest-neighbor interchange effects local improvements in a binary tree by replacing a 4-subtree by one of its two alternatives if this improves the objective function. We extend this to k-subtrees to reduce the number of local optima. Possible sequences of k-subtrees to be examined are produced by moving a window over the tree, incorporating one edge at a time while deactivating another. The direction of this movement is chosen according to a hill-climbing strategy. The algorithm includes a backtracking component. Series of simulations of molecular evolution data/parsimony analysis are carried out, fork=4, ..., 8, contrasting the hill-climbing strategy to one based on a random choice of next window, and comparing two stopping rules. Increasing window sizek is found to be the most effective way of improving the local optimum, followed by the choice of hill-climbing over the random strategy. A suggestion for achieving higher values ofk is based on a recursive use of the hill-climbing strategy.","Local optimization, Heuristics, Molecular evolution",https://link.springer.com//article/10.1007/BF01195680
On the consistency of the plurality rule consensus function for molecular sequences,"The plurality rule for molecular sequences is a consensus function ℘ which maps each profile of lengthk (i.e., each sequence ofk bases appearing at an aligned position ofk molecules) to a set of consensus results (i.e., ambiguity codes) that are descriptive summaries of the profile. Since the plurality rule ℘ is a median procedure, its results are solutions of an optimization problem that is well known and intensively studied in the theory of social choice. However, ℘'s behavior for long profiles according to its behavior for short profiles. Because consistency is a desirable feature of consensus functions, we explore the boundaries of its applicability to ℘. We distinguish between two types of consistency, weak and strong, and we apply them to profiles and consensus results as well as to ℘ itself. For ℘ we obtain simple characterizations of weakly consistent profiles, of weakly consistent results, and of strongly consistent results.","Ambiguity codes, Comparing molecular sequences, Consistency, Consensus function, DNA, Plurality rule, RNA",https://link.springer.com//article/10.1007/BF01195681
The MAP test for multimodality,"We introduce a test for detecting multimodality in distributions based on minimal constrained spanning trees. We define a Minimal Ascending Path Spanning Tree (MAPST) on a set of points as a spanning tree that has the minimal possible sum of lengths of links with the constraint that starting from any link, the lengths of the links are non-increasing towards a root node. We define similarly MAPSTs with more than one root. We present some algorithms for finding such trees. Based on these trees, we devise a test for multimodality, called the MAP Test (for Minimal Ascending Path). Using simulations, we estimate percentage points of the MAP statistic and assess the power of the test. Finally, we illustrate the use of MAPSTs for determining the number of modes in a distribution of positions of galaxies on photographic plates from a rich galaxy cluster.","Minimal constrained spanning trees, Nearest neighbor density estimates, Minimal ascending path spanning trees, Tests for modes, The MAP test",https://link.springer.com//article/10.1007/BF01201021
Partial dissimilarities with application to clustering,"We consider dissimilarities which are defined only on some pairs of items. Such situations may occur in some problems like unfolding or merging, or can be encountered as an intermediate step of a more general transformation. We give necessary and sufficient conditions for the existence of extensions with good properties and characterize the family of such extensions. Using partial dissimilarities we construct a dissimilarity-into-distance transformation family.","Dissimilarity, Partial dissimularity, Distance, Distance transformation, Clustering, Missing data",https://link.springer.com//article/10.1007/BF01201022
Preserving consensus hierarchies,"In numerical taxonomy we often have the task of finding a consensus hierarchy for a given set of hierarchies. This consensus hierarchy should reflect the substructures which are common to all hierarchies of the set. Because there are several kinds of substructures in a hierarchy, the general axiom to preserve common substructures leads to different axioms for each kind of substructure. In this paper we consider the three substructurescluster, separation, andnesting, and we give several characterizations of hierarchies preserving these substructures. These characterizations facilitate interpretation of axioms for preserving substructures and the examination of properties of consensus methods. Finally some extensions concerning the preserving of qualified substructures are discussed.","Numerical taxonomy, Consensus, Triad, Nesting, Betweenness axiom, Adams consensus, Generalized intersection rules, Majority rule",https://link.springer.com//article/10.1007/BF01201023
A generalization of GIPSCAL for the analysis of nonsymmetric data,"Graphical representation of nonsymmetric relationships data has usually proceeded via separate displays for the symmetric and the skew-symmetric parts of a data matrix. DEDICOM avoids splitting the data into symmetric and skewsymmetric parts, but lacks a graphical representation of the results. Chino's GIPSCAL combines features of both models, but may have a poor goodness-of-fit compared to DEDICOM. We simplify and generalize Chino's method in such a way that it fits the data better. We develop an alternating least squares algorithm for the resulting method, called Generalized GIPSCAL, and adjust it to handle GIPSCAL as well. In addition, we show that Generalized GIPSCAL is a constrained variant of DEDICOM and derive necessary and sufficient conditions for equivalence of the two models. Because these conditions are rather mild, we expect that in many practical cases DEDICOM and Generalized GIPSCAL are (nearly) equivalent, and hence that the graphical representation from Generalized GIPSCAL can be used to display the DEDICOM results graphically. Such a representation is given for an illustration. Finally, we show Generalized GIPSCAL to be a generalization of another method for joint representation of the symmetric and skew-symmetric parts of a data matrix.","DEDICOM, alternating least squares, multidimensional scaling",https://link.springer.com//article/10.1007/BF01201024
Error rates in quadratic discrimination with constraints on the covariance matrices,"In multivariate discrimination of several normal populations, the optimal classification procedure is based on quadratic discriminant functions. We compare expected error rates of the quadratic classification procedure if the covariance matrices are estimated under the following four models: (i) arbitrary covariance matrices, (ii) common principal components, (iii) proportional covariance matrices, and (iv) identical covariance matrices. Using Monte Carlo simulation to estimate expected error rates, we study the performance of the four discrimination procedures for five different parameter setups corresponding to “standard” situations that have been used in the literature. The procedures are examined for sample sizes ranging from 10 to 60, and for two to four groups. Our results quantify the extent to which a parsimonious method reduces error rates, and demonstrate that choosing a simple method of discrimination is often beneficial even if the underlying model assumptions are wrong.","Common principal components, Linear Discriminant Function, Monte Carlo Simulation, Proportional Covariance Matrices",https://link.springer.com//article/10.1007/BF01201025
Metric inference for social networks,"Using a natural metric on the space of networks, we define a probability measure for network-valued random variables. This measure is indexed by two parameters, which are interpretable as a location parameter and a dispersion parameter. From this structure, one can develop maximum likelihood estimates, hypothesis tests and confidence regions, all in the context of independent and identically distributed networks. The value of this perspective is illustrated through application to portions of the friedship cognitive social structure data gathered by Krackhardt (1987).","Random networks, Random graphs, Digraphs",https://link.springer.com//article/10.1007/BF01201026
A latent class procedure for the structural analysis of two-way compositional data,"This paper develops a new procedure for simultaneously performing multidimensional scaling and cluster analysis on two-way compositional data of proportions. The objective of the proposed procedure is to delineate patterns of variability in compositions across subjects by simultaneously clustering subjects into latent classes or groups and estimating a joint space of stimulus coordinates and class-specific vectors in a multidimensional space. We use a conditional mixture, maximum likelihood framework with an E-M algorithm for parameter estimation. The proposed procedure is illustrated using a compositional data set reflecting proportions of viewing time across television networks for an area sample of households.","Compositional data, Finite mixture distributions, Cluster analysis, Multidimensional scaling, E-M algorithm, Latent class analysis",https://link.springer.com//article/10.1007/BF02626090
A latent class vector model for preference ratings,"A latent class formulation of the well-known vector model for preference data is presented. Assuming preference ratings as input data, the model simultaneously clusters the subjects into a small number of homogeneous groups (or latent classes) and constructs a joint geometric representation of the choice objects and the latent classes according to a vector model. The distributional assumptions on which the latent class approach is based are analogous to the distributional assumptions that are consistent with the common practice of fitting the vector model to preference data by least squares methods. An EM algorithm for fitting the latent class vector model is described as well as a procedure for selecting the appropriate number of classes and the appropriate number of dimensions. Some illustrative applications of the latent class vector model are presented and some possible extensions are discussed.","Latent class analysis, Vector model, Preference analysis, EM Algorithm, Monte Carlo significance test",https://link.springer.com//article/10.1007/BF02626091
Fixed points approach to clustering,"Assume that a dissimilarity measure between elements and subsets of the set being clustered is given. We define the transformation of the set of subsets under which each subset is transformed into the set of all elements whose dissimilarity to it is not greater than a given threshold. Then a cluster is defined as a fixed point of this transformation. Three well-known clustering strategies are considered from this point of view: hierarchical clustering, graph-theoretic methods, and conceptual clustering. For hierarchical clustering generalizations are obtained that allow for overlapping clusters and/or clusters not forming a cover. Three properties of dissimilarity are introduced which guarantee the existence of fixed points for each threshold. We develop the relation to the theory of quasi-concave set functions, to help give an additional interpretation of clusters.","Fixed points, Hierarchical clustering, Graph-theoretic clustering, Conceptual clustering, Overlapping clusters",https://link.springer.com//article/10.1007/BF02626092
Constructing optimal ultrametrics,"Clique optimization (CLOPT) is a family of graph clustering procedures that construct parsimonious ultrametrics by executing a sequence of divisive and agglomerative operations. Every CLOPT procedure is associated with a distinct graph-partitioning heuristic. Seven HCS methods, a mathematical programming algorithm, and two CLOPT heuristics were evaluated on simulated data. These data were obtained by distorting ultrametric partitions and hierarchies. In general, internally optimal models yielded externally optimal models. By recovering near-optimal solutions more consistently, CLOPT2 emerged as the most robust technique.","Clustering, Graph, Heuristic, Hierarchical, Simulation",https://link.springer.com//article/10.1007/BF02626093
Spectral analysis of phylogenetic data,"The spectral analysis of sequence and distance data is a new approach to phylogenetic analysis. For two-state character sequences, the character values at a given site split the set of taxa into two subsets, a bipartition of the taxa set. The vector which counts the relative numbers of each of these bipartitions over all sites is called a sequence spectrum. Applying a transformation called a Hadamard conjugation, the sequence spectrum is transformed to the conjugate spectrum. This conjugation corrects for unobserved changes in the data, independently from the choice of phylogenetic tree. For any given phylogenetic tree with edge weights (probabilities of state change), we define a corresponding tree spectrum. The selection of a weighted phylogenetic tree from the given sequence data is made by matching the conjugate spectrum with a tree spectrum. We develop an optimality selection procedure using a least squares best fit, to find the phylogenetic tree whose tree spectrum most closely matches the conjugate spectrum. An inferred sequence spectrum can be derived from the selected tree spectrum using the inverse Hadamard conjugation to allow a comparison with the original sequence spectrum.A possible adaptation for the analysis of four-state character sequences with unequal frequencies is considered. A corresponding spectral analysis for distance data is also introduced. These analyses are illustrated with biological examples for both distance and sequence data. Spectral analysis using the Fast Hadamard transform allows optimal trees to be found for at least 20 taxa and perhaps for up to 30 taxa.The development presented here is self contained, although some mathematical proofs available elsewhere have been omitted. The analysis of sequence data is based on methods reported earlier, but the terminology and the application to distance data are new.","Phylogenetic trees, Bipartition, Hadamard transform, Hadamard conjugation, Spectrum, Nucleotide sequences, Distance data, Fast Hadamard Transform",https://link.springer.com//article/10.1007/BF02638451
The location model for mixtures of categorical and continuous variables,"Recent research into graphical association models has focussed interest on the conditional Gaussian distribution for analyzing mixtures of categorical and continuous variables. A special case of such models, utilizing the homogeneous conditional Gaussian distribution, has in fact been known since 1961 as the location model, and for the past 30 years has provided a basis for the multivariate analysis of mixed categorical and continuous variables. Extensive development of this model took place throughout the 1970’s and 1980’s in the context of discrimination and classification, and comprehensive methodology is now available for such analysis of mixed variables. This paper surveys these developments and summarizes current capabilities in the area. Topics include distances between groups, discriminant analysis, error rates and their estimation, model and feature selection, and the handling of missing data.","Classification, Discrimination, Distances, Error rates, Feature selection",https://link.springer.com//article/10.1007/BF02638452
An error variance approach to two-mode hierarchical clustering,"A new agglomerative method is proposed for the simultaneous hierarchical clustering of row and column elements of a two-mode data matrix. The procedure yields a nested sequence of partitions of the union of two sets of entities (modes). A two-mode cluster is defined as the union of subsets of the respective modes. At each step of the agglomerative process, the algorithm merges those clusters whose fusion results in the smallest possible increase in an internal heterogeneity measure. This measure takes into account both the variance within the respective cluster and its centroid effect defined as the squared deviation of its mean from the maximum entry in the input matrix. The procedure optionally yields an overlapping cluster solution by assigning further row and/or column elements to clusters existing at a preselected hierarchical level. Applications to real data sets drawn from consumer research concerning brand-switching behavior and from personality research concerning the interaction of behaviors and situations demonstrate the efficacy of the method at revealing the underlying two-mode similarity structure.","Clustering, Two-mode data, Ultrametric representation, Agglomerative algorithm, Heterogeneity index, Brand-switching, Behavior-situation congruence",https://link.springer.com//article/10.1007/BF02638453
A modified CANDECOMP method for fitting the extended INDSCAL model,"A modified CANDECOMP algorithm is presented for fitting the metric version of the Extended INDSCAL model to three-way proximity data. The Extended INDSCAL model assumes, in addition to the common dimensions, a unique dimension for each object. The modified CANDECOMP algorithm fits the Extended INDSCAL model in a dimension-wise fashion and ensures that the subject weights for the common and the unique dimensions are nonnegative. A Monte Carlo study is reported to illustrate that the method is fairly insensitive to the choice of the initial parameter estimates. A second Monte Carlo study shows that the method is able to recover an underlying Extended INDSCAL structure if present in the data. Finally, the method is applied for illustrative purposes to some empirical data on pain relievers. In the final section, some other possible uses of the new method are discussed.","Extended INDSCAL model, Extended Euclidean distance model, CANDECOMP, Alternating least squares method, Constrained INDSCAL model",https://link.springer.com//article/10.1007/BF02638454
Generalized measures of association for ranked data with an application to prediction accuracy,A common practice in cross validation research in the behavioral sciences is to employ either the product moment correlation or a simple tabulation of first-choice “hits” for measuring the accuracy with which various preference models predict subjects’ responses to a holdout sample of choice objects.We propose a nonparametric approach for summarizing the accuracy of predicted rankings across a set of holdout-sample options. The methods that we develop contain a novel way to deal with ties and an approach to the different weighting of rank positions.,"Cross validation, Nonparametric agreement, Foot-rule, Hit-rule",https://link.springer.com//article/10.1007/BF02638455
Computational solutions for the problem of negative saliences and nonsymmetry in INDSCAL,"Carroll and Chang have derived the symmetric CANDECOMP model from the INDSCAL model, to fit symmetric matrices of approximate scalar products in the least squares sense. Typically, the CANDECOMP algorithm is used to estimate the parameters. In the present paper it is shown that negative weights may occur with CANDECOMP. This phenomenon can be suppressed by updating the weights by the Nonnegative Least Squares Algorithm. A potential drawback of the resulting procedure is that it may produce two different versions of the stimulus space matrix. To obviate this possibility, a symmetry preserving algorithm is offered, which can be monitored to produce non-negative weights as well.","Symmetric CANDECOMP, Nonnegative least squares, Multidimensional scaling",https://link.springer.com//article/10.1007/BF02638456
Statistical properties of large published classifications,"Large published classifications typically consist of sets (called taxa) hierarchically arranged according to taxonomic rank. A statistical survey of 23 such classification reveals the following distinctive properties. The pattern of mandatory and optional taxonomic ranks is similar to a Guttman scale. Mean taxon size (defined as the number of next-lower-rank taxa per higher-rank taxon) is a U-shaped function of mandatory rank, and averages about seven across ranks with no significant differences between classifications. The variability of taxon size is a decreasing function of mandatory rank. The generality of these properties across classifications suggests that they are determined by the psychology of the classification process. In contrast, there are significant differences between classifications in the variability of taxon size and in the prevalence of optional ranks, both of which are greater in biological than in nonbiological classifications. These differences may reflect the nature of the materials classified.","Hierarchical classification, Taxonomic rank, Guttman scale, Cluster analysis, Folk taxonomy, Natural categories, Hollow curve",https://link.springer.com//article/10.1007/BF02621406
Multidimensional scaling in the city-block metric: A combinatorial approach,"We present an approach, independent of the common gradient-based necessary conditions for obtaining a (locally) optimal solution, to multidimensional scaling using the city-block distance function, and implementable in either a metric or nonmetric context. The difficulties encountered in relying on a gradient-based strategy are first reviewed: the general weakness in indicating a good solution that is implied by the satisfaction of the necessary condition of a zero gradient, and the possibility of actual nonconvergence of the associated optimization strategy. To avoid the dependence on gradients for guiding the optimization technique, an alternative iterative procedure is proposed that incorporates (a) combinatorial optimization to construct good object orders along the chosen number of dimensions and (b) nonnegative least-squares to re-estimate the coordinates for the objects based on the object orders. The re-estimated coordinates are used to improve upon the given object orders, which may in turn lead to better coordinates, and so on until convergence of the entire process occurs to a (locally) optimal solution. The approach is illustrated through several data sets on the perception of similarity of rectangles and compared to the results obtained with a gradient-based method.","City-block metric, Multidimensional scaling, Combinatorial optimization",https://link.springer.com//article/10.1007/BF02621407
Resistant orthogonal procrustes analysis,"In this paper two alternative loss criteria for the least squares Procrustes problem are studied. These alternative criteria are based on the Huber function and on the more radical biweight function, which are designed to be resistant to outliers. Using iterative majorization it is shown how a convergent reweighted least squares algorithm can be developed. In asimulation study it turns out that the proposed methods perform well over a specific range of contamination. When a uniform dilation factor is included, mixed results are obtained. The methods also yield a set of weights that can be used for diagnostic purposes.","Resistance, Procrustes analysis, Outliers, Iterative majorization, Reweighted least squares",https://link.springer.com//article/10.1007/BF02621408
A uniform approach to multidimensional scaling,"We present a method and an algorithm that puts interval and ordinal multidimensional scaling at two ends of a continuum. Theory and simulation show that the method compares favorably with classical scaling methods. A parameter is identified that produces scaling that combines benefits of interval, and ordinal scaling.","Multidimensional scaling, Ordinal Scaling, Interval Scaling, Metric Scaling, Non-Metric Scaling",https://link.springer.com//article/10.1007/BF02621409
An investigation of three-matrix permutation tests,"Several methods have recently been introduced for investigating relations between three interpoint proximity matricesA, B, C, each of which furnishes a different type of distance between the same objects. Smouse, Long, and Sokal (1986) investigate the partial correlation betweenA andB conditional onC. Dow and Cheverud (1985) ask whethercorr (A, C), equalscorr (B, C). Manly (1986) investigates regression-like models for predicting one matrix as a function of others.We have investigated rejection rates of these methods when their null hypotheses are true, but data are spatially autocorrelated (SA). That is,A, andB are distance matrices from independent realizations of the same SA generating process, andC is a matrix of geographic connections.SA causes all the models to be liberal because the hypothesis of equally likely row/column permutations invoked, by all these methods, is untrue when data are SA. Consequently, we cannot unreservedly recommend the use of any of these methods with SA data. However, if SA is weak, the Smouse-Long-Sokal method, used with a conservative critical value, is unlikely to reject falsely.","Spatial autocorrelation, Quadratic assignment, Partial correlations, Permutation tests, Proximity matrices",https://link.springer.com//article/10.1007/BF02621410
An algorithm to maximize the agreement between partitions,"Given the marginal description of two partitionsC andY of the same set, an index of agreement between the two partitions is given by the number of pairs appearing together in an equivalence class in both partitions. LetC be fixed andY an unknown partition represented by its marginal description. We consider the following problem: finding the distribution of objects (conditioned by the marginal description ofY) which maximizes the agreement betweenC andY. We discuss some approaches that have proposed, and we obtain a heuristic procedure from the solution of a linear transportation problem.","Agreement, Measures of association, contingency table",https://link.springer.com//article/10.1007/BF02618465
Clustering and clique partitioning: Simulated annealing and tabu search approaches,"We study the application of simulated annealing and tabu search to the solution of the clique partitioning problem. We illustrate the effecveness of these techniques by computational results associated not only with randomly generated problems, but also with real-life problems arising from applications concerning the optimal aggregation of binary relations into an equivalence relation. The need for these approaches is emphasized by the example of a special class of instances of the clique partitioning problem for which the most commonly used heuristics perform arbitrarily badly, while tabu search systematically obtains the optimal solution.","Clustering, Binary relations, Equivalence relation, Cliques, Combinatorial optimization, Heuristics, Simulated annealing, Tabu search",https://link.springer.com//article/10.1007/BF02618466
Direct multicriteria clustering algorithms,"In a multicriteria clustering problem, optimization over more than one criterion is required. The problem can be treated in different ways: by reduction to a clustering problem with the single criterion obtained as a combination of the given criteria; by constrained clustering algorithms where a selected critetion is considered as the clustering criterion and all others determine the constraints; or by direct algorithms. In this paper two types of direct algorithms for solving multicriteria clustering problem are proposed: the modified relocation algorithm, and the modified agglomerative algorithm. Different elaborations of these two types of algorithms are discussed and compared. Finally, two applications of the proposed algorithms are presented.","Multicriteria optimization, Pareto-efficiency, Agglomerative hierarchical algorithms, Local optimization, Relocation algorithms, Constrained clustering, Consensus clustering",https://link.springer.com//article/10.1007/BF02618467
The runt test for multimodality,"Single linkage clusters on a set of points are the maximal connected sets in a graph constructed by connecting all points closer than a given threshold distance. The complete set of single linkage clusters is obtained from all the graphs constructed using different threshold distances. The set of clusters forms a hierarchical tree, in which each non-singleton cluster divides into two or more subclusters; the runt size for each single linkage cluster is the number of points in its smallest subcluster. The maximum runt size over all single linkage clusters is our proposed test statistic for assessing multimodality. We give significance levels of the test for two null hypotheses, and consider its power against some bimodal alternatives.","Single linkage clusters, Minimal spanning tree, Tests for modes, The RUNT test",https://link.springer.com//article/10.1007/BF02618468
Significance of the length of the shortest tree,"The distribution of lengths of phylogenetic trees under the taxonomic principle of parsimony is compared with the distribution obtained by randomizing the characters of the sequence data. This comparison allows us to define a measure of the extent to which sequence data contain significant hierarchical information. We show how to calculate this measure exactly for up to 10 taxa, and provide a good approximation for larger sets of taxa. The measure is applied to test sequences on 10 and 15 taxa.","Binary trees, Parsimony, Minimum-length tree, Fitch's algorithm, Distributions",https://link.springer.com//article/10.1007/BF02618469
The complexity of reconstructing trees from qualitative characters and subtrees,"In taxonomy and other branches of classification it is useful to know when tree-like classifications on overlapping sets of labels can be consistently combined into a parent tree. This paper considers the computation complexity of this problem. Recognizing when a consistent parent tree exists is shown to be intractable (NP-complete) for sets of unrooted trees, even when each tree in the set classifies just four labels. Consequently determining the compatibility of qualitative characters and partial binary characters is, in general, also NP-complete. However for sets of rooted trees an algorithm is described which constructs the “strict consensus tree” of all consistent parent trees (when they exist) in polynomial time. The related question of recognizing when a set of subtrees uniquely defines a parent tree is also considered, and a simple necessary and sufficient condition is described for rooted trees.","Trees, Qualitative characters, Compatibility, Resolved quartets, Clusters, Strict consensus tree",https://link.springer.com//article/10.1007/BF02618470
Character and OTU stability in five taxonomic groups,"The character and OTU stability of classifications based on UPGMA clustering and maximum parsimony (MP) trees were compared for 5 datasets (families of angiosperms, families of orthopteroid insects, species of the fish genusIctalurus, genera of the salamander family Salamandridae, and genera of the frog family Myobatrachidae). Stability was investigated by taking different sized random subsamples of OTUs or characters, computing UPGMA clusters and an MP tree, and then comparing the resulting trees with those based on the entire dataset. Agreement was measured by two consensus indices, that of Colless, computed from strict consensus trees, and Stinebrickner's 0.5-consensus index.Tests of character stability generally showed a monotone decrease in agreement with the standard as smaller sets of characters are considered. The relative success of the two methods depended upon the dataset. Tests of OTU stability showed a monotone decrease in agreement for UPGMA as smaller sets of OTUs are considered. But for MP, agreement decreased and then increased again on the same scale. The apparent superiority of UPGMA relative to MP with respect to OTU stability depended upon the dataset. Considerations other than stability, such as computer efficiency or accuracy, will also determine the method of choice for classifications.","Taxonomic stability, Congruence, Consensus, Biological classification",https://link.springer.com//article/10.1007/BF02618471
Clustering criteria for discrete data and latent class models,"We show that a well-known clustering criterion for discrete data, the information criterion, is closely related to the classification maximum likelihood criterion for the latent class model. This relation can be derived from the Bryant-Windham construction. Emphasis is placed on binary clustering criteria which are analyzed under the maximum likelihood approach for different multivariate Bernoulli mixtures. This alternative form of criterion reveals non-apparent aspects of clustering techniques. All the criteria discussed can be optimized with the alternating optimization algorithm. Some illustrative applications are included.","Clustering of binary data, Multivariate Bernoulli mixture, Classification maximum likelihood",https://link.springer.com//article/10.1007/BF02616237
The generation of random ultrametric matrices representing dendrograms,"Many methods and algorithms to generate random trees of many kinds have been proposed in the literature. No procedure exists however for the generation of dendrograms with randomized fusion levels. Randomized dendrograms can be obtained by randomizing the associated cophenetic matrix. Two algorithms are described. The first one generates completely random dendrograms, i.e., trees with a random topology, random fusion level values, and random assignment of the labels. The second algorithm uses a double-permutation procedure to randomize a given dendrogram; it proceeds by randomization of the fixed fusion levels, instead of using random fusion level values. A proof is presented that the double-permutation procedure is a Uniform Random Generation Algorithmsensu Furnas (1984), and a complete example is given.","Random dendrograms, Random matrices, Uniform sampling, Tree algorithm, Monte Carlo studies, Clustering methodology",https://link.springer.com//article/10.1007/BF02616238
On the performance of a discriminant function,"In two-class discriminant problems, objects are allocated to one of the two classes by means of threshold rules based on discriminant functions. In this paper we propose to examine the quality of a discriminant functiong in terms of its performance curve. This curve is the plot of the two misclassification probabilities as the thresholdt assumes various real values. The role of such performance curves in evaluating and ordering discriminant functions and solving discriminant problems is presented. In particular, it is shown that: (i) the convexity of such a curve is a sufficient condition for optimal use of the information contained in the data reduced byg, and (ii)g with non-convex performance curve should be corrected by an explicitly obtained transformation.","Deferred decision, Forced decision, Grade transformation, Identification rule, Likelihood ratio, Neyman-Pearson Curve, Screening",https://link.springer.com//article/10.1007/BF02616239
A new approach to isotonic agglomerative hierarchical clustering,Hierarchical clustering methods must be isotonic for the construction of ultrametric. We present a general strategy to widen the class of isotonic methods implemented by agglomerative algorithms. At each step of the agglomeration we allow one of several admissible pairs to be chosen. Then under mild assumptions an appropriate definition of admissibility guarantees isotony. Moreover we consider the use of the new methods to compute locally optimal ultrametrics. Two examples demonstrate the ability to define new agglomerative methods superior to their traditional competitors.,"Ultrametric, Local optimality, Least squares approximation, Average linkage, Approximation from above, complete linkage",https://link.springer.com//article/10.1007/BF02616240
A Monte Carlo study of the 632 bootstrap estimator of error rate,"Efron (1983) proposed the 632 bootstrap error rate estimator, and demonstrated its good performance in simulations. However, further recent studies have suggested that it only performs well for a restricted range of true error rates. We conduced an intensive, simulation study of the 632 estimator to investigate this hypothesis in detail. There was no evidence to support the contention, although the estimator's performance did vary with the true error rate.We also investigated some more general aspects of error rate studies. Error rate is bounded by 0 and 1, so that comparisons based on an untransformed scale may be inappropriate. We therefore explored the consequences of stretching the error rate scale. Furthermore, the results of such studies are typically generated as mixture distributions, because the performance results are averages over the true error rate values arising from underlying distributions which have a fixed optimal error rate. We discovered, rather surprisingly, that this has little effect if the conclusions are based on mean squared error.","Error rate, Bootstrap, 632 estimator, Resubstitution estimator",https://link.springer.com//article/10.1007/BF02616241
A comparison of two approaches to fitting directed graphs to nonsymmetric proximity measures,"Two algorithms for fitting directed graphs to nonsymmetric proximity data are compared. The first approach, termed MAPNET, is a direct extension of a mathematical programming procedure for fitting undirected graphs to symmetric proximity data presented by Klauer and Carroll (1989). For a user-specified number of links, the algorithm seeks to provide the connected network that gives the least-squares approximation of the proximity data with the specified number of links, allowing for linear transformations of the data. The mathematical programming approach is compared to the NETSCAL method for fitting directed graphs (Hutchinson 1989), using the Monte Carlo methods and data sets employed by Hutchinson.","Multivariate data analysis, ordinal network representation, NETSCAL, proximities",https://link.springer.com//article/10.1007/BF02616242
Efficient algorithms for divisive hierarchical clustering with the diameter criterion,"Divisive hierarchical clustering algorithms with the diameter criterion proceed by recursively selecting the cluster with largest diameter and partitioning it into two clusters whose largest diameter is smallest possible. We provide two such algorithms with complexitiesO(\(\overline M \)N2) andO(N2logN) respectively, where\(\overline M \) denotes the maximum number of clusters in a partition andN the number of entities to be clustered. The former algorithm, an efficient implementation of an algorithm of Hubert, allows to find all partitions into at most\(\overline M \) clusters and is inO(N2) for fixed\(\overline M \). Moreover, if in each partitioning the size of the largest cluster is bounded byp times the number of entities in the set to be partitioned, with 1/2<=p<1, it provides a complete hierarchy of partitionsO(N2 logN) time. The latter algorithm, a refinement of an algorithm of Rao allows to build a complete hierarchy of partitions inO(N2 logN) time without any restriction. Comparative computational experiments with both algorithms and with an agglomerative hierarchical algorithm of Benzécri are reported.","Divisive hierarchical clustering, Diameter, Complexity, Polynomial algorithm",https://link.springer.com//article/10.1007/BF02616245
Large-sample results for optimization-based clustering methods,"Many common (nonhierarchical) clustering and classification methods are optimization-based methods, in the sense described by Windham (1987) in this Journal. This paper gives some large sample properties for estimates derived by such methods. Under appropriate conditions, such estimates converge with probability one to a limit, and are asymptotically normally distributed around that limiting value. The conditions are satisfied by most of the common examples of optimization-based methods.","Classification, Clustering, Maximum likelihood, Asymptotic properties",https://link.springer.com//article/10.1007/BF02616246
A latent class probit model for analyzing pick any/N data,"A latent class probit model is developed in which it is assumed that the binary data of a particular subject follow a finite mixture of multivariate Bermoulli distributions. An EM algorithm for fitting the model is described and a Monte Carlo procedure for testing the number of latent classes that is required for adequately describing the data is discussed. In the final section, an application of the latent class probit model to some intended purchase data for residential telecommunication devices is reported.","Probit Model, Latent Class Analysis, Finite Mixture Distribution, EM Algorithm, Monte Carlo Significance Test, Market Segmentation",https://link.springer.com//article/10.1007/BF02616247
A permutation-based algorithm for block clustering,"Hartigan (1972) discusses the direct clustering of a matrix of data into homogeneous blocks. He introduces a stepwise divisive method for block clustering within a certain class of block structures which induce clustering trees for both row and column margins. While this class of structures is appealing, the stopping criterion for his method, which is based on asymptotic theory and the assumption that the individual elements of the data matrix are normally distributed, is quite restrictive. In this paper we propose a permutation-based algorithm for block clustering within the same class of block structures. By using permutation arguments to decide where to split and when to stop, our algorithm becomes applicable in a wide variety of cases, including matrices of categorical data and matrices of small-to-moderate size. In addition, our algorithm offers considerable flexibility in how block homogeneity is defined. The algorithm is studied in a series of simulation experiments on matrices of known structure, and illustrated in examples drawn from the fields of taxonomy, political science, and data architecture.","Binary splitting, Blck clustering, Markov chain simulation method, Permutation distribution",https://link.springer.com//article/10.1007/BF02616248
Some notes on the diagonalization of the extended three-mode core matrix,"We extend previous results of Kroonenberg and de Leeuw (1980) and Kroonenberg (1983, Ch. 5) on transformations of the extended core matrix of the Tucker2 model (Kroonenberg and de Leeuw 1980). In particular, it is shown that non-singular transformations to diagonalize the core matrix will lead to PARAFAC solutions (Harshman 1970; Harshman and Lundy 1984), if such solutions exist.","Three-mode principal component analysis, Parallel factor analysis, PARAFAC, CANDECOMP",https://link.springer.com//article/10.1007/BF02616249
A mathematical model for classification and identification,"A unified model for classification and identification is presented by means of the theory of mathematical structures. The methodologies for classification and identification differ. For classification, an interchange scalar mechanism (ISCL) is proposed which improves the capacity of classifications irrespective of character weighting, the number of characters used, and the form of the clustering algorithm. It modifies a distance between two OTUs by considering their distances to their nearest neighbors. Formulas are developed for reliability, velocity, and capability of taxonomic keys.","Classification, Identification, Interchange scalar classifications, Numerical taxonomy, Taxonomic keys",https://link.springer.com//article/10.1007/BF02616250
A sequential fitting procedure for linear data analysis models,"A particular factor analysis model with parameter constraints is generalized to include classification problems definable within a framework of fitting linear models. The sequential fitting (SEFIT) approach of principal component analysis is extended to include several nonstandard data analysis and classification tasks. SEFIT methods attempt to explain the variability in the initial data (commonly defined by a sum of squares) through an additive decomposition attributable to the various terms in the model. New methods are developed for both traditional and fuzzy clustering that have useful theoretic and computational properties (principal cluster analysis, additive clustering, and so on). Connections to several known classification strategies are also stated.","Cluster analysis, Fuzzy clustering, (bi)linear model, Principal clusters, Additive clusters, Association measures for cross-classifications, Additive types",https://link.springer.com//article/10.1007/BF01908715
Piecewise hierarchical clustering,"We consider two or more ultrametric distance matrices defined over different, possibly overlapping, subsets. These matrices are merged into one ultrametric matrix defined over the whole set. Necessary and sufficient conditions for uniqueness of the merging are established. when these conditions are not satisfied, consistent algorithms are given.","Hierarchical clustering, Supertrees, Consensus trees, Subdomain classification",https://link.springer.com//article/10.1007/BF01908716
Weight constrained maximum split clustering,"ConsiderN entities to be classified, with given weights, and a matrix of dissimilarities between pairs of them. The split of a cluster is the smallest dissimilarity between an entity in that cluster and an entity outside it. The single-linkage algorithm provides partitions intoM clusters for which the smallest split is maximum. We consider the problems of finding maximum split partitions with exactlyM clusters and with at mostM clusters subject to the additional constraint that the sum of the weights of the entities in each cluster never exceeds a given bound. These two problems are shown to be NP-hard and reducible to a sequence of bin-packing problems. A Θ (N2) algorithm for the particular caseM =N of the second problem is also presented. Computational experience is reported.","Partition, Single-linkage, Split, Cluster weight, Constrained clustering",https://link.springer.com//article/10.1007/BF01908717
The coefficient of variation biplot,"This note introduces the coefficient of variation biplot, and suggests that it will provide a useful graphical display of data matrices in which the relative variability of the columns is of interest.","Biplot, Coefficient of variation",https://link.springer.com//article/10.1007/BF01908718
Constructing dissimilarity measures,"The paper addresses the problem of specifying differential weights for variables in the construction of a measure of dissimilarity. An assessor is required to provide subjective judgments of the pairwise dissimilarities within a training set of objects, and these dissimilarities are then modeled as a function of the recorded differences between the objects on each of the variables. The aim is to make explicit the relative importance that assessors attach to each of the variables, and thus obtain guidance on how these variables should be combined into a relevant dissimilarity matrix. The methodology is illustrated by application to some archaeological data.","Dissimilarity, Least squares monotone regression, Similarity, Standardization of variables, Weighting of variables",https://link.springer.com//article/10.1007/BF01908719
A preliminary study of optimal variable weighting in k-means clustering,"Recently, algorithms for optimally weighting variables in non-hierarchical and hierarchical clustering methods have been proposed. Preliminary Monte Carlo research has shown that at least one of these algorithms cross-validates extremely well.The present study applies a k-means, optimal weighting procedure to two empirical data sets and contrasts its cross-validation performance with that of unit (i.e., equal) weighting of the variables. We find that the optimal weighting procedure cross-validates better in one of the two data sets. In the second data set its comparative performance strongly depends on the approach used to find seed values for the initial k-means partitioning.","k-means clustering, optimal weighting, Rand index, Cross validation",https://link.springer.com//article/10.1007/BF01908720
Distribution of some similarity coefficients for dyadic binary data in the case of associated attributes,Parameters are derived of distributions of three coefficients of similarity between pairs (dyads) of operational taxonomic units for multivariate binary data (presence/absence of attributes) under statistical independence. These are applied to test independence for dyadic data. Association among attributes within operational taxonomic units is allowed. It is also permissible for the two units in the dyad to be drawn from different populations having different presence probabilities of attributes. The variance of the distribution of the similarity coefficients under statistical independence is shown to be relatively large in many empirical situations. This result implies that the practical interpretation of these coefficients requires much care. An application using the Jaccard index is given for the assessment of consensus between psychotherapists and their clients.,"Consensus, Dice coefficient, Jaccard coefficient, Simple Matching coefficient, Multivariate binary data, Observer agreement, Similarity coefficients, Beta distribution",https://link.springer.com//article/10.1007/BF01889701
Clique optimization: A method to construct parsimonious ultrametric trees from similarity data,"Clique optimization (CLOPT) is a clustering procedure that reduces similarity data,S, to a hierarchy of nested partitions modeled as an ordered sequence of clique sets. In the model,M, clique sets represent partitions in the hierarchy and are strictly ordered by a parameter, α. Applied as a threshold toM, α abstracts the corresponding clique set. Cliques, or completely connected components ofM, are the elements of these sets. The clique sets and their α's are determined by incrementally minimizing Σ(sij− mij)2 by a combination of divisive and agglomerative operations. Both operations search for optimal solutions in a restricted, 2-dimensional space of partitions. The first clique set abstracted by CLOPT is the single partition that is closest toS. CLOPT constructs models that compare favorably with HCS and it does so using fewer parameters.","Algorithm, Graph clustering, Hierarchical classification, Heuristic search, NP-complete",https://link.springer.com//article/10.1007/BF01889702
Approximate analysis of variance of spatially autocorrelated regional data,"The classical method for analysis of variance of data divided in geographic regions is impaired if the data are spatially autocorrelated within regions, because the condition of independence of the observations is not met. Positive autocorrelation reduces within-group variability, thus artificially increasing the relative amount of among-group variance. Negative autocorrelation may produce the opposite effect. This difficulty can be viewed as a loss of an unknown number of degrees of freedom. Such problems can be found in population genetics, in ecology and in other branches of biology, as well as in economics, epidemiology, geography, geology, marketing, political science, and sociology. A computer-intensive method has been developed to overcome this problem in certain cases. It is based on the computation of pooled within-group sums of squares for sampled permutations of internally connected areas on a map. The paper presents the theory, the algorithms, and results obtained using this method. A computer program, written in PASCAL, is available.","Analysis of variance, Choropleth map, Ecology, Genetics, Geography, Permutation test, Spatial autocorrelation",https://link.springer.com//article/10.1007/BF01889703
The median procedure for n-trees as a maximum likelihood method,A few axioms are presented which allow the median procedure for n-trees to be given a maximum likelihood interpretation.,"Consensus, Median procedure, Maximum likelihood",https://link.springer.com//article/10.1007/BF01889704
Between-group analysis with heterogeneous covariance matrices: The common principal component model,"Analysis of between-group differences using canonical variates assumes equality of population covariance matrices. Sometimes these matrices are sufficiently different for the null hypothesis of equality to be rejected, but there exist some common features which should be exploited in any analysis. The common principal component model is often suitable in such circumstances, and this model is shown to be appropriate in a practical example. Two methods for between-group analysis are proposed when this model replaces the equal dispersion matrix assumption. One method is by extension of the two-stage approach to canonical variate analysis using sequential principal component analyses as described by Campbell and Atchley (1981). The second method is by definition of a distance function between populations satisfying the common principal component model, followed by metric scaling of the resulting between-populations distance matrix. The two methods are compared with each other and with ordinary canonical variate analysis on the previously introduced data set.","Between-group analysis, Canonical variate analysis, Common principal component model, Eigenvalues and eigenvectors, Matusita distance between populations, Metric scaling, Principal component analysis",https://link.springer.com//article/10.1007/BF01889705
Cross-sectional approach for clustering time varying data,"Cluster analysis is to be performed on a three-mode data matrix of type: units, variables, time. A general model for calculating the distance between two units varying in time is proposed. One particular model is developed and used in an example concerned with clustering of 23 European countries according to the similarity of energy consumption in the years 1976–1982.","Generalized Ward clustering problem, Relocation clustering procedure, Time dimension, Dissimilarity between trajectories, Compound interest model",https://link.springer.com//article/10.1007/BF01889706
Metric family portraits,"By associating a whole distance matrix with a single point in a parameter space, a family of matrices (e.g., all those obeying the triangle inequality) can be shown as a cloud of points. Pictures of the cloud form a family portrait, and its characteristic shape and interrelationship with the portraits of other families can be explored. Critchley (unpublished) used this approach to illustrate, for distances between three points, algebraic results on the nesting relations between various metrics. In this paper, these diagrams are further investigated and then generalized. In the first generalization, projective geometry is used to allow the geometric representation of Additive Mixture, Additive Constant, and Missing Data problems. Then the six-dimensional portraits of four-point distance matrices are studied, revealing differences between the Euclidean, Additive Tree, and General Metric families. The paper concludes with caveats and insights concerning families of generaln-point metric matrices.","Metric space, Distance matrices, High-dimensional graphics",https://link.springer.com//article/10.1007/BF01908587
A validation study of a variable weighting algorithm for cluster analysis,"De Soete (1986, 1988) proposed a variable weighting procedure when Euclidean distance is used as the dissimilarity measure with an ultrametric hierarchical clustering method. The algorithm produces weighted distances which approximate ultrametric distances as closely as possible in a least squares sense. The present simulation study examined the effectiveness of the De Soete procedure for an applications problem for which it was not originally intended. That is, to determine whether or not the algorithm can be used to reduce the influence of variables which are irrelevant to the clustering present in the data. The simulation study examined the ability of the procedure to recover a variety of known underlying cluster structures. The results indicate that the algorithm is effective in identifying extraneous variables which do not contribute information about the true cluster structure. Weights near 0.0 were typically assigned to such extraneous variables. Furthermore, the variable weighting procedure was not adversely effected by the presence of other forms of error in the data. In general, it is recommended that the variable weighting procedure be used for applied analyses when Euclidean distance is employed with ultrametric hierarchical clustering methods.","Ultrametric trees, Hierarchical clustering, Euclidean distances",https://link.springer.com//article/10.1007/BF01908588
The inference of hierarchical schemes for multinomial data,"Data in an experimental array where a nominal dependent variable hasm>2 outcomes may be accounted for by one of a number of possible schemes consisting ofJ successive and/or parallel independentmi-nomial experiments where εmi =m +J − 1. Each such scheme can be represented by a tree diagram which is presumed to be valid everywhere in the array. A criterion based on likelihood is defined to assess the different schemes. The set of outcome probabilities of a scheme is shown to differ from that of all other schemes almost everywhere in the space of parameters. As sample size increases, the probability of correctly inferring the true tree tends to 1. Using Monte-Carlo simulation of the four-outcome case, we illustrate, for small sample sizes, how this probability depends on the parameters.","Hierarchical classification, Multinomial, Likelihood, Logistic regression",https://link.springer.com//article/10.1007/BF01908589
An index of goodness-of-fit based on noncentrality,"Akaike's Information Criterion is systematically dependent on sample size, and therefore cannot be used in practice as a basis for model selection. An alternative measure of goodness-of-fit, based like Akaike's on the noncentrality parameter, appears to be consistent over variations in sample size.","Goodness-of-fit, Covariance structures, Linear structural relations",https://link.springer.com//article/10.1007/BF01908590
An evaluation of five algorithms for generating an initial configuration for SINDSCAL,"Five different methods for obtaining a rational initial estimate of the stimulus space in the INDSCAL model were compared using the SINDSCAL program for fitting INDSCAL. The effect of the number of stimuli, the number of subjects, the dimensionality, and the amount of error on the quality and efficiency of the final SINDSCAL solution were investigated in a Monte Carlo study. We found that the quality of the final solution was not affected by the choice of the initialization method, suggesting that SINDSCAL finds a global optimum regardless of the initialization method used. The most efficient procedures were the methods proposed by by de Leeuw and Pruzansky (1978) and by Flury and Gautschi (1986) for the simultaneous diagonalization of several positive definite symmetric matrices, and a method based on linearly constraining the stimulus space using the CANDELINC approach developed by Carroll, Pruzansky, and Kruskal (1980).","Individual differences, Multidimensional scaling, Rational starting configuration, INDSCAL",https://link.springer.com//article/10.1007/BF01908591
Maximum sum-of-splits clustering,"ConsiderN entities to be classified, and a matrix of dissimilarities between pairs of them. The split of a cluster is the smallest dissimilarity between an entity of this cluster and an entity outside it. The single-linkage algorithm provides partitions intoM clusters for which the smallest split is maximum. We study here the average split of the clusters or, equivalently, the sum of splits. A Θ(N2) algorithm is provided to determine maximum sum-of-splits partitions intoM clusters for allM betweenN − 1 and 2, using the dual graph of the single-linkage dendrogram.","Partition, Split, Dendrogram, Dual graph, Complexity, Polynomial algorithm",https://link.springer.com//article/10.1007/BF01908598
Automated design of multiple-class piecewise linear classifiers,"A new method and a supporting theorem for designing multiple-class piecewise linear classifiers are described. The method involves the cutting of straight line segments joining pairs of opposed points (i.e., points from distinct classes) ind-dimensional space. We refer to such straight line segments aslinks. We show how nearly to minimize the number of hyperplanes required to cut all of these links, thereby yielding a near-Bayes-optimal decision surface regardless of the number of classes, and we describe the underlying theory. This method does not require parameters to be specified by users — an improvement over earlier methods. Experiments on multiple-class data obtained from ship images show that classifiers designed by this method yield approximately the same error rate as the bestk-nearest neighbor rule, while providing faster decisions.","Classifier, Piecewise linear, Hyperplane, Nearest neighbor rule, prototype, Cluster, Window training, Bayes decisions, training-set consistency",https://link.springer.com//article/10.1007/BF01908599
"Fast random generation of binary, t-ary and other types of trees","Trees, and particularly binary trees, appear frequently in the classification literature. When studying the properties of the procedures that fit trees to sets of data, direct analysis can be too difficult, and Monte Carlo simulations may be necessary, requiring the implementation of algorithms for the generation of certain families of trees at random. In the present paper we use the properties of Prufer's enumeration of the set of completely labeled trees to obtain algorithms for the generation of completely labeled, as well as terminally labeled t-ary (and in particular binary) trees at random, i.e., with uniform distribution. Actually, these algorithms are general in that they can be used to generate random trees from any family that can be characterized in terms of the node degrees. The algorithms presented here are as fast as (in the case of terminally labeled trees) or faster than (in the case of completely labeled trees) any other existing procedure, and the memory requirements are minimal. Another advantage over existing algorithms is that there is no need to store pre-calculated tables.","Tree algorithms, Monte Carlo studies, Clustering methodology",https://link.springer.com//article/10.1007/BF01908600
A classification of presence/absence based dissimilarity coefficients,Several desirable order properties for dissimilarity coefficients based on presence/absence of attributes are given and several popular dissimilarity coefficients are examined with respect to these properties. A characterization for rational functions with linear numerator and linear denominator satisfying all of the desirable properties is given.,"Measures of association, Profile similarities",https://link.springer.com//article/10.1007/BF01908601
A mathematical programming approach to fitting general graphs,"We present an algorithm for fitting general graphs to proximity data. The algorithm utilizes a mathematical programming procedure based on a penalty function approach to impose additivity constraints upon parameters. For a user-specified number of links, the algorithm seeks to provide the connected network that gives the least-squares approximation to the proximity data with the specified number of links, allowing for linear transformations of the data. The network distance is the minimum-path-length metric for connected graphs. As a limiting case, the algorithm provides a tree where each node corresponds to an object, if the number of links is set equal to the number of objects minus one. A Monte Carlo investigation indicates that the resulting networks tend to fall within one percentage point of the least-squares solution in terms of the variance accounted for, but do not always attain this global optimum. The network model is discussed in relation to ordinal network representations (Klauer 1989) and NETSCAL (Hutchinson 1989), and applied to several well-known data sets.","Multivariate data analysis, Ordinal network representation, NETSCAL, Proximities",https://link.springer.com//article/10.1007/BF01908602
Convergence of the majorization method for multidimensional scaling,"In this paper we study the convergence properties of an important class of multidimensional scaling algorithms. We unify and extend earlier qualitative results on convergence, which tell us when the algorithms are convergent. In order to prove global convergence results we use the majorization method. We also derive, for the first time, some quantitative convergence theorems, which give information about the speed of convergence. It turns out that in almost all cases convergence is linear, with a convergence rate close to unity. This has the practical consequence that convergence will usually be very slow, and this makes techniques to speed up convergence very important. It is pointed out that step-size techniques will generally not succeed in producing marked improvements in this respect.","Multidimensional scaling, Convergence, Step size, Local minima",https://link.springer.com//article/10.1007/BF01897162
A study of standardization of variables in cluster analysis,"A methodological problem in applied clustering involves the decision of whether or not to standardize the input variables prior to the computation of a Euclidean distance dissimilarity measure. Existing results have been mixed with some studies recommending standardization and others suggesting that it may not be desirable. The existence of numerous approaches to standardization complicates the decision process. The present simulation study examined the standardization problem. A variety of data structures were generated which varied the intercluster spacing and the scales for the variables. The data sets were examined in four different types of error environments. These involved error free data, error perturbed distances, inclusion of outliers, and the addition of random noise dimensions. Recovery of true cluster structure as found by four clustering methods was measured at the correct partition level and at reduced levels of coverage. Results for eight standardization strategies are presented. It was found that those approaches which standardize by division by the range of the variable gave consistently superior recovery of the underlying cluster structure. The result held over different error conditions, separation distances, clustering methods, and coverage levels. The traditionalz-score transformation was found to be less effective in several situations.","Standard scores, Cluster analysis",https://link.springer.com//article/10.1007/BF01897163
Variable selection in clustering,"Standard clustering algorithms can completely fail to identify clear cluster structure if that structure is confined to a subset of the variables. A forward selection procedure for identifying the subset is proposed and studied in the context of complete linkage hierarchical clustering. The basic approach can be applied to other clustering methods, too.","Variable selection, Cluster analysis of two-mode data, scaling of variables, Pillai trace statistic, Interactive data analysis",https://link.springer.com//article/10.1007/BF01897164
Thresholded consensus for n-trees,"A class of (multiple) consensus methods for n-trees (dendroids, hierarchical classifications) is studied. This class constitutes an extension of the so-called median consensus in the sense that we get two numbersm andm′ such that: If a clusterX occurs ink n-trees of a profileP, withk ≥m′, then it occurs in every consensus n-tree ofP. IfX occurs ink′ n-trees ofP, withm ≤k <m′, then it may, or may not, belong to a consensus n-tree ofP. IfX occurs ink″ n-trees ofP, withk″ <m then it cannot occur in any consensus n-tree ofP. If these conditions are satisfied, the multiconsensus function is said to be thresholded by the pair (m,m′). Two results are obtained. The first one characterizes the pairs of numbers that can be viewed as thresholds for some consensus function. The second one provides a characterization of thresholded consensus methods. As an application a characterization of the quota rules is provided.","Consensus, n-trees, Hierarchical clustering",https://link.springer.com//article/10.1007/BF01897165
Recent convergence results for the fuzzy c-means clustering algorithms,"One of the main techniques embodied in many pattern recognition systems is cluster analysis — the identification of substructure in unlabeled data sets. The fuzzy c-means algorithms (FCM) have often been used to solve certain types of clustering problems. During the last two years several new local results concerning both numerical and stochastic convergence of FCM have been found. Numerical results describe how the algorithms behave when evaluated as optimization algorithms for finding minima of the corresponding family of fuzzy c-means functionals. Stochastic properties refer to the accuracy of minima of FCM functionals as approximations to parameters of statistical populations which are sometimes assumed to be associated with the data. The purpose of this paper is to collect the main global and local, numerical and stochastic, convergence results for FCM in a brief and unified way.","Cluster analysis, Convergence, Fuzzy c-means algorithm, Optimization, Partitioning Algorithms, Pattern recognition",https://link.springer.com//article/10.1007/BF01897166
A maximum likelihood methodology for clusterwise linear regression,"This paper presents a conditional mixture, maximum likelihood methodology for performing clusterwise linear regression. This new methodology simultaneously estimates separate regression functions and membership inK clusters or groups. A review of related procedures is discussed with an associated critique. The conditional mixture, maximum likelihood methodology is introduced together with the E-M algorithm utilized for parameter estimation. A Monte Carlo analysis is performed via a fractional factorial design to examine the performance of the procedure. Next, a marketing application is presented concerning the evaluations of trade show performance by senior marketing executives. Finally, other potential applications and directions for future research are identified.","Cluster analysis, Multiple regression, Maximum likelihood estimation, E-M algorithm, Marketing trade shows",https://link.springer.com//article/10.1007/BF01897167
Hierarchical trees can be perfectly scaled in one dimension,"Holman (1972) proved theorems which led him to suggest that there was a fundamental opposition between hierarchical clustering and non-metric Euclidean multidimensional scaling. Empirical experience has shown this to be untrue. Explanations of this apparent contradiction have been offered previously by Kruskal (1977) and Critchley (1986). In this paper we point out the feasibility of perfectly scaling a hierarchical tree in one dimension when the primary approach to ties (Kruskal 1964) is taken. Indeed, there is a whole polyhedral convex cone of solutions for which we obtain an explicit expression.","Monotone regression, One-dimensional scaling, Polyhedral convex cone, Primary approach to ties, Tree, Ultrametric",https://link.springer.com//article/10.1007/BF01901668
Statistical tests on two characteristics of the shapes of cluster diagrams,"A cluster diagram is a rooted planar tree that depicts the hierarchical agglomeration of objects into groups of increasing size. On the null hypothesis that at each stage of the clustering procedure all possible joins are equally probable, we derive the probability distributions for two properties of these diagrams: (1)S, the number of single objects previously ungrouped that are joined in the final stages of clustering, and (2)mk, the number of groups ofk+1 objects that are formed during the process. Ecological applications of statistical tests for these properties are described and illustrated with data from weed communities of Saskatchewan fields.","Combinatorial enumeration, Dendrogram, Hierarchical cluster analysis, Shape, Significance testing",https://link.springer.com//article/10.1007/BF01901669
Clustering the rows and columns of a contingency table,"A number of ways of investigating heterogeneity in a two-way contingency table are reviewed. In particular, we consider chi-square decompositions of the Pearson chi-square statistic with respect to the nodes of a hierarchical clustering of the rows and/or the columns of the table. A cut-off point which indicates “significant clustering” may be defined on the binary trees associated with the respective row and column cluster analyses. This approach provides a simple graphical procedure which is useful in interpreting a significant chi-square statistic of a contingency table.","Chi-square statistic, Cluster analysis, Contingency tables, Correspondence analysis, Multiple comparisons, Wishart distribution",https://link.springer.com//article/10.1007/BF01901670
Robust classification procedures based on dichotomous and continuous variables,"For classifying a univariate or a multivariate observation in one of the two populations, Tiku and Balakrishnan (1984) and Balakrishnan, Tiku and Shaarawi (1985) developed robust (to departures from normality) procedures. These procedures are extended here to situations where the classification has to be based on the observed value of a pair of variables, one being a dichotomous random variable and the other a univariate or a multivariate continuous random variable.","Classification, Robust likelihood, MML estimators, Outliers",https://link.springer.com//article/10.1007/BF01901671
On characterizing optimization-based clustering methods,"This paper suggests a simplification of a recent approach suggested by Windham to characterizing optimization-based clustering methods. The simplification is based on noting an analogy between certain quantities in Windham's formulation and corresponding quantities in mathematical statistics, particularly sufficient statistics and the exponential family of densities.","Classification, Numerical classification, Sufficient statistics, Exponential families",https://link.springer.com//article/10.1007/BF01901672
Comments on “aggregation of equivalence relations” by P. C. Fishburn and A. Rubinstein,"The theorem of the paper “Aggregation of Equivalence Relations,” by Fishburn and Rubinstein, states a result already known. This theorem improves a result from Mirkin (1975) and appears as a corollary occurring in Leclerc (1984).","Consensus, Non-hierarchical clustering, Arrowian theorem",https://link.springer.com//article/10.1007/BF01901673
Function approximation for incompletely specified regression models,"This report extends earlier work by Brailovsky on regression theory and methodology, giving particular emphasis to function approximation for incompletely specified models. The interest here is with situations where the form of the regression relation is not known in advance. We discuss several difficulties that arise in using local approximation and linear regression methods, and propose ways to overcome these problems. To aid the data analyst in developing a suitable model, an illustrative table is derived for determining the number of initial explanatory functions justifiable for a given prespecified confidence level. The general approach formulated here is illustrated with an application to medical data. Relevance to classification and possible extensions are discussed.","Function approximation, Subset regression, Nonlinear models",https://link.springer.com//article/10.1007/BF01901675
Least squares algorithms for constructing constrained ultrametric and additive tree representations of symmetric proximity data,"A mathematical programming algorithm is developed for fitting ultrametric or additive trees to proximity data where external constraints are imposed on the topology of the tree. The two procedures minimize a least squares loss function. The method is illustrated on both synthetic and real data. A constrained ultrametric tree analysis was performed on similarities between 32 subjects based on preferences for ten odors, while a constrained additive tree analysis was carried out on some proximity data between kinship terms. Finally, some extensions of the methodology to other tree fitting procedures are mentioned.","Hierarchical clustering, Path length trees, Mathematical programming, Constrained classification methods",https://link.springer.com//article/10.1007/BF01896984
The Young-Householder algorithm and the least squares multidimensional scaling of squared distances,"It is shown that replacement of the zero diagonal elements of the symmetric data matrix of approximate squared distances by certain other quantities in the Young-Householder algorithm will yield a least squares fit to squared distances instead of to scalar products. Iterative algorithms for obtaining these replacement diagonal elements are described and relationships with the ELEGANT algorithm (de Leeuw 1975; Takane 1977) are discussed. In “large residual” situations a penalty function approach, motivated by the ELEGANT algorithm, is adopted. Empirical comparisons of the algorithms are given.","Classical scaling, ELEGANT algorithm, Newton-Raphson, Squared distances",https://link.springer.com//article/10.1007/BF01896985
Parameter modification for clustering criteria,"The more ways there are of understanding a clustering technique, the more effectively the results can be analyzed and used. I will give a general procedure, calledparameter modification, to obtain from a clustering criterion a variety of equivalent forms of the criterion. These alternative forms reveal aspects of the technique that are not necessarily apparent in the original formulation. This procedure is successful in improving the understanding of a significant number of clustering techniques.The insight obtained will be illustrated by applying parameter modification to partitioning, mixture and fuzzy clustering methods, resulting in a unified approach to the study of these methods and a general algorithm for optimizing them.","Classification, Numerical classification, Partitioning, Multivariate mixture analysis, Fuzzy clustering, Numerical optimization",https://link.springer.com//article/10.1007/BF01896986
Minimum sum of diameters clustering,"The problem of determining a partition of a given set ofN entities intoM clusters such that the sum of the diameters of these clusters is minimum has been studied by Brucker (1978). He proved that it is NP-complete forM≥3 and mentioned that its complexity was unknown forM=2. We provide anO(N3 logN) algorithm for this latter case. Moreover, we show that determining a partition into two clusters which minimizes any given function of the diameters can be done inO(N5) time.","Partition, Diameter, Complexity, Polynomial algorithm, Divisive hierarchical clustering",https://link.springer.com//article/10.1007/BF01896987
On the classification of recall strings using lattice-theoretic measures,"Lattice theory is used to develop techniques for classifying groups of subjects on the basis of their recall strategies or multiple recall strategies within individual subjects. Using the ordered tree algorithm to represent sets of recall orders, it is shown how both trees and single recall strings can be represented as points within a nonsemimodular, graded lattice. Distances within the lattice structure are used to construct a dissimilarity measure,S, which can then be used to partition the individual recall strings. The measureS between strings is compared to Kendall's tau in three empirical tests, examining differences between individual subjects, differences between groups of subjects, and differences within a subject. It was shown that onlyS could recover the original differences. Differences between comparing chunks versus comparing orders are discussed.","Lattice theory, Memory, Cluster analysis",https://link.springer.com//article/10.1007/BF01896988
Additive clustering and qualitative factor analysis methods for similarity matrices,"We review methods of qualitative factor analysis (QFA) developed by the author and his collaborators over the last decade and discuss the use of QFA methods for the additive clustering problem. The QFA method includes, first, finding a square Boolean matrix in a fixed set of Boolean matrices with “simple structures” to approximate a given similarity matrix, and, second, repeating this process again and again using residual similarity matrices. We present convergence properties for three versions of the method, provide “cluster” interpretations for results obtained from the algorithms, and give formulas for the evaluation of “factor shares” of the initial similarities variance.","Additive Clustering, Qualitative factor analysis, Macrostructures, Average compactness",https://link.springer.com//article/10.1007/BF01890073
A test for spatial homogeneity in cluster analysis,"This paper proposes a measure of spatial homogeneity for sets of d-dimensional points based on nearest neighbor distances. Tests for spatial uniformity are examined which assess the tendency of the entire data set to aggregate and evaluate the character of individual clusters. The sizes and powers of three statistical tests of uniformity against aggregation, regularity, and unimodality are studied to determine robustness. The paper also studies the effects of normalization and incorrect prior information. A “percentile frame” sampling procedure is proposed that does not require a sampling window but is superior to a toroidal frame and to buffer zone sampling in particular situations. Examples test two data sets for homogeneity and search the results of a hierarchical clustering for homogeneous clusters.","Cluster homogeneity, Spatial uniformity, Cluster validity, Data normalization",https://link.springer.com//article/10.1007/BF01890074
Invariants of phylogenies in a simple case with discrete states,"Under a simple model of transition between two states, we can work out the probabilities of different data outcomes in four species with any given phylogeny. For a given tree topology, if all characters are evolving under the same probabilistic model, there are two quadratic forms in the frequencies of outcomes that must be zero. It may be possible to test the null hypothesis that the tree is of a particular topology by testing whether these quadratic forms are zero. One of the tests is a test for independence in a simple 2×2 contingency table. If there are differences of evolutionary rate among characters, these quadratic forms will no longer necessarily be zero.","Phylogenies, Statistical tests",https://link.springer.com//article/10.1007/BF01890075
A comparison between two distance-based discriminant principles,"A distance-based classification procedure suggested by Matusita (1956) has long been available as an alternative to the usual Bayes decision rule. Unsatisfactory features of both approaches when applied to multinomial data led Goldstein and Dillon (1978) to propose a new distance-based principle for classification. We subject the Goldstein/Dillon principle to some theoretical scrutiny by deriving the population classification rules appropriate not only to multinomial data but also to multivariate normal and mixed multinomial/multinormal data. These rules demonstrate equivalence of the Goldstein/Dillon and Matusita approaches for the first two data types, and similar equivalence is conjectured (but not explicitly obtained) for the mixed data case. Implications for sample-based rules are noted.","Classification rules, Discriminant analysis, Distance between groups, Divergence between populations, Influence functions",https://link.springer.com//article/10.1007/BF01890076
Parsimonious trees,"Dendrograms based onn objects can contain as many asn − 1 levels (internal nodes) and prove difficult to interpret. Two methods are described for transforming a dendrogram into a more readily interpretable parsimonious tree. These involve limiting either (i) the number of different values taken by the heights of the internal nodes, or (ii) the number of internal nodes. An illustrative example is presented.","Data simplification, Hierarchical classification",https://link.springer.com//article/10.1007/BF01890077
Complete linkage as a multiple stopping rule for single linkage clustering,"Two commonly used clustering criteria are single linkage, which maximizes the minimum distance between clusters, and complete linkage, which minimizes the maximum distance within a cluster. By synthesizing these criteria, partitions of objects are sought which maximize a combined measure of the minimum distance between clusters and the maximum distance within a cluster. Each combined measure is shown to select a partition in the single linkage hierarchy. Therefore, in effect, complete linkage is used to provide a stopping rule for single linkage. An algorithm is outlined which uses the distance between each pair of objects twice only. To illustrate the method, an example is given using 23 Glamorganshire soil profiles.","Algorithm, Cohesion, Isolation, Soil profile",https://link.springer.com//article/10.1007/BF01890078
On the use of ordered sets in problems of comparison and consensus of classifications,"Ordered set theory provides efficient tools for the problems of comparison and consensus of classifications Here, an overview of results obtained by the ordinal approach is presented Latticial or semilatticial structures of the main sets of classification models are described Many results on partitions are adaptable to dendrograms; many results on n-trees hold in any median semilattice and thus have counterparts on ordered trees and Buneman (phylogenetic) trees For the comparison of classifications, the semimodularity of the ordinal structures involved yields computable least-move metrics based on weighted or unweighted elementary transformations In the unweighted case, these metrics have simple characteristic properties For the consensus of classifications, the constructive, axiomatic, and optimization approaches are considered Natural consensus rules (majoritary, oligarchic, ) have adequate ordinal formalizations A unified presentation of Arrow-like characterization results is given In the cases of n-trees, ordered trees and Buneman trees, the majority rule is a significant example where the three approaches converge","Hierarchical classification, Median, Metric, Numerical taxonomy, Partial order, Partition, Phylogeny, Ultrametric",https://link.springer.com//article/10.1007/BF01894188
On lattice consensus methods,"We investigate the consensus problem for classifications of three types: partitions, dendrograms, and n-trees For partitions or dendrograms, lattice polynomials define natural consensus functions We extend these lattice methods to n-trees, introducing a general class of consensus functions that includes the intersection consensus functions in current use These lattice consensus methods have a number of desirable mathematical properties We prove that they all satisfy the Pareto Axiom For each of the three classification types, we determine which lattice consensus functions satisfy the Betweenness Axiom","Consensus function, Lattice polynomial, Partition, Dendrogram, n-tree, Betweenness Axiom, Pareto Axiom",https://link.springer.com//article/10.1007/BF01894189
Comparison of classifications using measures intermediate between metric dissimilarity and consensus similarity,"Two fundamental approaches to the comparison of classifications (e g, partitions on the same finite set of objects) can be distinguished One approach is based upon measures of metric dissimilarity while the other is based upon measures of similarity, or consensus These approaches are not necessarily simple complements of each other Instead, each captures different, limited views of comparison of two classifications The properties of these measures are clarified by their relationships to Day's complexity models and to association measures of numerical taxonomy The two approaches to comparison are equated with the use of separation and minimum value sensitive measures, suggesting the potential application of an intermediate sensitive measure to the problem of comparison of classifications Such a measure is a linear combination of separation sensitive and minimum value sensitive components The application of these intermediate measures is contrasted with the two extremes The intermediate measure for the comparison of classifications is applied to a problem of character weighting arising in the analysis of Australian stream basins","Classification, Comparison, Numerical taxonomy, Consensus, Intermediate sensitive measures, Partial orders, Stream basins, Inter-partition distance",https://link.springer.com//article/10.1007/BF01894190
Clustering and isolation in the consensus problem for partitions,"We examine the problem of aggregating several partitions of a finite set into a single consensus partition We note that the dual concepts of clustering and isolation are especially significant in this connection. The hypothesis that a consensus partition should respect unanimity with respect to either concept leads us to stress a consensus interval rather than a single partition. The extremes of this interval are characterized axiomatically. If a sufficient totality of traits has been measured, and if measurement errors are independent, then a “true” classifying partition can be expected to lie in the consensus interval. The structure of the partitions in the interval lends itself to partial solutions of the consensus problem Conditional entropy may be used to quantify the uncertainty inherent in the interval as a whole","Lattice of partitions, Consensus function, Consensus interval, Conditional entropy, Clustering, Isolation",https://link.springer.com//article/10.1007/BF01894191
"N-trees as nestings: Complexity, similarity, and consensus","Interpreting a taxonomic tree as a set of objects leads to natural measures of complexity and similarity, and sets natural lower bounds on a consensus tree Interpretations differing as to the kind of objects constituting a tree lead to different measures and consensus Subset nesting is preferred over the clusters (strict consensus) and even the triads interpretations because of its superior expression of shared structure Algorithms for computing the complexity and similarity of trees, as well as a consensus index onto [0,1], are presented for this interpretation The “full consensus” is defined as the only tree which includes all the nestings shared in a profile of rival trees and whose clusters reflect only nestings shared in the profile The full consensus is proved to exist uniquely for each profile, and to equal the Adams consensus","Full consensus, Adams consensus, Adams-2 consensus, Strict consensus, Rooted trees",https://link.springer.com//article/10.1007/BF01894192
s-Consensus index method: An additional axiom,"A consensus index method is an ordered pair consisting of a consensus method and a consensus index Day and McMorris (1985) have specified two minimal axioms, one which should be satisfied by the consensus method and the other by the consensus index The axiom for consensus indices is not satisfied by the s-consensus index In this paper, an additional axiom, which states that a consensus index equal to one implies profile unanimity, is proposed The s-consensus method together with a modification of the s-consensus index (i e, normalized by the number of distinct nontrivial clusters in the profile) is shown to satisfy the two axioms proposed by Day and McMorris and the new axiom","Consensus, Intersection method, n-tree, Numerical taxonomy",https://link.springer.com//article/10.1007/BF01894193
The median procedure for n-trees,"Let (X,d) be a metric space The functionM:Xk → 2x defined by\(M(x_1 ,,x_k ) = \{ x \in X:\sum\limits_{i = 1}^k {d(x,x_i )}\) is the minimum } is called themedian procedure and has been found useful in various applications involving the notion of consensus Here we present axioms that characterizeM whenX is a certain class of trees (hierarchical classifications), andd is the symmetric difference metric","n-Trees, Consensus, Median procedure, Majority rule",https://link.springer.com//article/10.1007/BF01894194
Consensus supertrees: The synthesis of rooted trees containing overlapping sets of labeled leaves,"Given two dendrograms (rooted tree diagrams) which have some but not all of their base points in common, a supertree is a dendrogram from which each of the original trees can be regarded as samples The distinction is made between inconsistent and consistent sample trees, defined by whether or not the samples provide contradictory information about the supertree An algorithm for obtaining the strict consensus supertree of two consistent sample trees is presented, as are procedures for merging two inconsistent sample trees Some suggestions for future work are made","Consensus trees, Hierarchical classification, Population classification, Sample classification, Supertrees",https://link.springer.com//article/10.1007/BF01894195
Tree enumeration modulo a consensus,"The number of trees withn labeled terminal vertices grows too rapidly withn to permit exhaustive searches for Steiner trees or other kinds of optima in cladistics and related areas Often, however, structured constraints are known and may be imposed on the set of trees to be scanned These constraints may be formulated in terms of a consensus among the trees to be searched We calculate the reduction in the number of trees to be enumerated as a function of properties of the imposed consensus","Consensus, Tree enumeration",https://link.springer.com//article/10.1007/BF01894196
Metric and Euclidean properties of dissimilarity coefficients,"We assemble here properties of certain dissimilarity coefficients and are specially concerned with their metric and Euclidean status. No attempt is made to be exhaustive as far as coefficients are concerned, but certain mathematical results that we have found useful are presented and should help establish similar properties for other coefficients. The response to different types of data is investigated, leading to guidance on the choice of an appropriate coefficient.","Choice of coefficient, Dissimilarity, Distance, Euclidean property, Metric property, Similarity",https://link.springer.com//article/10.1007/BF01896809
Two classes of element-wise transformations preserving the psd nature of coefficient matrices,Two classes of element-wise transformations are proved to preserve the positive semi-definite nature of coefficient matrices. The correctness of a conjecture by Gower and Legendre on the positive semidefinite nature of a certain coefficient matrix is proved. It is shown that the matrix of monotonicity coefficients proposed by Bentler is positive semidefinite for data without ties.,"Association coefficient, Similarity coefficient, Monotonicity coefficient, Gramian matrix, Binomial series",https://link.springer.com//article/10.1007/BF01896810
Computing the nearest neighbor interchange metric for unlabeled binary trees is NP-complete,One of the most important problems in classification is that of quantitative comparison of hierarchical trees. In this note we answer an open problem of Culík and Wood (1982) concerning the nearest neighbor interchange metric by proving that its underlying decision problem is NP-complete.,"Nearest neighbor interchange metric, Binary tree (dendogram), NP-completeness theory",https://link.springer.com//article/10.1007/BF01896811
Aggregation of equivalence relations,"Each ofn attributes partitions a set of items into equivalence classes. Aconsistent aggregator of then partitions is defined as an aggregate partition that satisfies an independence condition and a unanimity condition. It is shown that the class of consistent aggregators is precisely the class ofconjunctive aggregators. That is, for each consistent aggregator there is a nonempty subsetN of the attributes such that two items are equivalent in the aggregate partition if and only if they are equivalent with respect to each attribute inN.","Classification, aggregation, consistency, conjunctive aggregator",https://link.springer.com//article/10.1007/BF01896812
"The minimal rank correlation, subject to order restrictions, with application to the weighted linear choice model","The weighted linear choice model is one of the most popular models in the social sciences. In this model the utility of a choice object is represented as a weighted sum of attribute-level desirabilities, where the weights are attribute importances. In many empirical contexts the choice objects are such that individuals are highly correlated in terms of their desirability ordering of levels within attribute (e.g., price levels, durability levels, etc.) but may differ appreciably in terms of their evaluations of each attribute's importance.In this paper we address the problem of how dissimilar two individuals may be, in a rank correlation sense, given that they agree completely on the desirability ordering of levels within attributes, but may disagree considerably regarding the importance they attach to the attributes themselves. The problem has interesting implications regarding the potential value of clustering individuals' utility functions for market segmentation or other such purposes.","Choice, Rank correlation, Linear additive model",https://link.springer.com//article/10.1007/BF01896813
A special Jackknife for Multidimensional Scaling,"In this paper we develop a version of the Jackknife which seems especially suited for Multidimensional Scaling. It deletes one stimulus at a time, and combines the resulting solutions by a least squares matching method. The results can be used for stability analysis, and for purposes of cross validation.","Multidimensional scaling, Jackknife, Cross validation, Stability",https://link.springer.com//article/10.1007/BF01896814
A hierarchical approach to multigroup factorial invariance,"A procedure is presented which permits the analysis of factor analytic problems in which several groups exist. The analysis incorporates a hierarchical scheme of searching for factorial invariance and is an extension of Meredith's (1964) Method One procedure. By overlaying a contextual frame of reference on a traditional factor analysis solution, it is possible to use this technique to examine structural similarity and dissimilarity between groups. The procedure is exhibited in an example and in addition a comparison is made to discriminant analysis.","Discriminant analysis, Factor analysis, Structural similarity, Generalized canonical correlation",https://link.springer.com//article/10.1007/BF01896815
Optimal algorithms for comparing trees with labeled leaves,"LetRn denote the set of rooted trees withn leaves in which: the leaves are labeled by the integers in {1, ...,n}; and among interior vertices only the root may have degree two. Associated with each interior vertexv in such a tree is the subset, orcluster, of leaf labels in the subtree rooted atv. Cluster {1, ...,n} is calledtrivial. Clusters are used in quantitative measures of similarity, dissimilarity and consensus among trees. For anyk trees inRn, thestrict consensus tree C(T1, ...,Tk) is that tree inRn containing exactly those clusters common to every one of thek trees. Similarity between treesT1 andT2 inRn is measured by the numberS(T1,T2) of nontrivial clusters in bothT1 andT2; dissimilarity, by the numberD(T1,T2) of clusters inT1 orT2 but not in both. Algorithms are known to computeC(T1, ...,Tk) inO(kn2) time, andS(T1,T2) andD(T1,T2) inO(n2) time. I propose a special representation of the clusters of any treeT Rn, one that permits testing in constant time whether a given cluster exists inT. I describe algorithms that exploit this representation to computeC(T1, ...,Tk) inO(kn) time, andS(T1,T2) andD(T1,T2) inO(n) time. These algorithms are optimal in a technical sense. They enable well-known indices of consensus between two trees to be computed inO(n) time. All these results apply as well to comparable problems involving unrooted trees with labeled leaves.","Algorithm complexity, Algorithm design, Comparing hierarchical classifications, Comparing phylogenetic trees, Consensus index, Strict consensus tree",https://link.springer.com//article/10.1007/BF01908061
Evolutionary and psychological effects in pre-evolutionary classifications,"Relative frequency of genera as a function of number of species per genus is plotted for six eighteenth-century classifications: Linnaeus' classifications of animals, plants, minerals, and diseases, and Sauvages' classifications of plants and diseases. The distributions for animals and plants form positively skewed hollow curves similar but not identical to those found in modern biological classifications and predicted by mathematical models of evolution. The distributions for minerals and diseases, however, are more nearly symmetric and convex. The difference between the eighteenth-century and modern classifications of animals and plants probably reflects psychological properties of the taxonomists' judgments; but the difference between the classifications of animals and plants and those of minerals and diseases reflects evolutionary properties of the materials classified, since all six classifications were constructed by the same taxonomists using the same methods. Consequently, the observable effects of evolution are strong enough to be detected in classifications constructed before the acceptance of evolutionary theory; and traditional classifications can contain substantial scientific information despite their reliance on incompletely understood processes of judgment.","Taxonomy, Evolution, Judgment, Species, Genus",https://link.springer.com//article/10.1007/BF01908062
Buyer similarity measures in conjoint analysis: Some alternative proposals,"In the commercial application of conjoint analysis, it is not unusual to compute pairwise similarity measures for buyers based on their commonality across part-worth utilities. The resulting similarity matrix may then be processed by various clustering techniques. This paper considers other ways to measure buyer similarity in conjoint analysis and compares the resulting measures to the traditional use of part-worth commonalities. An empirical example is used to illustrate the approaches and compare their results.","Conjoint analysis, Similarity measures, Test products, Cluster analysis, Market segmentation",https://link.springer.com//article/10.1007/BF01908063
Statistical theory in clustering,"A number of statistical models for forming and evaluating clusters are reviewed. Hierarchical algorithms are evaluated by their ability to discover high density regions in a population, and complete linkage hopelessly fails; the others don't do too well either. Single linkage is at least of mathematical interest because it is related to the minimum spanning tree and percolation. Mixture methods are examined, related to k-means, and the failure of likelihood tests for the number of components is noted. The DIP test for estimating the number of modes in a univariate population measures the distance between the empirical distribution function and the closest unimodal distribution function (or k-modal distribution function when testing for k modes). Its properties are examined and multivariate extensions are proposed. Ultrametric and evolutionary distances on trees are considered briefly.","Theory of clustering, High density clusters, Tests of unimodality",https://link.springer.com//article/10.1007/BF01908064
On some significance tests in cluster analysis,"We investigate the properties of several significance tests for distinguishing between the hypothesisH of a “homogeneous” population and an alternativeA involving “clustering” or “heterogeneity,” with emphasis on the case of multidimensional observationsx1, ...,xn εℝp. Four types of test statistics are considered: the (s-th) largest gap between observations, their mean distance (or similarity), the minimum within-cluster sum of squares resulting from a k-means algorithm, and the resulting maximum F statistic. The asymptotic distributions underH are given forn→∞ and the asymptotic power of the tests is derived for neighboring alternatives.","Significance test, Homogeneity, Heterogeneity, Gap test, Minimum within-cluster sum of squares, Maximum F statistics, Asymptotic normal distribution",https://link.springer.com//article/10.1007/BF01908065
The mixture method of clustering applied to three-way data,"Clustering or classifying individuals into groups such that there is relative homogeneity within the groups and heterogeneity between the groups is a problem which has been considered for many years. Most available clustering techniques are applicable only to a two-way data set, where one of the modes is to be partitioned into groups on the basis of the other mode. Suppose, however, that the data set is three-way. Then what is needed is a multivariate technique which will cluster one of the modes on the basis of both of the other modes simultaneously. It is shown that by appropriate specification of the underlying model, the mixture maximum likelihood approach to clustering can be applied in the context of a three-way table. It is illustrated using a soybean data set which consists of multiattribute measurements on a number of genotypes each grown in several environments. Although the problem is set in the framework of clustering genotypes, the technique is applicable to other types of three-way data sets.","Clustering, Mixture maximum likelihood, Three-way data",https://link.springer.com//article/10.1007/BF01908066
Numerical classification of proximity data with assignment measures,"An approach to numerical classification is described, which treats the assignment of objects to types as a continuous variable, called an assignment measure. Describing a classification by an assignment measure allows one not only to determine the types of objects, but also to see relationships among the objects of the same type and among the types themselves.A classification procedure, the Assignment-Prototype algorithm, is described and evaluated. It is a numerical technique for obtaining assignment measures directly from one-mode, two-way proximity matrices.","Numerical classification, Cluster analysis, Assignment measures, Proximity data, Dissimilarity data, Assignment-Prototype Algorithm",https://link.springer.com//article/10.1007/BF01908073
Optimal variable weighting for hierarchical clustering: An alternating least-squares algorithm,"This paper presents the development of a new methodology which simultaneously estimates in a least-squares fashion both an ultrametric tree and respective variable weightings for profile data that have been converted into (weighted) Euclidean distances. We first review the relevant classification literature on this topic. The new methodology is presented including the alternating least-squares algorithm used to estimate the parameters. The method is applied to a synthetic data set with known structure as a test of its operation. An application of this new methodology to ethnic group rating data is also discussed. Finally, extensions of the procedure to model additive, multiple, and three-way trees are mentioned.","Ultrametric trees, Mathematical programming, Variable importance",https://link.springer.com//article/10.1007/BF01908074
Comparing partitions,"The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1.","Measures of agreement, Measures of association, Consensus indices",https://link.springer.com//article/10.1007/BF01908075
Cluster analysis of dyad distributions in networks,Existing statistical models for network data that are easy to estimate and fit are based on the assumption of dyad independence or conditional dyad independence if the individuals are categorized into subgroups. We discuss how such models might be overparameterized and argue that there is a need for subgrouping methods to find appropriate models. We propose clustering of dyad distributions as such a method and illustrate it by analyzing how cooperative learning methods affect friendship data for school children.,"Log-linear network models, Clustering, Dyad distributions, Cooperative learning",https://link.springer.com//article/10.1007/BF01908076
Investigation of proportional link linkage clustering methods,"Proportional link linkage (PLL) clustering methods are a parametric family of monotone invariant agglomerative hierarchical clustering methods. This family includes the single, minimedian, and complete linkage clustering methods as special cases; its members are used in psychological and ecological applications. Since the literature on clustering space distortion is oriented to quantitative input data, we adapt its basic concepts to input data with only ordinal significance and analyze the space distortion properties of PLL methods. To enable PLL methods to be used when the numbern of objects being clustered is large, we describe an efficient PLL algorithm that operates inO(n2 logn) time andO(n2) space.","Algorithm complexity, Algorithm design, Integer link linkage clustering method, Monotone invariance, SAHN clustering method, Space distortion",https://link.springer.com//article/10.1007/BF01908077
Obtaining common pruned trees,"Given two or more dendrograms (rooted tree diagrams) based on the same set of objects, ways are presented of defining and obtaining common pruned trees. Bounds on the size of a largest common pruned tree are introduced, as is a categorization of objects according to whether they belong to all, some, or no largest common pruned trees. Also described is a procedure for regrafting pruned branches, yielding trees for which one can assess the reliability of the depicted relationships. The tree obtained by regrafting branches on to a largest common pruned tree is shown to contain all the classes present in the strict consensus tree. The theory is illustrated by application to two classifications of a set of forty-nine stratigraphical pollen spectra.","Common pruned trees, Consensus trees, Hierarchical classification, Regrafting",https://link.springer.com//article/10.1007/BF01908078
Efficient algorithms for agglomerative hierarchical clustering methods,"Whenevern objects are characterized by a matrix of pairwise dissimilarities, they may be clustered by any of a number of sequential, agglomerative, hierarchical, nonoverlapping (SAHN) clustering methods. These SAHN clustering methods are defined by a paradigmatic algorithm that usually requires 0(n3) time, in the worst case, to cluster the objects. An improved algorithm (Anderberg 1973), while still requiring 0(n3) worst-case time, can reasonably be expected to exhibit 0(n2) expected behavior. By contrast, we describe a SAHN clustering algorithm that requires 0(n2 logn) time in the worst case. When SAHN clustering methods exhibit reasonable space distortion properties, further improvements are possible. We adapt a SAHN clustering algorithm, based on the efficient construction of nearest neighbor chains, to obtain a reasonably general SAHN clustering algorithm that requires in the worst case 0(n2) time and space.Whenevern objects are characterized byk-tuples of real numbers, they may be clustered by any of a family of centroid SAHN clustering methods. These methods are based on a geometric model in which clusters are represented by points ink-dimensional real space and points being agglomerated are replaced by a single (centroid) point. For this model, we have solved a class of special packing problems involving point-symmetric convex objects and have exploited it to design an efficient centroid clustering algorithm. Specifically, we describe a centroid SAHN clustering algorithm that requires 0(n2) time, in the worst case, for fixedk and for a family of dissimilarity measures including the Manhattan, Euclidean, Chebychev and all other Minkowski metrics.","Algorithm complexity, Algorithm design, Centroid clustering method, Geometric model, SAHN clustering method",https://link.springer.com//article/10.1007/BF01890115
The representation of three-way proximity data by single and multiple tree structure models,"Models for the representation of proximity data (similarities/dissimilarities) can be categorized into one of three groups of models: continuous spatial models, discrete nonspatial models, and hybrid models (which combine aspects of both spatial and discrete models). Multidimensional scaling models and associated methods, used for thespatial representation of such proximity data, have been devised to accommodate two, three, and higher-way arrays. At least one model/method for overlapping (but generally non-hierarchical) clustering called INDCLUS (Carroll and Arabie 1983) has been devised for the case of three-way arrays of proximity data. Tree-fitting methods, used for thediscrete network representation of such proximity data, have only thus far been devised to handle two-way arrays. This paper develops a new methodology called INDTREES (for INdividual Differences in TREE Structures) for fitting various(discrete) tree structures to three-way proximity data. This individual differences generalization is one in which different individuals, for example, are assumed to base their judgments on the same family of trees, but are allowed to have different node heights and/or branch lengths.We initially present an introductory overview focussing on existing two-way models. The INDTREES model and algorithm are then described in detail. Monte Carlo results for the INDTREES fitting of four different three-way data sets are presented. In the application, a single ultrametric tree is fitted to three-way proximity data derived from intention-to-buy-data for various brands of over-the-counter pain relievers for relieving three common types of maladies. Finally, we briefly describe how the INDTREES procedure can be extended to accommodate hybrid modelling, as well as to handle other types of applications.","Clustering, Alternating least squares, Discrete optimization",https://link.springer.com//article/10.1007/BF01890116
Unclassed matrix shading and optimal ordering in hierarchical cluster analysis,A method is presented for the graphic display of proximity matrices as a complement to the common data analysis techniques of hierarchical clustering. The procedure involves the use of computer generated shaded matrices based on unclassed choropleth mapping in conjunction with a strategy for matrix reorganization. The latter incorporates a combination of techniques for seriation and the ordering of binary trees.,"Hierarchical clustering, Choropleth mapping, Matrix reorganization, Unidimensional seriation, Ordered binary trees",https://link.springer.com//article/10.1007/BF01890117
A computationally efficient approximation to the nearest neighbor interchange metric,"The nearest neighbor interchange (nni) metric is a distance measure providing a quantitative measure of dissimilarity between two unrooted binary trees with labeled leaves. The metric has a transparent definition in terms of a simple transformation of binary trees, but its use in nontrivial problems is usually prevented by the absence of a computationally efficient algorithm. Since recent attempts to discover such an algorithm continue to be unsuccessful, we address the complementary problem of designing an approximation to the nni metric. Such an approximation should be well-defined, efficient to compute, comprehensible to users, relevant to applications, and a close fit to the nni metric; the challenge, of course, is to compromise these objectives in such a way that the final design is acceptable to users with practical and theoretical orientations. We describe an approximation algorithm that appears to satisfy adequately these objectives. The algorithm requires O(n) space to compute dissimilarity between binary trees withn labeled leaves; it requires O(n logn) time for rooted trees and O(n2 logn) time for unrooted trees. To help the user interpret the dissimilarity measures based on this algorithm, we describe empirical distributions of dissimilarities between pairs of randomly selected trees for both rooted and unrooted cases.","Algorithm complexity, Algorithm design, Binary tree, Crossover metric, Dissimilarity measure, Distance measure, Hierarchical classification, nni metric",https://link.springer.com//article/10.1007/BF01890118
A note on Cohen's profile similarity coefficientrc,"Analytic procedures for classifying objects are commonly based on the product-moment correlation as a measure of object similarity. This statistic, however, generally does not represent an invariant index of similarity between two objects if they are measured along different bipolar variables where the direction of measurement for each variable is arbitrary. A computer simulation study compared Cohen's (1969) proposed solution to the problem, the invariant similarity coefficientrc, with the mean product-moment correlation based on all possible changes in the measurement direction of individual variables within a profile of scores. The empirical observation thatrc approaches the mean product-moment correlation with increases in the number of scores in the profiles was interpreted as encouragement for the use ofrc in classification research. Some cautions regarding its application were noted.","Transpose factor analysis, Q-technique, Profile analysis, Bipolar variables, Direction of measurement",https://link.springer.com//article/10.1007/BF01890119
GENFOLD2: A set of models and algorithms for the general UnFOLDing analysis of preference/dominance data,"A general set of multidimensional unfolding models and algorithms is presented to analyze preference or dominance data. This class of models termed GENFOLD2 (GENeral UnFOLDing Analysis-Version 2) allows one to perform internal or external analysis, constrained or unconstrained analysis, conditional or unconditional analysis, metric or nonmetric analysis, while providing the flexibility of specifying and/or testing a variety of different types of unfolding-type preference models mentioned in the literature including Caroll's (1972, 1980) simple, weighted, and general unfolding analysis. An alternating weighted least-squares algorithm is utilized and discussed in terms of preventing degenerate solutions in the estimation of the specified parameters. Finally, two applications of this new method are discussed concerning preference data for ten brands of pain relievers and twelve models of residential communication devices.","Multidimensional scaling, Unfolding analysis, Preference models",https://link.springer.com//article/10.1007/BF01890122
"The generation of random, binary unordered trees","Several techniques are given for the uniform generation of trees for use in Monte Carlo studies of clustering and tree representations. First, general strategies are reviewed for random selection from a set of combinatorial objects with special emphasis on two that use random mapping operations. Theorems are given on how the number of such objects in the set (e.g., whether the number is prime) affects which strategies can be used. Based on these results, methods are presented for the random generation of six types of binary unordered trees. Three types of labeling and both rooted and unrooted forms are considered. Presentation of each method includes the theory of the method, the generation algorithm, an analysis of its computational complexity and comments on the distribution of trees over which it samples. Formal proofs and detailed algorithms are in appendices.","Uniform sampling, Tree construction, Monte Carlo studies, Tree algorithms, Clustering methodology, Classification methodology",https://link.springer.com//article/10.1007/BF01890123
Ultrametric tree representations of incomplete dissimilarity data,The least squares algorithm for fitting ultrametric trees to proximity data originally proposed by Carroll and Pruzansky and further elaborated by De Soete is extended to handle missing data. A Monte Carlo evaluation reveals that the algorithm is capable of recovering an ultrametric tree underlying an incomplete set of error-perturbed dissimilarities quite well.,"Least squares method, Incomplete data, Proximity data, Clustering",https://link.springer.com//article/10.1007/BF01890124
"On the null distribution of distance between two groups, using mixed continuous and categorical variables","The location model is a useful tool in parametric analysis of mixed continuous and categorical variables. In this model, the continuous variables are assumed to follow different multivariate normal distributions for each possible combination of categorical variable values. Using this model, a distance between two populations involving mixed variables can be defined. To date, however, no distributional results have been available, against which to assess the outcomes of practical applications of this distance. The null distribution of estimated distance is therefore considered in this paper, for a range of possible situations. No explicit analytical expressions are derived for this distribution, but easily implementable Monte Carlo schemes are described. These are then applied to previously cited examples.","Distance between groups, Location model, Mixed variables, Monte Carlo methods, Simulation",https://link.springer.com//article/10.1007/BF01890125
Asymptotic properties of univariate sample k-means clusters,"A random sample of sizeN is divided intok clusters that minimize the within clusters sum of squares locally. Some large sample properties of this k-means clustering method (ask approaches ∞ withN) are obtained. In one dimension, it is established that the sample k-means clusters are such that the within-cluster sums of squares are asymptotically equal, and that the sizes of the cluster intervals are inversely proportional to the one-third power of the underlying density at the midpoints of the intervals. The difficulty involved in generalizing the results to the multivariate case is mentioned.","K-means clusters, Within-clusters sum of squares, Cluster lengths, Non-standard asymptotics",https://link.springer.com//article/10.1007/BF01890126
